{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os, datetime, random, json, warnings\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from math import sqrt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "directory_path = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:-1]) + \"\\\\data\"\n",
    "os.path.exists(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingConfig(object):\n",
    "    epochs = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "\n",
    "\n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    # 内层一位卷积核数量、外层卷积核数量应该等于 embeddingSize\n",
    "    filters = 128\n",
    "    # attention head number\n",
    "    numHeads = 8\n",
    "    # set the number of transformer block\n",
    "    numBlocks = 1\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # multi-head-attention layer dropout-rate\n",
    "    keepProb = 0.9\n",
    "    \n",
    "    # fully-connection layer dropout-rate\n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200\n",
    "    batchSize = 128\n",
    "    dataSource = directory_path + \"\\\\preProcess\\\\labeledTrain.csv\"\n",
    "    stopWordSource = directory_path + \"\\\\english\"\n",
    "    \n",
    "    # binary-classes set to 1, otherwise set to other number\n",
    "    numClasses = 1\n",
    "    rate = 0.8\n",
    "    training = TrainingConfig()\n",
    "    model = ModelConfig()\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource\n",
    "        self._sequenceLength = config.sequenceLength\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = dict()\n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        self.wordEmbedding = None\n",
    "        self.labelList = []\n",
    "    \n",
    "    \n",
    "    def _readData(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if self.config.numClasses == 1:\n",
    "            labels = df[\"sentiment\"].tolist()\n",
    "        elif self.config.numClasses > 1:\n",
    "            labels = df[\"rate\"].tolist()\n",
    "        \n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "        \n",
    "        return reviews, labels\n",
    "    \n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        return labels\n",
    "    \n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "    \n",
    "    \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        reviews = []\n",
    "        for review in x:\n",
    "            if len(review) > self._sequenceLength:\n",
    "                reviews.append(review[:self._sequenceLength])\n",
    "            else:\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "        \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        trainReviews = np.asarray(reviews[: trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[: trainIndex], dtype=\"float32\")\n",
    "        evalReviews = np.asarray(reviews[trainIndex :], dtype=\"int64\")\n",
    "        evalLabels = np.array(y[trainIndex :], dtype=\"float32\")\n",
    "        \n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "    \n",
    "    \n",
    "    def _getWordEmbedding(self, words):\n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(directory_path + \"\\\\word2vec\\\\word2Vec.bin\", \n",
    "                                                                 binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(\"{} is not exist in vocab...\".format(word))\n",
    "        \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "        \n",
    "        \n",
    "    def _genVocabulary(self, reviews, labels):\n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        wordCount = Counter(subWords)\n",
    "        sortedWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        words = [item[0] for item in sortedWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        with open(directory_path + \"\\\\wordJson\\\\word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        with open(directory_path + \"\\\\wordJson\\\\label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "    \n",
    "    \n",
    "    \n",
    "    def dataGen(self):\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
    "        \n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        self.trainReviews, self.trainLabels, self.evalReviews, self.evalLabels = self._genTrainEvalData(reviewIds, \n",
    "                                                                                                       labelIds, \n",
    "                                                                                                       word2idx, \n",
    "                                                                                                       self._rate)\n",
    "\n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextBatch(x, y, batchSize):\n",
    "    perm = np.arange(len(x))\n",
    "    np.random.shuffle(perm)\n",
    "    x = x[perm]\n",
    "    y = y[perm]\n",
    "    \n",
    "    numBatches = len(x) // batchSize\n",
    "    for index in range(numBatches):\n",
    "        start = index * batchSize\n",
    "        end = start + batchSize\n",
    "        batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "        batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "        \n",
    "        yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fixedPositionEmbedding(batchSize, sequenceLen):\n",
    "    \"\"\"\n",
    "        生成 position embedding\n",
    "    \"\"\"\n",
    "    embeddedPosition = []\n",
    "    for batch in range(batchSize):\n",
    "        x = []\n",
    "        for step in range(sequenceLen):\n",
    "            a = np.zeros(sequenceLen)\n",
    "            a[step] = 1\n",
    "            x.append(a)\n",
    "        embeddedPosition.append(x)\n",
    "    \n",
    "    return np.array(embeddedPosition, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "wordEmbedding = data.wordEmbedding\n",
    "labelList = data.labelList\n",
    "embeddedPosition = fixedPositionEmbedding(config.batchSize, config.sequenceLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6, 6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = fixedPositionEmbedding(4, 6)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(item: list) -> float:\n",
    "    \"\"\"\n",
    "    计算列表中元素的平均值\n",
    "    :param item: 列表对象\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    \"\"\"\n",
    "    计算二类和多类的准确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == true_y[i]:\n",
    "            corr += 1\n",
    "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "    return acc\n",
    "\n",
    "\n",
    "def binary_precision(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的精确率计算\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    pred_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == positive:\n",
    "            pred_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "    return prec\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    true_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if true_y[i] == positive:\n",
    "            true_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    rec = corr / true_corr if true_corr > 0 else 0\n",
    "    return rec\n",
    "\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    \"\"\"\n",
    "    二类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param beta: beta值\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    try:\n",
    "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    except:\n",
    "        f_b = 0\n",
    "    return f_b\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的精确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    \"\"\"\n",
    "    多类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :param beta: beta值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到二分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到多分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param labels:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = multi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    return acc, recall, precision, f_beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Transformer(object):\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "        self.config = config\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeppProb\")\n",
    "        self.embeddedPosition = tf.placeholder(tf.float32, [None, config.sequenceLength, config.sequenceLength], \n",
    "                                              name=\"embeddedPosition\")\n",
    "        \n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        \"\"\"\n",
    "            word embedding layer 中 position embedding 有两种定义方式\n",
    "            a) 直接使用固定的 one-hot 形式传入，然后与词向量进行拼接\n",
    "            b) 采用 attention is all you need 论文中的方式\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "            self.embedded = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "            # 采用方式 a)，表现不错\n",
    "            self.embeddedWords = tf.concat([self.embedded, self.embeddedPosition], -1)\n",
    "        \n",
    "        with tf.name_scope(\"transformer\"):\n",
    "            for index in range(config.model.numBlocks):\n",
    "                with tf.name_scope(\"transformer-{}\".format(index + 1)):\n",
    "                    # 维度为 [batch_size, sequence_length, embedding_size]\n",
    "                    multiHeadAtt = self._multiheadAttention(rawKeys=self.inputX, \n",
    "                                                           queries=self.embeddedWords, \n",
    "                                                           keys=self.embeddedWords)\n",
    "                    \n",
    "                    # 维度为 [batch_size, sequence_length, embedding_size]\n",
    "                    self.embeddedWords = self._feedForward(multiHeadAtt, \n",
    "                                                          [config.model.filters, config.model.embeddingSize + config.sequenceLength])\n",
    "             \n",
    "            outputs = tf.reshape(self.embeddedWords, [-1, config.sequenceLength * (config.model.embeddingSize + config.sequenceLength)])\n",
    "        \n",
    "        outputSize = outputs.get_shape()[-1].value\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            outputW = tf.get_variable(\"outputW\", shape=[outputSize, config.numClasses], \n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB = tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            \n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(outputs, outputW, outputB, name=\"logits\")\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.float32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                              dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                       labels=self.inputY)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "    \n",
    "    \n",
    "    def _layerNormalization(self, inputs, scope=\"layerNorm\"):\n",
    "        # LayerNorm 层与 BN 层有所区别\n",
    "        epsilon = self.config.model.epsilon\n",
    "        \n",
    "        # [batch_size, sequence_length, embedding_size]\n",
    "        inputsShape = inputs.get_shape()\n",
    "        paramsShape = inputsShape[-1:]\n",
    "        \n",
    "        # LayerNorm 是在最后的维度上计算输入数据的均值和方差，BN 层考虑的是所有维度的\n",
    "        # 此处的 mean、variance 的维度均是 [batch_size, sequence_length, 1]\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        \n",
    "        beta = tf.Variable(tf.zeros(paramsShape))\n",
    "        gamma = tf.Variable(tf.ones(paramsShape))\n",
    "        \n",
    "        normalized = (inputs - mean) / ((variance + epsilon) ** .5)\n",
    "        outputs = gamma * normalized + beta\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def _multiheadAttention(self, rawKeys, queries, keys, numUnits=None, \n",
    "                            causality=False, scope=\"multiheadAttention\"):\n",
    "        \"\"\"\n",
    "            rawKeys: 用于计算 mask，因为 keys 是加上了 position embedding 的，其中不存在 padding 为 0 的值\n",
    "        \"\"\"\n",
    "        numHeads = self.config.model.numHeads\n",
    "        keepProb = self.config.model.keepProb\n",
    "        if numUnits is None:\n",
    "            # 无输入值，则将数据的最后一维作为输入，亦即：embedding_size\n",
    "            numUnits = queries.get_shape().as_list()[-1]\n",
    "        \n",
    "        \"\"\"\n",
    "            tf.layers.dense 可以做多维 tensor 的非线性映射，在计算 self-attention 过程中，一定要对\n",
    "            这三个值进行非线性映射；此步骤对应于论文中的 multi-head attention 中的对分割后的数据进行\n",
    "            权重映射的过程；在此实现过程中，先映射、后分割，原则上是等价的\n",
    "            Q、K、V 的维度均是 [batch_size, sequence_length, embedding_size]\n",
    "        \"\"\"\n",
    "        # 映射\n",
    "        Q = tf.layers.dense(queries, numUnits, activation=tf.nn.relu)\n",
    "        K = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "        V = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "        \n",
    "        # 分割：将数据按照最后一维分割成 num_heads 个，然后按照第一维进行拼接\n",
    "        # Q、K、V 的维度均是 [batch_size * numHeads, sequenceLength, embedding_size / numHeads]\n",
    "        Q_ = tf.concat(tf.split(Q, numHeads, axis=-1), axis=0)\n",
    "        K_ = tf.concat(tf.split(K, numHeads, axis=-1), axis=0)\n",
    "        V_ = tf.concat(tf.split(V, numHeads, axis=-1), axis=0)\n",
    "        \n",
    "        # 计算 queries 和 keys 之间的点积，维度为：[batch_size * numHeads, queries_len, key_len]\n",
    "        # queries_len：表示的是 queries 的序列长度\n",
    "        # key_len：表示的是 keys 的序列长度\n",
    "        similary = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
    "        \n",
    "        # 针对计算结果进行缩放\n",
    "        scaledSimilary = similary / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "        \n",
    "        \"\"\"\n",
    "            在输入的系列中，会存在 padding 这样的填充词，这样的词针对 model 毫无帮助，理论上当 padding \n",
    "            均为 0 时，计算出来的相应的权重也该是 0；但是在 transformer model 中引入了 position embedding \n",
    "            后，word-embedding + position-embedding 后就不为 0 了，因此，需要在添加 position embedding 之前，\n",
    "            将其位置 mask 为 0；虽然在 queries 中也存在这样的填充词，但理论上 model 的结果和输入有关，且在\n",
    "            self-attention 中，queries = keys，因此只要一个为 0，计算出来的 weight 就为 0\n",
    "        \"\"\"\n",
    "        \n",
    "        # 利用 tf.tile() 进行张量扩张、维度为：[batch_size * numHeads, keys_len]\n",
    "        keyMasks = tf.tile(rawKeys, [numHeads, 1])\n",
    "        # 新增一个维度，并进行扩展，结果维度为：[batch_size * numHeads, queries_len, keys_len]\n",
    "        keyMasks = tf.tile(tf.expand_dims(keyMasks, 1), [1, tf.shape(queries)[1], 1])\n",
    "        \n",
    "        # tf.ones_like() 生成元素全为 1，维度与 scaledSimilary 相同，然后得到负无穷大的值\n",
    "        paddings = tf.ones_like(scaledSimilary) ** (-2 ** (32 + 1))\n",
    "        \n",
    "        # 维度为：[batch_size * numHeads, queries_len, keys_len]\n",
    "        maskedSimilary = tf.where(tf.equal(keyMasks, 0), paddings, scaledSimilary)\n",
    "        \n",
    "        # 在计算当前词的同时，只考虑上文、不考虑下文\n",
    "        # 在文本分类是，可以只使用 transformer encoder；decoder是生成模型，主要用在语言生成中\n",
    "        if causality:\n",
    "            # shape: [queries_len, keys_len]\n",
    "            diagVals = tf.ones_like(maskedSimilary[0, :, :])\n",
    "            \n",
    "            # shape: [queries_len, keys_len]\n",
    "            tril = tf.contrib.linalg.LinearOperatorTril(diagVals).to_dense()\n",
    "            \n",
    "            # shape: [batch_size * numHeads, queries_len, keys_len]\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(maskedSimilary)[0], 1, 1])\n",
    "            paddings = tf.ones_like(masks) ** (-2 ** (32 + 1))\n",
    "            \n",
    "            # shape: [batch_size * numHeads, queries_len, keys_len]\n",
    "            maskedSimilary = tf.where(tf.equal(masks, 0), paddings, maskedSimilary)\n",
    "        \n",
    "        weights = tf.nn.softmax(maskedSimilary)\n",
    "        # 加权\n",
    "        outputs = tf.matmul(weights, V_)\n",
    "        # 将 multi-head-attention 计算的输出重组为： [batch_size * numHeads, sequence_Length, embedding_size / numHeads]\n",
    "        outputs = tf.concat(tf.split(outputs, numHeads, axis=0), axis=2)\n",
    "        outputs = tf.nn.dropout(outputs, keep_prob=keepProb)\n",
    "        \n",
    "        # 针对每个 sub-layers 建立残差连接、以及： H(x) = F(x) + x\n",
    "        outputs += queries\n",
    "        # normalization layer\n",
    "        outputs = self._layerNormalization(outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def _feedForward(self, inputs, filters, scope=\"multiheadAttention\"):\n",
    "        # 在此，前向传播采用 CNN \n",
    "        # inner-layer\n",
    "        params = {\n",
    "            \"inputs\": inputs, \n",
    "            \"filters\": filters[0], \n",
    "            \"kernel_size\": 1, \n",
    "            \"activation\": tf.nn.relu, \n",
    "            \"use_bias\": True\n",
    "        }\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # outer-layer\n",
    "        params = {\n",
    "            \"inputs\": outputs, \n",
    "            \"filters\": filters[1], \n",
    "            \"kernel_size\": 1, \n",
    "            \"activation\": None, \n",
    "            \"use_bias\": True\n",
    "        }\n",
    "        # 在此用到了一维卷积，实际上卷积尺寸还是二维的，只不过需要制定高度，宽度和 embedding_size 的尺寸一致\n",
    "        # 维度：[batch_size, sequence_length, embedding_size]\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # residual-layer\n",
    "        outputs += inputs\n",
    "        # 归一化\n",
    "        outputs = self._layerNormalization(outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def _positionEmbedding(self, scope=\"positionEmbedding\"):\n",
    "        \"\"\"\n",
    "            生成可训练的位置向量\n",
    "        \"\"\"\n",
    "        batchSize = self.config.batchSize\n",
    "        sequenceLen = self.config.sequenceLength\n",
    "        embeddingSize = self.config.model.embeddingSize\n",
    "        \n",
    "        # 生成位置的索引，并扩张在 batch 中的所有样本\n",
    "        positionIndex = tf.tile(tf.expand_dims(tf.range(sequenceLen), 0), [batchSize, 1])\n",
    "        \n",
    "        # 根据正弦和余弦函数来回去每个位置上的 embedding 的第一部分\n",
    "        positionEmbedding = np.array([[pos / np.power(10000, (i - i % 2) / embeddingSize) for i in range(embeddingSize)] \n",
    "                                      for pos in range(sequenceLen)])\n",
    "        \n",
    "        # 根据奇偶性分别使用 sin() 和 cos() 函数进行包装\n",
    "        positionEmbedding[:, 0::2] = np.sin(positionEmbedding[:, 0::2])\n",
    "        positionEmbedding[:, 1::2] = np.cos(positionEmbedding[:, 1::2])\n",
    "        \n",
    "        positionEmbedding_ = np.cast(positionEmbedding, dtype=tf.float32)\n",
    "        \n",
    "        # [batchSize, sequenceLen, embeddingSize]\n",
    "        positionEmbedded = tf.nn.embedding_lookup(positionEmbedding_, positionIndex)\n",
    "    \n",
    "        return positionEmbedded\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=sess_config)\n",
    "    with sess.as_default():\n",
    "        transformer = Transformer(config, wordEmbedding)\n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        gradsAndVars = optimizer.compute_gradients(transformer.loss)\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"writing to {}\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", transformer.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        savedModelPath = \"\\\\\".join(directory_path.split(\"\\\\\")[:-1]) + \"\\\\model\\\\transformer\\\\savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "            os.rmdir(savedModelPath)\n",
    "#             import shutil\n",
    "#             shutil.rmtree(savedModelPath)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "        def trainStep(batchX, batchY):\n",
    "            feed_dict = {\n",
    "                transformer.inputX: batchX, \n",
    "                transformer.inputY: batchY, \n",
    "                transformer.dropoutKeepProb: config.model.dropoutKeepProb, \n",
    "                transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run(\n",
    "            [trainOp, summaryOp, globalStep, transformer.loss, transformer.predictions], feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, recall, prec, f_beta\n",
    "        \n",
    "        \n",
    "        \n",
    "        def evalStep(batchX, batchY):\n",
    "            feed_dict = {\n",
    "                transformer.inputX: batchX, \n",
    "                transformer.inputY: batchY, \n",
    "                transformer.dropoutKeepProb: 1.0, \n",
    "                transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run(\n",
    "            [summaryOp, globalStep, transformer.loss, transformer.predictions], feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, recall, prec, f_beta\n",
    "        \n",
    "        \n",
    "        print(\"{} start to train model...\".format(datetime.datetime.now().isoformat()))\n",
    "        for index in range(config.training.epochs):\n",
    "            print(\"the {}-th/{} epoch training...\".format(str(index), str(config.training.epochs)))\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, recall, prec, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "                currentStep = tf.train.global_step(sess, globalStep)\n",
    "                print(\"train: {}, step: {}, loss: {}, acc: {}, recall: {}, prec: {}, f_beta: {}\".format(\n",
    "                    datetime.datetime.now().isoformat(), currentStep, loss, acc, recall, prec, f_beta))\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"start to evaluate...\")\n",
    "                    losses, accs, recalls, precs, f_betas = [], [], [], [], []\n",
    "                    for evalBatch in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, recall, prec, f_beta = evalStep(evalBatch[0], evalBatch[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        recalls.append(recall)\n",
    "                        precs.append(prec)\n",
    "                        f_betas.append(f_beta)\n",
    "                    \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"evaluate: \\n\")\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, recall: {}, prec: {}, f_beta: {}\".format(\n",
    "                        time_str, currentStep, mean(losses), mean(accs), mean(recalls), mean(precs), mean(f_betas)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    path = saver.save(sess, \"\\\\\".join(savedModelPath.split(\"\\\\\")[:-1]) + \"\\\\model\\\\transformer\", \n",
    "                                     global_step=currentStep)\n",
    "                    print(\"save model checkpoint to {} \\n\".format(path))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
