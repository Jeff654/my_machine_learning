{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os, csv, time, datetime, random, json, warnings\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:-1]) + \"\\\\data\"\n",
    "os.path.exists(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingConfig(object):\n",
    "    epochs = 4\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    hiddenSizes = [256, 128]\n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200\n",
    "    batchSize = 128\n",
    "    dataSource = directory_path + \"\\\\preProcess\\\\labeledTrain.csv\"\n",
    "    stopWordSource = directory_path + \"\\\\english\"\n",
    "    \n",
    "    # 1 表示二分类，多分类可使用其他数字\n",
    "    numClasses = 1\n",
    "    rate = 0.8\n",
    "    training = TrainingConfig()\n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource\n",
    "        self._sequenceLength = config.sequenceLength\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = dict()\n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        self.embedding = None\n",
    "        self.labelList = []\n",
    "    \n",
    "    \n",
    "    def _readData(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if self.config.numClasses == 1:\n",
    "            labels = df[\"sentiment\"].tolist()\n",
    "        elif self.config.numClasses > 1:\n",
    "            labels = df[\"rate\"].tolist()\n",
    "        \n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "        \n",
    "        return reviews, labels\n",
    "    \n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        return labelIds\n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "    \n",
    "    \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        reviews = []\n",
    "        for review in x:\n",
    "            if len(review) > self._sequenceLength:\n",
    "                reviews.append(review[:self._sequenceLength])\n",
    "            else:\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "        \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
    "        \n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "    \n",
    "    \n",
    "    def _genVocabulary(self, reviews, labels):\n",
    "        all_words = [word for review in reviews for word in review]\n",
    "        subWords = [word for word in all_words if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)\n",
    "        sortedWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        words = [item[0] for item in sortedWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        with open(directory_path + \"\\\\wordJson\\\\word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        \n",
    "        with open(directory_path + \"\\\\wordJson\\\\label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "    \n",
    "    \n",
    "    def _getWordEmbedding(self, words):\n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(directory_path + \"\\\\word2vec\\\\word2Vec.bin\", \n",
    "                                                                 binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(\"{} is not exist...\".format(word))\n",
    "            \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        with open(stopWordPath) as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "    \n",
    "    \n",
    "    def dataGen(self):\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        self.trainReviews, self.trainLabels, self.evalReviews, self.evalLabels = self._genTrainEvalData(reviewIds, \n",
    "                                                                                                        labelIds, \n",
    "                                                                                                        word2idx, \n",
    "                                                                                                        self._rate)\n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextBatch(x, y, batchSize):\n",
    "    perm = np.arange(len(x))\n",
    "    np.random.shuffle(perm)\n",
    "    x = x[perm]\n",
    "    y = y[perm]\n",
    "    \n",
    "    batches = len(x) // batchSize\n",
    "    for index in range(batches):\n",
    "        start = index * batchSize\n",
    "        end = start + batchSize\n",
    "        batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "        batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "        \n",
    "        yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(item: list) -> float:\n",
    "    \"\"\"\n",
    "    计算列表中元素的平均值\n",
    "    :param item: 列表对象\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    \"\"\"\n",
    "    计算二类和多类的准确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == true_y[i]:\n",
    "            corr += 1\n",
    "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "    return acc\n",
    "\n",
    "\n",
    "def binary_precision(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的精确率计算\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    pred_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == positive:\n",
    "            pred_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "    return prec\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    true_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if true_y[i] == positive:\n",
    "            true_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    rec = corr / true_corr if true_corr > 0 else 0\n",
    "    return rec\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    \"\"\"\n",
    "    二类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param beta: beta值\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    try:\n",
    "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    except:\n",
    "        f_b = 0\n",
    "    return f_b\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的精确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    \"\"\"\n",
    "    多类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :param beta: beta值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到二分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到多分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param labels:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = multi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BiLSTMAttention(object):\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        l2Loss = tf.constant(0.0)\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "        \n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for index, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(index)):\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, \n",
    "                                                                                      state_is_tuple=True), \n",
    "                                                              output_keep_prob=self.dropoutKeepProb)\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, \n",
    "                                                                                      state_is_tuple=True), \n",
    "                                                              output_keep_prob=self.dropoutKeepProb)\n",
    "                    \n",
    "                    outputs_, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords, \n",
    "                                                                                  dtype=tf.float32, \n",
    "                                                                                  scope=\"bi-lstm\" + str(index))\n",
    "                    \n",
    "                    print(\"before concat, self.embeddedWords shape: \", self.embeddedWords.shape)\n",
    "                    print(\"the shape of outputs_: \", outputs_[0].shape, outputs_[1].shape)\n",
    "                    self.embeddedWords = tf.concat(outputs_, 2)\n",
    "                    \n",
    "                    print(\"after concat, self.embeddedWords shape: \", self.embeddedWords.shape)\n",
    "        \n",
    "        \n",
    "        outputs = tf.split(self.embeddedWords, 2, -1)\n",
    "        print(\"the item of outputs shape: \", outputs[0].shape, outputs[1].shape)\n",
    "        \n",
    "        # 在 Bi-LSTM + Attention 论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "            print(\"the internal hidden layer shape is: \", H.shape)\n",
    "            \n",
    "            # attention 输出\n",
    "            output = self.attention(H)\n",
    "            outputSize = config.model.hiddenSizes[-1]\n",
    "            \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\"outputW\", shape=[outputSize, config.numClasses], \n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB = tf.Variable(tf.constant(0.0, shape=[config.numClasses]), name=\"outputB\")\n",
    "            \n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(output, outputW, outputB, name=\"logits\")\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.float32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                              dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                       labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + l2Loss * config.model.l2RegLambda\n",
    "    \n",
    "    \n",
    "    def attention(self, H):\n",
    "        \"\"\"\n",
    "            利用 attention 得到句子的向量表示\n",
    "        \"\"\"\n",
    "        hiddenSize = config.model.hiddenSizes[-1]\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 针对 Bi-LSTM 的输出做一个非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 针对 W 和 M 做矩阵运算\n",
    "        # W = [batch_size, step_time, hidden_size] ==> [batch_size * step_time, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1], 每一个时间步的输出由向量转换成为一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用 alpha 的值，针对 H 进行加权求和\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        sequeezeR = tf.reshape(r, [-1, hiddenSize])\n",
    "        sentence_presentation = tf.tanh(sequeezeR)\n",
    "        output = tf.nn.dropout(sentence_presentation, self.dropoutKeepProb)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "labelList = data.labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before concat, self.embeddedWords shape:  (?, 200, 200)\n",
      "the shape of outputs_:  (?, 200, 256) (?, 200, 256)\n",
      "after concat, self.embeddedWords shape:  (?, 200, 512)\n",
      "before concat, self.embeddedWords shape:  (?, 200, 512)\n",
      "the shape of outputs_:  (?, 200, 128) (?, 200, 128)\n",
      "after concat, self.embeddedWords shape:  (?, 200, 256)\n",
      "the item of outputs shape:  (?, 200, 128) (?, 200, 128)\n",
      "the internal hidden layer shape is:  (?, 200, 128)\n",
      "start to train model, 0 epoch / epochs 4\n",
      "train: step: 1, loss: 0.7066668272018433, acc: 0.4609375, recall: 0.46296296296296297, prec: 0.38461538461538464, f_beta: 0.4201680672268908\n",
      "train: step: 2, loss: 1.3528417348861694, acc: 0.546875, recall: 0.0, prec: 0, f_beta: 0\n",
      "train: step: 3, loss: 0.7422524094581604, acc: 0.484375, recall: 0.03278688524590164, prec: 0.2222222222222222, f_beta: 0.05714285714285715\n",
      "train: step: 4, loss: 0.8137862682342529, acc: 0.5625, recall: 1.0, prec: 0.5625, f_beta: 0.72\n",
      "train: step: 5, loss: 0.8296093940734863, acc: 0.5, recall: 1.0, prec: 0.5, f_beta: 0.6666666666666666\n",
      "train: step: 6, loss: 0.6992434859275818, acc: 0.5234375, recall: 0.8695652173913043, prec: 0.5357142857142857, f_beta: 0.6629834254143646\n",
      "train: step: 7, loss: 0.7228884696960449, acc: 0.4609375, recall: 0.3939393939393939, prec: 0.4727272727272727, f_beta: 0.42975206611570244\n",
      "train: step: 8, loss: 0.7010785341262817, acc: 0.46875, recall: 0.10344827586206896, prec: 0.2727272727272727, f_beta: 0.15\n",
      "train: step: 9, loss: 0.7395455837249756, acc: 0.5078125, recall: 0.05172413793103448, prec: 0.2727272727272727, f_beta: 0.08695652173913043\n",
      "train: step: 10, loss: 0.702055811882019, acc: 0.546875, recall: 0.10714285714285714, prec: 0.42857142857142855, f_beta: 0.17142857142857143\n",
      "train: step: 11, loss: 0.7016193270683289, acc: 0.5, recall: 0.16923076923076924, prec: 0.5238095238095238, f_beta: 0.2558139534883721\n",
      "train: step: 12, loss: 0.6943877935409546, acc: 0.5390625, recall: 0.373134328358209, prec: 0.5952380952380952, f_beta: 0.4587155963302752\n",
      "train: step: 13, loss: 0.7060470581054688, acc: 0.4765625, recall: 0.6290322580645161, prec: 0.46987951807228917, f_beta: 0.5379310344827586\n",
      "train: step: 14, loss: 0.7033134698867798, acc: 0.5, recall: 0.7777777777777778, prec: 0.44680851063829785, f_beta: 0.5675675675675675\n",
      "train: step: 15, loss: 0.7077144384384155, acc: 0.5234375, recall: 0.6896551724137931, prec: 0.4819277108433735, f_beta: 0.5673758865248226\n",
      "train: step: 16, loss: 0.6875919103622437, acc: 0.515625, recall: 0.5342465753424658, prec: 0.582089552238806, f_beta: 0.5571428571428572\n",
      "train: step: 17, loss: 0.6994801759719849, acc: 0.546875, recall: 0.4666666666666667, prec: 0.5185185185185185, f_beta: 0.4912280701754386\n",
      "train: step: 18, loss: 0.7191017866134644, acc: 0.4765625, recall: 0.288135593220339, prec: 0.40476190476190477, f_beta: 0.33663366336633666\n",
      "train: step: 19, loss: 0.705845057964325, acc: 0.53125, recall: 0.22413793103448276, prec: 0.4642857142857143, f_beta: 0.3023255813953489\n",
      "train: step: 20, loss: 0.7165482640266418, acc: 0.546875, recall: 0.2153846153846154, prec: 0.6666666666666666, f_beta: 0.3255813953488372\n",
      "train: step: 21, loss: 0.7064386010169983, acc: 0.4609375, recall: 0.10526315789473684, prec: 0.25, f_beta: 0.14814814814814814\n",
      "train: step: 22, loss: 0.7172389030456543, acc: 0.453125, recall: 0.14285714285714285, prec: 0.7333333333333333, f_beta: 0.23913043478260868\n",
      "train: step: 23, loss: 0.7056375741958618, acc: 0.484375, recall: 0.18571428571428572, prec: 0.5909090909090909, f_beta: 0.28260869565217395\n",
      "train: step: 24, loss: 0.6766018867492676, acc: 0.5234375, recall: 0.3880597014925373, prec: 0.5652173913043478, f_beta: 0.46017699115044247\n",
      "train: step: 25, loss: 0.707697331905365, acc: 0.5078125, recall: 0.4, prec: 0.52, f_beta: 0.45217391304347826\n",
      "train: step: 26, loss: 0.701027512550354, acc: 0.484375, recall: 0.47368421052631576, prec: 0.5806451612903226, f_beta: 0.5217391304347826\n",
      "train: step: 27, loss: 0.7234234809875488, acc: 0.46875, recall: 0.5166666666666667, prec: 0.44285714285714284, f_beta: 0.47692307692307695\n",
      "train: step: 28, loss: 0.7139407396316528, acc: 0.4296875, recall: 0.5862068965517241, prec: 0.40963855421686746, f_beta: 0.48226950354609927\n",
      "train: step: 29, loss: 0.7200520038604736, acc: 0.5234375, recall: 0.6610169491525424, prec: 0.4875, f_beta: 0.5611510791366906\n",
      "train: step: 30, loss: 0.7115210294723511, acc: 0.40625, recall: 0.421875, prec: 0.4090909090909091, f_beta: 0.4153846153846154\n",
      "train: step: 31, loss: 0.7081331014633179, acc: 0.5546875, recall: 0.4647887323943662, prec: 0.6346153846153846, f_beta: 0.5365853658536585\n",
      "train: step: 32, loss: 0.6883832812309265, acc: 0.53125, recall: 0.36065573770491804, prec: 0.5116279069767442, f_beta: 0.4230769230769231\n",
      "train: step: 33, loss: 0.7329187989234924, acc: 0.484375, recall: 0.29411764705882354, prec: 0.5263157894736842, f_beta: 0.3773584905660377\n",
      "train: step: 34, loss: 0.7348817586898804, acc: 0.390625, recall: 0.27941176470588236, prec: 0.3958333333333333, f_beta: 0.3275862068965517\n",
      "train: step: 35, loss: 0.7205447554588318, acc: 0.484375, recall: 0.42424242424242425, prec: 0.5, f_beta: 0.4590163934426229\n",
      "train: step: 36, loss: 0.7071607708930969, acc: 0.5078125, recall: 0.546875, prec: 0.5072463768115942, f_beta: 0.5263157894736842\n",
      "train: step: 37, loss: 0.6990852355957031, acc: 0.5625, recall: 0.7910447761194029, prec: 0.5578947368421052, f_beta: 0.654320987654321\n",
      "train: step: 38, loss: 0.70977783203125, acc: 0.4375, recall: 0.7666666666666667, prec: 0.4423076923076923, f_beta: 0.5609756097560975\n",
      "train: step: 39, loss: 0.7209725975990295, acc: 0.5234375, recall: 0.8767123287671232, prec: 0.5517241379310345, f_beta: 0.6772486772486772\n",
      "train: step: 40, loss: 0.6989920139312744, acc: 0.546875, recall: 0.9242424242424242, prec: 0.5350877192982456, f_beta: 0.6777777777777777\n",
      "train: step: 41, loss: 0.693590521812439, acc: 0.4921875, recall: 0.9104477611940298, prec: 0.5083333333333333, f_beta: 0.6524064171122994\n",
      "train: step: 42, loss: 0.6805052757263184, acc: 0.640625, recall: 0.9333333333333333, prec: 0.6306306306306306, f_beta: 0.7526881720430109\n",
      "train: step: 43, loss: 0.681920051574707, acc: 0.6015625, recall: 0.9333333333333333, prec: 0.603448275862069, f_beta: 0.7329842931937173\n",
      "train: step: 44, loss: 0.7034486532211304, acc: 0.5, recall: 0.9242424242424242, prec: 0.5083333333333333, f_beta: 0.6559139784946235\n",
      "train: step: 45, loss: 0.6766879558563232, acc: 0.609375, recall: 0.9230769230769231, prec: 0.6206896551724138, f_beta: 0.7422680412371133\n",
      "train: step: 46, loss: 0.7050614356994629, acc: 0.484375, recall: 0.8709677419354839, prec: 0.48214285714285715, f_beta: 0.6206896551724138\n",
      "train: step: 47, loss: 0.7200030088424683, acc: 0.484375, recall: 0.7297297297297297, prec: 0.54, f_beta: 0.6206896551724138\n",
      "train: step: 48, loss: 0.7107205390930176, acc: 0.4453125, recall: 0.7419354838709677, prec: 0.45544554455445546, f_beta: 0.5644171779141104\n",
      "train: step: 49, loss: 0.7162914872169495, acc: 0.4375, recall: 0.6515151515151515, prec: 0.4673913043478261, f_beta: 0.5443037974683544\n",
      "train: step: 50, loss: 0.6827641725540161, acc: 0.5703125, recall: 0.75, prec: 0.5730337078651685, f_beta: 0.6496815286624205\n",
      "train: step: 51, loss: 0.7037620544433594, acc: 0.5, recall: 0.6774193548387096, prec: 0.4883720930232558, f_beta: 0.5675675675675677\n",
      "train: step: 52, loss: 0.7020295262336731, acc: 0.53125, recall: 0.6376811594202898, prec: 0.5569620253164557, f_beta: 0.5945945945945945\n",
      "train: step: 53, loss: 0.6807407140731812, acc: 0.625, recall: 0.7125, prec: 0.6951219512195121, f_beta: 0.7037037037037036\n",
      "train: step: 54, loss: 0.6933722496032715, acc: 0.5234375, recall: 0.7027027027027027, prec: 0.5714285714285714, f_beta: 0.6303030303030303\n",
      "train: step: 55, loss: 0.7140045166015625, acc: 0.4296875, recall: 0.8545454545454545, prec: 0.41964285714285715, f_beta: 0.562874251497006\n",
      "train: step: 56, loss: 0.6979238986968994, acc: 0.515625, recall: 0.855072463768116, prec: 0.5315315315315315, f_beta: 0.6555555555555556\n",
      "train: step: 57, loss: 0.6991074085235596, acc: 0.5390625, recall: 0.9508196721311475, prec: 0.5087719298245614, f_beta: 0.6628571428571429\n",
      "train: step: 58, loss: 0.7314863204956055, acc: 0.4140625, recall: 0.8103448275862069, prec: 0.42342342342342343, f_beta: 0.5562130177514792\n",
      "train: step: 59, loss: 0.7033929824829102, acc: 0.484375, recall: 0.8524590163934426, prec: 0.47706422018348627, f_beta: 0.611764705882353\n",
      "train: step: 60, loss: 0.702804684638977, acc: 0.4609375, recall: 0.7321428571428571, prec: 0.43157894736842106, f_beta: 0.5430463576158939\n",
      "train: step: 61, loss: 0.7004519701004028, acc: 0.53125, recall: 0.7741935483870968, prec: 0.5106382978723404, f_beta: 0.6153846153846153\n",
      "train: step: 62, loss: 0.6805444955825806, acc: 0.546875, recall: 0.6140350877192983, prec: 0.49295774647887325, f_beta: 0.5468750000000001\n",
      "train: step: 63, loss: 0.7130070924758911, acc: 0.5, recall: 0.3283582089552239, prec: 0.5365853658536586, f_beta: 0.40740740740740744\n",
      "train: step: 64, loss: 0.7076883912086487, acc: 0.5390625, recall: 0.31746031746031744, prec: 0.5555555555555556, f_beta: 0.40404040404040403\n",
      "train: step: 65, loss: 0.7029172778129578, acc: 0.5234375, recall: 0.2833333333333333, prec: 0.4857142857142857, f_beta: 0.35789473684210527\n",
      "train: step: 66, loss: 0.6862597465515137, acc: 0.546875, recall: 0.15789473684210525, prec: 0.47368421052631576, f_beta: 0.23684210526315788\n",
      "train: step: 67, loss: 0.7054722309112549, acc: 0.4921875, recall: 0.046875, prec: 0.42857142857142855, f_beta: 0.08450704225352113\n",
      "train: step: 68, loss: 0.6899943947792053, acc: 0.53125, recall: 0.12903225806451613, prec: 0.5714285714285714, f_beta: 0.2105263157894737\n",
      "train: step: 69, loss: 0.6968896389007568, acc: 0.53125, recall: 0.11475409836065574, prec: 0.5384615384615384, f_beta: 0.1891891891891892\n",
      "train: step: 70, loss: 0.7097294330596924, acc: 0.421875, recall: 0.13043478260869565, prec: 0.391304347826087, f_beta: 0.1956521739130435\n",
      "train: step: 71, loss: 0.7011844515800476, acc: 0.453125, recall: 0.27692307692307694, prec: 0.43902439024390244, f_beta: 0.33962264150943394\n",
      "train: step: 72, loss: 0.7018650770187378, acc: 0.4921875, recall: 0.46153846153846156, prec: 0.5, f_beta: 0.48000000000000004\n",
      "train: step: 73, loss: 0.6916457414627075, acc: 0.515625, recall: 0.6440677966101694, prec: 0.4810126582278481, f_beta: 0.5507246376811594\n",
      "train: step: 74, loss: 0.6849029064178467, acc: 0.546875, recall: 0.7575757575757576, prec: 0.5434782608695652, f_beta: 0.6329113924050633\n",
      "train: step: 75, loss: 0.7089356184005737, acc: 0.53125, recall: 0.7090909090909091, prec: 0.46987951807228917, f_beta: 0.5652173913043479\n",
      "train: step: 76, loss: 0.6767575740814209, acc: 0.5703125, recall: 0.6666666666666666, prec: 0.6075949367088608, f_beta: 0.6357615894039735\n",
      "train: step: 77, loss: 0.6783357858657837, acc: 0.5859375, recall: 0.6271186440677966, prec: 0.5441176470588235, f_beta: 0.5826771653543307\n",
      "train: step: 78, loss: 0.702302873134613, acc: 0.5, recall: 0.46153846153846156, prec: 0.5084745762711864, f_beta: 0.4838709677419355\n",
      "train: step: 79, loss: 0.6982934474945068, acc: 0.5078125, recall: 0.2857142857142857, prec: 0.6060606060606061, f_beta: 0.3883495145631068\n",
      "train: step: 80, loss: 0.7000541687011719, acc: 0.5, recall: 0.1875, prec: 0.5, f_beta: 0.2727272727272727\n",
      "train: step: 81, loss: 0.6848945617675781, acc: 0.5390625, recall: 0.09836065573770492, prec: 0.6, f_beta: 0.16901408450704225\n",
      "train: step: 82, loss: 0.6938113570213318, acc: 0.5078125, recall: 0.06060606060606061, prec: 0.8, f_beta: 0.11267605633802819\n",
      "train: step: 83, loss: 0.7053041458129883, acc: 0.4921875, recall: 0.11764705882352941, prec: 0.6153846153846154, f_beta: 0.19753086419753085\n",
      "train: step: 84, loss: 0.679692268371582, acc: 0.53125, recall: 0.22727272727272727, prec: 0.625, f_beta: 0.3333333333333333\n",
      "train: step: 85, loss: 0.6756199598312378, acc: 0.609375, recall: 0.40625, prec: 0.6842105263157895, f_beta: 0.5098039215686275\n",
      "train: step: 86, loss: 0.668074369430542, acc: 0.625, recall: 0.8, prec: 0.5432098765432098, f_beta: 0.6470588235294118\n",
      "train: step: 87, loss: 0.6717008352279663, acc: 0.625, recall: 0.8571428571428571, prec: 0.5806451612903226, f_beta: 0.6923076923076923\n",
      "train: step: 88, loss: 0.676655113697052, acc: 0.6171875, recall: 0.8253968253968254, prec: 0.5777777777777777, f_beta: 0.6797385620915032\n",
      "train: step: 89, loss: 0.6669222116470337, acc: 0.609375, recall: 0.8103448275862069, prec: 0.5465116279069767, f_beta: 0.6527777777777777\n",
      "train: step: 90, loss: 0.6566488742828369, acc: 0.6328125, recall: 0.6666666666666666, prec: 0.5970149253731343, f_beta: 0.6299212598425197\n",
      "train: step: 91, loss: 0.6552503705024719, acc: 0.578125, recall: 0.288135593220339, prec: 0.5862068965517241, f_beta: 0.38636363636363635\n",
      "train: step: 92, loss: 0.682442307472229, acc: 0.5234375, recall: 0.2857142857142857, prec: 0.5294117647058824, f_beta: 0.37113402061855666\n",
      "train: step: 93, loss: 0.6261693835258484, acc: 0.7109375, recall: 0.7205882352941176, prec: 0.7313432835820896, f_beta: 0.7259259259259259\n",
      "train: step: 94, loss: 0.6467660665512085, acc: 0.6640625, recall: 0.9166666666666666, prec: 0.5913978494623656, f_beta: 0.7189542483660131\n",
      "train: step: 95, loss: 0.6351914405822754, acc: 0.65625, recall: 0.8923076923076924, prec: 0.6105263157894737, f_beta: 0.725\n",
      "train: step: 96, loss: 0.6292800903320312, acc: 0.609375, recall: 0.2878787878787879, prec: 0.8636363636363636, f_beta: 0.4318181818181818\n",
      "train: step: 97, loss: 0.6064150333404541, acc: 0.703125, recall: 0.6268656716417911, prec: 0.7636363636363637, f_beta: 0.6885245901639345\n",
      "train: step: 98, loss: 0.6453101634979248, acc: 0.6328125, recall: 0.9242424242424242, prec: 0.5922330097087378, f_beta: 0.7218934911242603\n",
      "train: step: 99, loss: 0.5524045825004578, acc: 0.7421875, recall: 0.8970588235294118, prec: 0.7011494252873564, f_beta: 0.7870967741935484\n",
      "train: step: 100, loss: 0.5921192169189453, acc: 0.6953125, recall: 0.5147058823529411, prec: 0.8536585365853658, f_beta: 0.6422018348623854\n",
      "start to evaluate...\n",
      "2019-09-25T20:38:30.810041, step: 100, loss: 0.5361844614530221, acc: 0.7443910256410257, precision: 0.8109718985141546, recall: 0.647727192010878, f_beta: 0.7185176600748087\n",
      "train: step: 101, loss: 0.5569536685943604, acc: 0.703125, recall: 0.5735294117647058, prec: 0.8125, f_beta: 0.6724137931034482\n",
      "train: step: 102, loss: 0.5627182126045227, acc: 0.6953125, recall: 0.8918918918918919, prec: 0.6804123711340206, f_beta: 0.7719298245614035\n",
      "train: step: 103, loss: 0.6341012716293335, acc: 0.71875, recall: 0.9814814814814815, prec: 0.6022727272727273, f_beta: 0.7464788732394366\n",
      "train: step: 104, loss: 0.6091899871826172, acc: 0.6484375, recall: 0.52, prec: 0.8125, f_beta: 0.6341463414634146\n",
      "train: step: 105, loss: 0.5164830684661865, acc: 0.7734375, recall: 0.6140350877192983, prec: 0.8333333333333334, f_beta: 0.7070707070707071\n",
      "train: step: 106, loss: 0.4860168993473053, acc: 0.765625, recall: 0.7796610169491526, prec: 0.7301587301587301, f_beta: 0.7540983606557377\n",
      "train: step: 107, loss: 0.5285394191741943, acc: 0.75, recall: 0.765625, prec: 0.7424242424242424, f_beta: 0.7538461538461539\n",
      "train: step: 108, loss: 0.489650696516037, acc: 0.765625, recall: 0.8571428571428571, prec: 0.6857142857142857, f_beta: 0.7619047619047619\n",
      "train: step: 109, loss: 0.5379908680915833, acc: 0.765625, recall: 0.7619047619047619, prec: 0.7619047619047619, f_beta: 0.7619047619047619\n",
      "train: step: 110, loss: 0.4887622594833374, acc: 0.7578125, recall: 0.6153846153846154, prec: 0.8695652173913043, f_beta: 0.7207207207207207\n",
      "train: step: 111, loss: 0.496867299079895, acc: 0.7421875, recall: 0.703125, prec: 0.7627118644067796, f_beta: 0.7317073170731708\n",
      "train: step: 112, loss: 0.4947299063205719, acc: 0.765625, recall: 0.8867924528301887, prec: 0.6619718309859155, f_beta: 0.7580645161290323\n",
      "train: step: 113, loss: 0.47001245617866516, acc: 0.796875, recall: 0.8833333333333333, prec: 0.7361111111111112, f_beta: 0.803030303030303\n",
      "train: step: 114, loss: 0.4469761252403259, acc: 0.796875, recall: 0.7096774193548387, prec: 0.8461538461538461, f_beta: 0.7719298245614036\n",
      "train: step: 115, loss: 0.3971076011657715, acc: 0.875, recall: 0.7878787878787878, prec: 0.9629629629629629, f_beta: 0.8666666666666665\n",
      "train: step: 116, loss: 0.41368505358695984, acc: 0.796875, recall: 0.7647058823529411, prec: 0.8387096774193549, f_beta: 0.7999999999999999\n",
      "train: step: 117, loss: 0.5191890001296997, acc: 0.7734375, recall: 0.9016393442622951, prec: 0.7051282051282052, f_beta: 0.7913669064748202\n",
      "train: step: 118, loss: 0.5250331163406372, acc: 0.7578125, recall: 0.8793103448275862, prec: 0.68, f_beta: 0.7669172932330827\n",
      "train: step: 119, loss: 0.3780232071876526, acc: 0.8515625, recall: 0.8412698412698413, prec: 0.8548387096774194, f_beta: 0.848\n",
      "train: step: 120, loss: 0.4398742914199829, acc: 0.8125, recall: 0.6666666666666666, prec: 0.8837209302325582, f_beta: 0.7599999999999999\n",
      "train: step: 121, loss: 0.43837666511535645, acc: 0.8046875, recall: 0.6949152542372882, prec: 0.8541666666666666, f_beta: 0.7663551401869159\n",
      "train: step: 122, loss: 0.3874339461326599, acc: 0.8359375, recall: 0.9014084507042254, prec: 0.8205128205128205, f_beta: 0.8590604026845639\n",
      "train: step: 123, loss: 0.4213380813598633, acc: 0.8515625, recall: 0.9333333333333333, prec: 0.7887323943661971, f_beta: 0.8549618320610686\n",
      "train: step: 124, loss: 0.3954750597476959, acc: 0.8359375, recall: 0.9838709677419355, prec: 0.7530864197530864, f_beta: 0.8531468531468532\n",
      "train: step: 125, loss: 0.4181869924068451, acc: 0.796875, recall: 0.8260869565217391, prec: 0.8028169014084507, f_beta: 0.8142857142857144\n",
      "train: step: 126, loss: 0.37707293033599854, acc: 0.8203125, recall: 0.7894736842105263, prec: 0.8035714285714286, f_beta: 0.7964601769911505\n",
      "train: step: 127, loss: 0.4548851251602173, acc: 0.8046875, recall: 0.6901408450704225, prec: 0.9423076923076923, f_beta: 0.7967479674796748\n",
      "train: step: 128, loss: 0.4413420855998993, acc: 0.78125, recall: 0.7301587301587301, prec: 0.8070175438596491, f_beta: 0.7666666666666667\n",
      "train: step: 129, loss: 0.3770771026611328, acc: 0.828125, recall: 0.9411764705882353, prec: 0.7164179104477612, f_beta: 0.8135593220338982\n",
      "train: step: 130, loss: 0.42523065209388733, acc: 0.7890625, recall: 0.8387096774193549, prec: 0.7536231884057971, f_beta: 0.7938931297709925\n",
      "train: step: 131, loss: 0.3142716884613037, acc: 0.8984375, recall: 0.9142857142857143, prec: 0.9014084507042254, f_beta: 0.9078014184397163\n",
      "train: step: 132, loss: 0.3988095223903656, acc: 0.84375, recall: 0.84375, prec: 0.84375, f_beta: 0.84375\n",
      "train: step: 133, loss: 0.3949145972728729, acc: 0.84375, recall: 0.8245614035087719, prec: 0.8245614035087719, f_beta: 0.8245614035087719\n",
      "train: step: 134, loss: 0.39756321907043457, acc: 0.8125, recall: 0.8225806451612904, prec: 0.796875, f_beta: 0.8095238095238094\n",
      "train: step: 135, loss: 0.37384533882141113, acc: 0.8515625, recall: 0.8225806451612904, prec: 0.864406779661017, f_beta: 0.8429752066115702\n",
      "train: step: 136, loss: 0.38238394260406494, acc: 0.8359375, recall: 0.8769230769230769, prec: 0.8142857142857143, f_beta: 0.8444444444444444\n",
      "train: step: 137, loss: 0.35504183173179626, acc: 0.8359375, recall: 0.9152542372881356, prec: 0.7714285714285715, f_beta: 0.8372093023255813\n",
      "train: step: 138, loss: 0.32269978523254395, acc: 0.859375, recall: 0.8, prec: 0.8333333333333334, f_beta: 0.816326530612245\n",
      "train: step: 139, loss: 0.35074982047080994, acc: 0.859375, recall: 0.8541666666666666, prec: 0.7884615384615384, f_beta: 0.8200000000000001\n",
      "train: step: 140, loss: 0.540924072265625, acc: 0.7734375, recall: 0.676056338028169, prec: 0.8888888888888888, f_beta: 0.7679999999999999\n",
      "train: step: 141, loss: 0.36138272285461426, acc: 0.84375, recall: 0.8484848484848485, prec: 0.8484848484848485, f_beta: 0.8484848484848486\n",
      "train: step: 142, loss: 0.43048179149627686, acc: 0.8515625, recall: 0.8833333333333333, prec: 0.8153846153846154, f_beta: 0.848\n",
      "train: step: 143, loss: 0.348460853099823, acc: 0.8671875, recall: 0.8985507246376812, prec: 0.8611111111111112, f_beta: 0.8794326241134751\n",
      "train: step: 144, loss: 0.35040414333343506, acc: 0.859375, recall: 0.9333333333333333, prec: 0.8, f_beta: 0.8615384615384616\n",
      "train: step: 145, loss: 0.3694842457771301, acc: 0.8359375, recall: 0.8484848484848485, prec: 0.835820895522388, f_beta: 0.8421052631578948\n",
      "train: step: 146, loss: 0.35801178216934204, acc: 0.8828125, recall: 0.9054054054054054, prec: 0.8933333333333333, f_beta: 0.8993288590604026\n",
      "train: step: 147, loss: 0.29487279057502747, acc: 0.8828125, recall: 0.8356164383561644, prec: 0.953125, f_beta: 0.8905109489051095\n",
      "train: step: 148, loss: 0.3445188105106354, acc: 0.8671875, recall: 0.9, prec: 0.8307692307692308, f_beta: 0.8640000000000001\n",
      "train: step: 149, loss: 0.4505770802497864, acc: 0.796875, recall: 0.8636363636363636, prec: 0.7702702702702703, f_beta: 0.8142857142857143\n",
      "train: step: 150, loss: 0.37339574098587036, acc: 0.8515625, recall: 0.9344262295081968, prec: 0.7916666666666666, f_beta: 0.8571428571428572\n",
      "train: step: 151, loss: 0.36945945024490356, acc: 0.84375, recall: 0.7627118644067796, prec: 0.8823529411764706, f_beta: 0.8181818181818181\n",
      "train: step: 152, loss: 0.3347131609916687, acc: 0.84375, recall: 0.7121212121212122, prec: 0.9791666666666666, f_beta: 0.8245614035087719\n",
      "train: step: 153, loss: 0.3600292205810547, acc: 0.8515625, recall: 0.8028169014084507, prec: 0.9193548387096774, f_beta: 0.8571428571428572\n",
      "train: step: 154, loss: 0.3404904007911682, acc: 0.8515625, recall: 0.9516129032258065, prec: 0.7866666666666666, f_beta: 0.8613138686131386\n",
      "train: step: 155, loss: 0.30670684576034546, acc: 0.8671875, recall: 0.927536231884058, prec: 0.8421052631578947, f_beta: 0.882758620689655\n",
      "train: step: 156, loss: 0.309861421585083, acc: 0.8828125, recall: 0.9705882352941176, prec: 0.8354430379746836, f_beta: 0.8979591836734694\n",
      "start to train model, 1 epoch / epochs 4\n",
      "train: step: 157, loss: 0.2580597400665283, acc: 0.890625, recall: 0.9482758620689655, prec: 0.8333333333333334, f_beta: 0.8870967741935484\n",
      "train: step: 158, loss: 0.2611158788204193, acc: 0.8671875, recall: 0.864406779661017, prec: 0.85, f_beta: 0.8571428571428572\n",
      "train: step: 159, loss: 0.34007009863853455, acc: 0.8828125, recall: 0.8142857142857143, prec: 0.9661016949152542, f_beta: 0.8837209302325583\n",
      "train: step: 160, loss: 0.39492735266685486, acc: 0.828125, recall: 0.7049180327868853, prec: 0.9148936170212766, f_beta: 0.7962962962962963\n",
      "train: step: 161, loss: 0.3235962688922882, acc: 0.8984375, recall: 0.8813559322033898, prec: 0.896551724137931, f_beta: 0.888888888888889\n",
      "train: step: 162, loss: 0.3272148370742798, acc: 0.8828125, recall: 0.9310344827586207, prec: 0.8307692307692308, f_beta: 0.8780487804878049\n",
      "train: step: 163, loss: 0.2748561501502991, acc: 0.890625, recall: 0.9354838709677419, prec: 0.8529411764705882, f_beta: 0.8923076923076922\n",
      "train: step: 164, loss: 0.31430184841156006, acc: 0.8671875, recall: 0.9298245614035088, prec: 0.803030303030303, f_beta: 0.8617886178861788\n",
      "train: step: 165, loss: 0.35948774218559265, acc: 0.8515625, recall: 0.7796610169491526, prec: 0.8846153846153846, f_beta: 0.8288288288288288\n",
      "train: step: 166, loss: 0.2903912365436554, acc: 0.859375, recall: 0.7878787878787878, prec: 0.9285714285714286, f_beta: 0.8524590163934426\n",
      "train: step: 167, loss: 0.3188549876213074, acc: 0.8515625, recall: 0.7681159420289855, prec: 0.9464285714285714, f_beta: 0.848\n",
      "train: step: 168, loss: 0.30487698316574097, acc: 0.8671875, recall: 0.8461538461538461, prec: 0.8870967741935484, f_beta: 0.8661417322834646\n",
      "train: step: 169, loss: 0.21293425559997559, acc: 0.9375, recall: 0.9682539682539683, prec: 0.9104477611940298, f_beta: 0.9384615384615386\n",
      "train: step: 170, loss: 0.2618665099143982, acc: 0.90625, recall: 0.9620253164556962, prec: 0.8941176470588236, f_beta: 0.926829268292683\n",
      "train: step: 171, loss: 0.31642478704452515, acc: 0.890625, recall: 0.9705882352941176, prec: 0.8461538461538461, f_beta: 0.9041095890410958\n",
      "train: step: 172, loss: 0.2642148435115814, acc: 0.890625, recall: 0.9538461538461539, prec: 0.8493150684931506, f_beta: 0.8985507246376813\n",
      "train: step: 173, loss: 0.29187726974487305, acc: 0.8984375, recall: 0.873015873015873, prec: 0.9166666666666666, f_beta: 0.894308943089431\n",
      "train: step: 174, loss: 0.22144702076911926, acc: 0.890625, recall: 0.8428571428571429, prec: 0.9516129032258065, f_beta: 0.8939393939393939\n",
      "train: step: 175, loss: 0.33941450715065, acc: 0.875, recall: 0.8035714285714286, prec: 0.9, f_beta: 0.8490566037735849\n",
      "train: step: 176, loss: 0.22329235076904297, acc: 0.9140625, recall: 0.9, prec: 0.9402985074626866, f_beta: 0.9197080291970803\n",
      "train: step: 177, loss: 0.23182988166809082, acc: 0.9140625, recall: 0.8783783783783784, prec: 0.9701492537313433, f_beta: 0.921985815602837\n",
      "train: step: 178, loss: 0.2958359122276306, acc: 0.8671875, recall: 0.9710144927536232, prec: 0.8170731707317073, f_beta: 0.8874172185430462\n",
      "train: step: 179, loss: 0.39538300037384033, acc: 0.828125, recall: 0.9696969696969697, prec: 0.7619047619047619, f_beta: 0.8533333333333334\n",
      "train: step: 180, loss: 0.3338828980922699, acc: 0.8671875, recall: 0.873015873015873, prec: 0.859375, f_beta: 0.8661417322834646\n",
      "train: step: 181, loss: 0.24793332815170288, acc: 0.8515625, recall: 0.7735849056603774, prec: 0.8541666666666666, f_beta: 0.811881188118812\n",
      "train: step: 182, loss: 0.23470145463943481, acc: 0.890625, recall: 0.8615384615384616, prec: 0.9180327868852459, f_beta: 0.8888888888888888\n",
      "train: step: 183, loss: 0.34481877088546753, acc: 0.828125, recall: 0.75, prec: 0.9107142857142857, f_beta: 0.8225806451612904\n",
      "train: step: 184, loss: 0.25061848759651184, acc: 0.890625, recall: 0.8333333333333334, prec: 0.9259259259259259, f_beta: 0.8771929824561403\n",
      "train: step: 185, loss: 0.2827872633934021, acc: 0.890625, recall: 0.9104477611940298, prec: 0.8840579710144928, f_beta: 0.8970588235294118\n",
      "train: step: 186, loss: 0.2502700686454773, acc: 0.8984375, recall: 0.9692307692307692, prec: 0.8513513513513513, f_beta: 0.9064748201438849\n",
      "train: step: 187, loss: 0.3399091064929962, acc: 0.828125, recall: 0.9402985074626866, prec: 0.7777777777777778, f_beta: 0.8513513513513513\n",
      "train: step: 188, loss: 0.2699434757232666, acc: 0.875, recall: 0.9285714285714286, prec: 0.8552631578947368, f_beta: 0.8904109589041096\n",
      "train: step: 189, loss: 0.2378581017255783, acc: 0.921875, recall: 0.9629629629629629, prec: 0.8666666666666667, f_beta: 0.912280701754386\n",
      "train: step: 190, loss: 0.3093709349632263, acc: 0.8515625, recall: 0.8333333333333334, prec: 0.847457627118644, f_beta: 0.8403361344537815\n",
      "train: step: 191, loss: 0.29284483194351196, acc: 0.8515625, recall: 0.8059701492537313, prec: 0.9, f_beta: 0.8503937007874016\n",
      "train: step: 192, loss: 0.2845347225666046, acc: 0.9140625, recall: 0.8666666666666667, prec: 0.9454545454545454, f_beta: 0.9043478260869566\n",
      "train: step: 193, loss: 0.32009732723236084, acc: 0.8515625, recall: 0.8433734939759037, prec: 0.9210526315789473, f_beta: 0.8805031446540881\n",
      "train: step: 194, loss: 0.26506686210632324, acc: 0.8671875, recall: 0.8225806451612904, prec: 0.8947368421052632, f_beta: 0.8571428571428571\n",
      "train: step: 195, loss: 0.2740597724914551, acc: 0.8984375, recall: 0.9166666666666666, prec: 0.873015873015873, f_beta: 0.894308943089431\n",
      "train: step: 196, loss: 0.26577243208885193, acc: 0.8828125, recall: 0.9375, prec: 0.8450704225352113, f_beta: 0.8888888888888888\n",
      "train: step: 197, loss: 0.2897035479545593, acc: 0.8984375, recall: 0.9354838709677419, prec: 0.8656716417910447, f_beta: 0.8992248062015503\n",
      "train: step: 198, loss: 0.2645827531814575, acc: 0.8828125, recall: 0.9661016949152542, prec: 0.8142857142857143, f_beta: 0.8837209302325583\n",
      "train: step: 199, loss: 0.3036859929561615, acc: 0.8671875, recall: 0.9206349206349206, prec: 0.8285714285714286, f_beta: 0.8721804511278196\n",
      "train: step: 200, loss: 0.2215341478586197, acc: 0.9140625, recall: 0.90625, prec: 0.9206349206349206, f_beta: 0.9133858267716536\n",
      "start to evaluate...\n",
      "2019-09-25T20:54:08.359978, step: 200, loss: 0.3187728982705336, acc: 0.8681891025641025, precision: 0.8885267374375999, recall: 0.8458960063624489, f_beta: 0.8658757865853355\n",
      "train: step: 201, loss: 0.18477439880371094, acc: 0.9296875, recall: 0.9354838709677419, prec: 0.9206349206349206, f_beta: 0.9279999999999999\n",
      "train: step: 202, loss: 0.35367798805236816, acc: 0.859375, recall: 0.7794117647058824, prec: 0.9464285714285714, f_beta: 0.8548387096774193\n",
      "train: step: 203, loss: 0.2736537456512451, acc: 0.8671875, recall: 0.8208955223880597, prec: 0.9166666666666666, f_beta: 0.8661417322834646\n",
      "train: step: 204, loss: 0.36326128244400024, acc: 0.84375, recall: 0.8571428571428571, prec: 0.8571428571428571, f_beta: 0.8571428571428571\n",
      "train: step: 205, loss: 0.29240483045578003, acc: 0.8671875, recall: 0.9714285714285714, prec: 0.8192771084337349, f_beta: 0.8888888888888888\n",
      "train: step: 206, loss: 0.25325700640678406, acc: 0.9140625, recall: 0.9850746268656716, prec: 0.868421052631579, f_beta: 0.923076923076923\n",
      "train: step: 207, loss: 0.3150022625923157, acc: 0.859375, recall: 0.8333333333333334, prec: 0.9090909090909091, f_beta: 0.8695652173913043\n",
      "train: step: 208, loss: 0.2361222505569458, acc: 0.9140625, recall: 0.9206349206349206, prec: 0.90625, f_beta: 0.9133858267716536\n",
      "train: step: 209, loss: 0.19451561570167542, acc: 0.9375, recall: 0.9841269841269841, prec: 0.8985507246376812, f_beta: 0.9393939393939393\n",
      "train: step: 210, loss: 0.2441939413547516, acc: 0.890625, recall: 0.8840579710144928, prec: 0.9104477611940298, f_beta: 0.8970588235294118\n",
      "train: step: 211, loss: 0.27540239691734314, acc: 0.890625, recall: 0.8923076923076924, prec: 0.8923076923076924, f_beta: 0.8923076923076924\n",
      "train: step: 212, loss: 0.39698612689971924, acc: 0.8203125, recall: 0.7868852459016393, prec: 0.8275862068965517, f_beta: 0.8067226890756303\n",
      "train: step: 213, loss: 0.2850593328475952, acc: 0.890625, recall: 0.8767123287671232, prec: 0.927536231884058, f_beta: 0.9014084507042253\n",
      "train: step: 214, loss: 0.22290635108947754, acc: 0.8984375, recall: 0.9, prec: 0.9130434782608695, f_beta: 0.9064748201438848\n",
      "train: step: 215, loss: 0.2606044411659241, acc: 0.90625, recall: 0.9315068493150684, prec: 0.9066666666666666, f_beta: 0.918918918918919\n",
      "train: step: 216, loss: 0.21588575839996338, acc: 0.921875, recall: 0.9661016949152542, prec: 0.8769230769230769, f_beta: 0.9193548387096773\n",
      "train: step: 217, loss: 0.26431238651275635, acc: 0.890625, recall: 0.864406779661017, prec: 0.8947368421052632, f_beta: 0.8793103448275862\n",
      "train: step: 218, loss: 0.23071378469467163, acc: 0.8984375, recall: 0.9193548387096774, prec: 0.8769230769230769, f_beta: 0.8976377952755904\n",
      "train: step: 219, loss: 0.3079022467136383, acc: 0.859375, recall: 0.8727272727272727, prec: 0.8135593220338984, f_beta: 0.8421052631578948\n",
      "train: step: 220, loss: 0.21799063682556152, acc: 0.890625, recall: 0.8333333333333334, prec: 0.9482758620689655, f_beta: 0.8870967741935484\n",
      "train: step: 221, loss: 0.37827327847480774, acc: 0.8671875, recall: 0.8507462686567164, prec: 0.890625, f_beta: 0.8702290076335878\n",
      "train: step: 222, loss: 0.3207063674926758, acc: 0.890625, recall: 0.8714285714285714, prec: 0.9242424242424242, f_beta: 0.8970588235294117\n",
      "train: step: 223, loss: 0.21603551506996155, acc: 0.9375, recall: 0.9736842105263158, prec: 0.925, f_beta: 0.9487179487179489\n",
      "train: step: 224, loss: 0.33154192566871643, acc: 0.8671875, recall: 0.9285714285714286, prec: 0.8441558441558441, f_beta: 0.8843537414965986\n",
      "train: step: 225, loss: 0.23121881484985352, acc: 0.9140625, recall: 0.9491525423728814, prec: 0.875, f_beta: 0.9105691056910569\n",
      "train: step: 226, loss: 0.27363288402557373, acc: 0.8984375, recall: 0.9032258064516129, prec: 0.8888888888888888, f_beta: 0.8959999999999999\n",
      "train: step: 227, loss: 0.17971065640449524, acc: 0.9375, recall: 0.9180327868852459, prec: 0.9491525423728814, f_beta: 0.9333333333333333\n",
      "train: step: 228, loss: 0.30238908529281616, acc: 0.859375, recall: 0.7419354838709677, prec: 0.9583333333333334, f_beta: 0.8363636363636364\n",
      "train: step: 229, loss: 0.30290159583091736, acc: 0.875, recall: 0.8275862068965517, prec: 0.8888888888888888, f_beta: 0.8571428571428572\n",
      "train: step: 230, loss: 0.2508488893508911, acc: 0.90625, recall: 0.9137931034482759, prec: 0.8833333333333333, f_beta: 0.8983050847457628\n",
      "train: step: 231, loss: 0.2975342273712158, acc: 0.875, recall: 0.9107142857142857, prec: 0.8225806451612904, f_beta: 0.864406779661017\n",
      "train: step: 232, loss: 0.23798152804374695, acc: 0.9140625, recall: 0.9402985074626866, prec: 0.9, f_beta: 0.9197080291970803\n",
      "train: step: 233, loss: 0.2675114870071411, acc: 0.890625, recall: 0.8870967741935484, prec: 0.8870967741935484, f_beta: 0.8870967741935484\n",
      "train: step: 234, loss: 0.2595008909702301, acc: 0.921875, recall: 0.9166666666666666, prec: 0.9166666666666666, f_beta: 0.9166666666666666\n",
      "train: step: 235, loss: 0.26730573177337646, acc: 0.90625, recall: 0.9264705882352942, prec: 0.9, f_beta: 0.9130434782608695\n",
      "train: step: 236, loss: 0.21966567635536194, acc: 0.9375, recall: 0.9420289855072463, prec: 0.9420289855072463, f_beta: 0.9420289855072463\n",
      "train: step: 237, loss: 0.24547311663627625, acc: 0.890625, recall: 0.8813559322033898, prec: 0.8813559322033898, f_beta: 0.8813559322033898\n",
      "train: step: 238, loss: 0.23706470429897308, acc: 0.8828125, recall: 0.8484848484848485, prec: 0.9180327868852459, f_beta: 0.8818897637795275\n",
      "train: step: 239, loss: 0.2082626223564148, acc: 0.90625, recall: 0.8805970149253731, prec: 0.9365079365079365, f_beta: 0.9076923076923077\n",
      "train: step: 240, loss: 0.2202514111995697, acc: 0.9140625, recall: 0.8923076923076924, prec: 0.9354838709677419, f_beta: 0.9133858267716536\n",
      "train: step: 241, loss: 0.2901430130004883, acc: 0.8671875, recall: 0.9384615384615385, prec: 0.8243243243243243, f_beta: 0.8776978417266187\n",
      "train: step: 242, loss: 0.16424977779388428, acc: 0.9296875, recall: 0.948051948051948, prec: 0.9358974358974359, f_beta: 0.9419354838709677\n",
      "train: step: 243, loss: 0.21455925703048706, acc: 0.90625, recall: 0.9206349206349206, prec: 0.8923076923076924, f_beta: 0.90625\n",
      "train: step: 244, loss: 0.19114193320274353, acc: 0.9296875, recall: 0.9, prec: 0.9183673469387755, f_beta: 0.9090909090909091\n",
      "train: step: 245, loss: 0.34981778264045715, acc: 0.859375, recall: 0.8615384615384616, prec: 0.8615384615384616, f_beta: 0.8615384615384615\n",
      "train: step: 246, loss: 0.30224788188934326, acc: 0.890625, recall: 0.8484848484848485, prec: 0.9333333333333333, f_beta: 0.888888888888889\n",
      "train: step: 247, loss: 0.3408370614051819, acc: 0.8671875, recall: 0.8524590163934426, prec: 0.8666666666666667, f_beta: 0.8595041322314049\n",
      "train: step: 248, loss: 0.26659005880355835, acc: 0.875, recall: 0.9117647058823529, prec: 0.8611111111111112, f_beta: 0.8857142857142858\n",
      "train: step: 249, loss: 0.2684321105480194, acc: 0.8671875, recall: 0.896551724137931, prec: 0.8253968253968254, f_beta: 0.8595041322314049\n",
      "train: step: 250, loss: 0.24866503477096558, acc: 0.8984375, recall: 0.8909090909090909, prec: 0.875, f_beta: 0.8828828828828829\n",
      "train: step: 251, loss: 0.29286372661590576, acc: 0.90625, recall: 0.8857142857142857, prec: 0.9393939393939394, f_beta: 0.9117647058823529\n",
      "train: step: 252, loss: 0.20275767147541046, acc: 0.9296875, recall: 0.9552238805970149, prec: 0.9142857142857143, f_beta: 0.9343065693430657\n",
      "train: step: 253, loss: 0.2591855525970459, acc: 0.8828125, recall: 0.9354838709677419, prec: 0.8405797101449275, f_beta: 0.8854961832061068\n",
      "train: step: 254, loss: 0.2532883286476135, acc: 0.890625, recall: 0.85, prec: 0.9107142857142857, f_beta: 0.8793103448275861\n",
      "train: step: 255, loss: 0.23427844047546387, acc: 0.8828125, recall: 0.7931034482758621, prec: 0.9387755102040817, f_beta: 0.8598130841121495\n",
      "train: step: 256, loss: 0.28477078676223755, acc: 0.875, recall: 0.8805970149253731, prec: 0.8805970149253731, f_beta: 0.8805970149253731\n",
      "train: step: 257, loss: 0.18344058096408844, acc: 0.9375, recall: 0.9354838709677419, prec: 0.9354838709677419, f_beta: 0.9354838709677419\n",
      "train: step: 258, loss: 0.19413639605045319, acc: 0.9296875, recall: 0.9508196721311475, prec: 0.90625, f_beta: 0.9279999999999999\n",
      "train: step: 259, loss: 0.18258151412010193, acc: 0.953125, recall: 0.9523809523809523, prec: 0.9523809523809523, f_beta: 0.9523809523809523\n",
      "train: step: 260, loss: 0.2956167161464691, acc: 0.890625, recall: 0.9, prec: 0.9, f_beta: 0.9\n",
      "train: step: 261, loss: 0.21999847888946533, acc: 0.90625, recall: 0.9516129032258065, prec: 0.8676470588235294, f_beta: 0.9076923076923077\n",
      "train: step: 262, loss: 0.3162911534309387, acc: 0.8828125, recall: 0.9230769230769231, prec: 0.8571428571428571, f_beta: 0.888888888888889\n",
      "train: step: 263, loss: 0.21418842673301697, acc: 0.8984375, recall: 0.8666666666666667, prec: 0.9122807017543859, f_beta: 0.8888888888888888\n",
      "train: step: 264, loss: 0.25182807445526123, acc: 0.890625, recall: 0.8360655737704918, prec: 0.9272727272727272, f_beta: 0.8793103448275862\n",
      "train: step: 265, loss: 0.2257780283689499, acc: 0.8984375, recall: 0.8909090909090909, prec: 0.875, f_beta: 0.8828828828828829\n",
      "train: step: 266, loss: 0.26134926080703735, acc: 0.921875, recall: 0.921875, prec: 0.921875, f_beta: 0.921875\n",
      "train: step: 267, loss: 0.3253743648529053, acc: 0.890625, recall: 0.8767123287671232, prec: 0.927536231884058, f_beta: 0.9014084507042253\n",
      "train: step: 268, loss: 0.23105348646640778, acc: 0.90625, recall: 0.9423076923076923, prec: 0.8448275862068966, f_beta: 0.890909090909091\n",
      "train: step: 269, loss: 0.19777679443359375, acc: 0.921875, recall: 0.9047619047619048, prec: 0.9344262295081968, f_beta: 0.9193548387096775\n",
      "train: step: 270, loss: 0.3604286313056946, acc: 0.875, recall: 0.890625, prec: 0.8636363636363636, f_beta: 0.8769230769230768\n",
      "train: step: 271, loss: 0.28057900071144104, acc: 0.890625, recall: 0.9016393442622951, prec: 0.873015873015873, f_beta: 0.8870967741935485\n",
      "train: step: 272, loss: 0.20497846603393555, acc: 0.9140625, recall: 0.9130434782608695, prec: 0.9264705882352942, f_beta: 0.9197080291970804\n",
      "train: step: 273, loss: 0.27171429991722107, acc: 0.8984375, recall: 0.896551724137931, prec: 0.8813559322033898, f_beta: 0.888888888888889\n",
      "train: step: 274, loss: 0.24683693051338196, acc: 0.90625, recall: 0.890625, prec: 0.9193548387096774, f_beta: 0.9047619047619047\n",
      "train: step: 275, loss: 0.20494374632835388, acc: 0.9375, recall: 0.95, prec: 0.9193548387096774, f_beta: 0.9344262295081968\n",
      "train: step: 276, loss: 0.35073772072792053, acc: 0.859375, recall: 0.8333333333333334, prec: 0.8870967741935484, f_beta: 0.859375\n",
      "train: step: 277, loss: 0.2418840080499649, acc: 0.8984375, recall: 0.8688524590163934, prec: 0.9137931034482759, f_beta: 0.8907563025210085\n",
      "train: step: 278, loss: 0.3008619546890259, acc: 0.890625, recall: 0.9016393442622951, prec: 0.873015873015873, f_beta: 0.8870967741935485\n",
      "train: step: 279, loss: 0.207817941904068, acc: 0.9296875, recall: 0.921875, prec: 0.9365079365079365, f_beta: 0.9291338582677166\n",
      "train: step: 280, loss: 0.23974813520908356, acc: 0.8828125, recall: 0.875, prec: 0.8888888888888888, f_beta: 0.8818897637795274\n",
      "train: step: 281, loss: 0.20137213170528412, acc: 0.9375, recall: 0.9333333333333333, prec: 0.9333333333333333, f_beta: 0.9333333333333333\n",
      "train: step: 282, loss: 0.21557673811912537, acc: 0.9296875, recall: 0.9615384615384616, prec: 0.8771929824561403, f_beta: 0.9174311926605504\n",
      "train: step: 283, loss: 0.21050885319709778, acc: 0.921875, recall: 0.9090909090909091, prec: 0.9090909090909091, f_beta: 0.9090909090909091\n",
      "train: step: 284, loss: 0.41943204402923584, acc: 0.8515625, recall: 0.84375, prec: 0.8571428571428571, f_beta: 0.8503937007874015\n",
      "train: step: 285, loss: 0.29414063692092896, acc: 0.8828125, recall: 0.9354838709677419, prec: 0.8405797101449275, f_beta: 0.8854961832061068\n",
      "train: step: 286, loss: 0.2171318084001541, acc: 0.9375, recall: 0.9253731343283582, prec: 0.9538461538461539, f_beta: 0.9393939393939394\n",
      "train: step: 287, loss: 0.20892071723937988, acc: 0.90625, recall: 0.8870967741935484, prec: 0.9166666666666666, f_beta: 0.9016393442622951\n",
      "train: step: 288, loss: 0.2506343722343445, acc: 0.9140625, recall: 0.8955223880597015, prec: 0.9375, f_beta: 0.9160305343511451\n",
      "train: step: 289, loss: 0.2192261964082718, acc: 0.890625, recall: 0.85, prec: 0.9107142857142857, f_beta: 0.8793103448275861\n",
      "train: step: 290, loss: 0.22660157084465027, acc: 0.921875, recall: 0.9848484848484849, prec: 0.8783783783783784, f_beta: 0.9285714285714285\n",
      "train: step: 291, loss: 0.1836424320936203, acc: 0.9296875, recall: 0.9166666666666666, prec: 0.9322033898305084, f_beta: 0.9243697478991596\n",
      "train: step: 292, loss: 0.31315213441848755, acc: 0.90625, recall: 0.8955223880597015, prec: 0.9230769230769231, f_beta: 0.9090909090909091\n",
      "train: step: 293, loss: 0.38848984241485596, acc: 0.8125, recall: 0.8771929824561403, prec: 0.746268656716418, f_beta: 0.8064516129032258\n",
      "train: step: 294, loss: 0.281388521194458, acc: 0.875, recall: 0.8873239436619719, prec: 0.8873239436619719, f_beta: 0.8873239436619719\n",
      "train: step: 295, loss: 0.2656180262565613, acc: 0.8984375, recall: 0.9166666666666666, prec: 0.9041095890410958, f_beta: 0.9103448275862068\n",
      "train: step: 296, loss: 0.25264304876327515, acc: 0.875, recall: 0.859375, prec: 0.8870967741935484, f_beta: 0.8730158730158729\n",
      "train: step: 297, loss: 0.22093108296394348, acc: 0.90625, recall: 0.8985507246376812, prec: 0.9253731343283582, f_beta: 0.9117647058823529\n",
      "train: step: 298, loss: 0.35678577423095703, acc: 0.8359375, recall: 0.8, prec: 0.8888888888888888, f_beta: 0.8421052631578948\n",
      "train: step: 299, loss: 0.26751911640167236, acc: 0.8828125, recall: 0.890625, prec: 0.8769230769230769, f_beta: 0.883720930232558\n",
      "train: step: 300, loss: 0.30684059858322144, acc: 0.8828125, recall: 0.9824561403508771, prec: 0.8, f_beta: 0.8818897637795275\n",
      "start to evaluate...\n",
      "2019-09-25T21:11:50.299585, step: 300, loss: 0.29522474912496716, acc: 0.8808092948717948, precision: 0.8530525782456237, recall: 0.9230412939202995, f_beta: 0.8857757733083046\n",
      "train: step: 301, loss: 0.17540302872657776, acc: 0.921875, recall: 0.9726027397260274, prec: 0.8987341772151899, f_beta: 0.9342105263157895\n",
      "train: step: 302, loss: 0.27849864959716797, acc: 0.875, recall: 0.8732394366197183, prec: 0.8985507246376812, f_beta: 0.8857142857142857\n",
      "train: step: 303, loss: 0.24247108399868011, acc: 0.921875, recall: 0.9104477611940298, prec: 0.9384615384615385, f_beta: 0.9242424242424243\n",
      "train: step: 304, loss: 0.18293210864067078, acc: 0.953125, recall: 0.9152542372881356, prec: 0.9818181818181818, f_beta: 0.9473684210526316\n",
      "train: step: 305, loss: 0.25191783905029297, acc: 0.875, recall: 0.8688524590163934, prec: 0.8688524590163934, f_beta: 0.8688524590163934\n",
      "train: step: 306, loss: 0.24842141568660736, acc: 0.8984375, recall: 0.875, prec: 0.9180327868852459, f_beta: 0.8959999999999999\n",
      "train: step: 307, loss: 0.2847883105278015, acc: 0.8828125, recall: 0.875, prec: 0.8596491228070176, f_beta: 0.8672566371681416\n",
      "train: step: 308, loss: 0.19055578112602234, acc: 0.9375, recall: 0.9166666666666666, prec: 0.9482758620689655, f_beta: 0.9322033898305084\n",
      "train: step: 309, loss: 0.22565241158008575, acc: 0.9296875, recall: 0.9375, prec: 0.9230769230769231, f_beta: 0.9302325581395349\n",
      "train: step: 310, loss: 0.2322865128517151, acc: 0.90625, recall: 0.9152542372881356, prec: 0.8852459016393442, f_beta: 0.9\n",
      "train: step: 311, loss: 0.17048296332359314, acc: 0.9296875, recall: 0.9464285714285714, prec: 0.8983050847457628, f_beta: 0.9217391304347826\n",
      "train: step: 312, loss: 0.21631574630737305, acc: 0.90625, recall: 0.9253731343283582, prec: 0.8985507246376812, f_beta: 0.9117647058823529\n",
      "start to train model, 2 epoch / epochs 4\n",
      "train: step: 313, loss: 0.12334048748016357, acc: 0.9609375, recall: 0.9433962264150944, prec: 0.9615384615384616, f_beta: 0.9523809523809524\n",
      "train: step: 314, loss: 0.09220337867736816, acc: 0.984375, recall: 1.0, prec: 0.9692307692307692, f_beta: 0.9843749999999999\n",
      "train: step: 315, loss: 0.11956191062927246, acc: 0.9609375, recall: 0.9230769230769231, prec: 0.9795918367346939, f_beta: 0.9504950495049506\n",
      "train: step: 316, loss: 0.1764899492263794, acc: 0.9375, recall: 0.9117647058823529, prec: 0.96875, f_beta: 0.9393939393939394\n",
      "train: step: 317, loss: 0.2638181447982788, acc: 0.90625, recall: 0.9137931034482759, prec: 0.8833333333333333, f_beta: 0.8983050847457628\n",
      "train: step: 318, loss: 0.23210418224334717, acc: 0.9140625, recall: 0.8805970149253731, prec: 0.9516129032258065, f_beta: 0.9147286821705426\n",
      "train: step: 319, loss: 0.12405848503112793, acc: 0.9453125, recall: 0.9811320754716981, prec: 0.896551724137931, f_beta: 0.9369369369369369\n",
      "train: step: 320, loss: 0.14211532473564148, acc: 0.953125, recall: 0.9841269841269841, prec: 0.9253731343283582, f_beta: 0.9538461538461538\n",
      "train: step: 321, loss: 0.15381047129631042, acc: 0.9453125, recall: 0.9859154929577465, prec: 0.9210526315789473, f_beta: 0.9523809523809524\n",
      "train: step: 322, loss: 0.18602219223976135, acc: 0.9296875, recall: 0.9166666666666666, prec: 0.9565217391304348, f_beta: 0.9361702127659574\n",
      "train: step: 323, loss: 0.10782225430011749, acc: 0.96875, recall: 0.9333333333333333, prec: 1.0, f_beta: 0.9655172413793104\n",
      "train: step: 324, loss: 0.066136434674263, acc: 0.9765625, recall: 1.0, prec: 0.9583333333333334, f_beta: 0.9787234042553191\n",
      "train: step: 325, loss: 0.11080121248960495, acc: 0.9453125, recall: 0.9629629629629629, prec: 0.9122807017543859, f_beta: 0.9369369369369369\n",
      "train: step: 326, loss: 0.21192768216133118, acc: 0.9453125, recall: 0.9393939393939394, prec: 0.9538461538461539, f_beta: 0.9465648854961831\n",
      "train: step: 327, loss: 0.1410951167345047, acc: 0.953125, recall: 0.9629629629629629, prec: 0.9285714285714286, f_beta: 0.9454545454545454\n",
      "train: step: 328, loss: 0.2313685566186905, acc: 0.9453125, recall: 0.9411764705882353, prec: 0.9552238805970149, f_beta: 0.9481481481481482\n",
      "train: step: 329, loss: 0.3498603105545044, acc: 0.8828125, recall: 0.8923076923076924, prec: 0.8787878787878788, f_beta: 0.8854961832061069\n",
      "train: step: 330, loss: 0.17722150683403015, acc: 0.9453125, recall: 0.9428571428571428, prec: 0.9565217391304348, f_beta: 0.9496402877697843\n",
      "train: step: 331, loss: 0.09508584439754486, acc: 0.9609375, recall: 0.9493670886075949, prec: 0.9868421052631579, f_beta: 0.967741935483871\n",
      "train: step: 332, loss: 0.1506904661655426, acc: 0.9375, recall: 0.8852459016393442, prec: 0.9818181818181818, f_beta: 0.9310344827586207\n",
      "train: step: 333, loss: 0.16300427913665771, acc: 0.953125, recall: 0.984375, prec: 0.9264705882352942, f_beta: 0.9545454545454545\n",
      "train: step: 334, loss: 0.20606011152267456, acc: 0.953125, recall: 0.9848484848484849, prec: 0.9285714285714286, f_beta: 0.9558823529411765\n",
      "train: step: 335, loss: 0.09727233648300171, acc: 0.96875, recall: 0.9666666666666667, prec: 0.9666666666666667, f_beta: 0.9666666666666667\n",
      "train: step: 336, loss: 0.15413141250610352, acc: 0.9453125, recall: 0.9076923076923077, prec: 0.9833333333333333, f_beta: 0.944\n",
      "train: step: 337, loss: 0.11347785592079163, acc: 0.96875, recall: 0.9420289855072463, prec: 1.0, f_beta: 0.9701492537313433\n",
      "train: step: 338, loss: 0.15925438702106476, acc: 0.9453125, recall: 0.9384615384615385, prec: 0.953125, f_beta: 0.9457364341085271\n",
      "train: step: 339, loss: 0.10639059543609619, acc: 0.96875, recall: 0.9444444444444444, prec: 1.0, f_beta: 0.9714285714285714\n",
      "train: step: 340, loss: 0.11618965119123459, acc: 0.9453125, recall: 0.9508196721311475, prec: 0.9354838709677419, f_beta: 0.943089430894309\n",
      "train: step: 341, loss: 0.11596576869487762, acc: 0.9609375, recall: 0.9523809523809523, prec: 0.967741935483871, f_beta: 0.96\n",
      "train: step: 342, loss: 0.11767937988042831, acc: 0.9609375, recall: 0.9833333333333333, prec: 0.9365079365079365, f_beta: 0.9593495934959351\n",
      "train: step: 343, loss: 0.13109150528907776, acc: 0.9453125, recall: 0.9393939393939394, prec: 0.9538461538461539, f_beta: 0.9465648854961831\n",
      "train: step: 344, loss: 0.15580564737319946, acc: 0.9375, recall: 0.9365079365079365, prec: 0.9365079365079365, f_beta: 0.9365079365079365\n",
      "train: step: 345, loss: 0.1456107497215271, acc: 0.9296875, recall: 0.9571428571428572, prec: 0.9178082191780822, f_beta: 0.9370629370629371\n",
      "train: step: 346, loss: 0.118392214179039, acc: 0.96875, recall: 0.9538461538461539, prec: 0.9841269841269841, f_beta: 0.96875\n",
      "train: step: 347, loss: 0.1673669070005417, acc: 0.9453125, recall: 0.9365079365079365, prec: 0.9516129032258065, f_beta: 0.944\n",
      "train: step: 348, loss: 0.14088653028011322, acc: 0.953125, recall: 0.9538461538461539, prec: 0.9538461538461539, f_beta: 0.9538461538461539\n",
      "train: step: 349, loss: 0.11810310930013657, acc: 0.9609375, recall: 0.9375, prec: 0.9836065573770492, f_beta: 0.96\n",
      "train: step: 350, loss: 0.15932466089725494, acc: 0.9375, recall: 0.9523809523809523, prec: 0.9230769230769231, f_beta: 0.9375\n",
      "train: step: 351, loss: 0.19718897342681885, acc: 0.9296875, recall: 0.95, prec: 0.9047619047619048, f_beta: 0.9268292682926829\n",
      "train: step: 352, loss: 0.0968947485089302, acc: 0.9609375, recall: 0.9743589743589743, prec: 0.9620253164556962, f_beta: 0.9681528662420382\n",
      "train: step: 353, loss: 0.14426812529563904, acc: 0.9609375, recall: 0.9565217391304348, prec: 0.9705882352941176, f_beta: 0.9635036496350365\n",
      "train: step: 354, loss: 0.131570965051651, acc: 0.9375, recall: 0.9344262295081968, prec: 0.9344262295081968, f_beta: 0.9344262295081968\n",
      "train: step: 355, loss: 0.13545110821723938, acc: 0.9765625, recall: 0.9701492537313433, prec: 0.9848484848484849, f_beta: 0.9774436090225564\n",
      "train: step: 356, loss: 0.1440732777118683, acc: 0.953125, recall: 0.9375, prec: 0.967741935483871, f_beta: 0.9523809523809523\n",
      "train: step: 357, loss: 0.06145390495657921, acc: 0.984375, recall: 0.9838709677419355, prec: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 358, loss: 0.09292816370725632, acc: 0.9609375, recall: 0.9821428571428571, prec: 0.9322033898305084, f_beta: 0.9565217391304348\n",
      "train: step: 359, loss: 0.10346105694770813, acc: 0.9765625, recall: 1.0, prec: 0.9523809523809523, f_beta: 0.975609756097561\n",
      "train: step: 360, loss: 0.19134140014648438, acc: 0.9375, recall: 0.9508196721311475, prec: 0.9206349206349206, f_beta: 0.9354838709677418\n",
      "train: step: 361, loss: 0.1722005307674408, acc: 0.9453125, recall: 0.9436619718309859, prec: 0.9571428571428572, f_beta: 0.9503546099290779\n",
      "train: step: 362, loss: 0.0961230993270874, acc: 0.9765625, recall: 0.984375, prec: 0.9692307692307692, f_beta: 0.9767441860465116\n",
      "train: step: 363, loss: 0.14864948391914368, acc: 0.9453125, recall: 0.9315068493150684, prec: 0.9714285714285714, f_beta: 0.9510489510489512\n",
      "train: step: 364, loss: 0.11554545164108276, acc: 0.9609375, recall: 0.9545454545454546, prec: 0.9692307692307692, f_beta: 0.9618320610687022\n",
      "train: step: 365, loss: 0.1946844905614853, acc: 0.9453125, recall: 0.9666666666666667, prec: 0.9206349206349206, f_beta: 0.943089430894309\n",
      "train: step: 366, loss: 0.0732644721865654, acc: 0.984375, recall: 0.9852941176470589, prec: 0.9852941176470589, f_beta: 0.9852941176470589\n",
      "train: step: 367, loss: 0.14878278970718384, acc: 0.9375, recall: 0.9855072463768116, prec: 0.9066666666666666, f_beta: 0.9444444444444444\n",
      "train: step: 368, loss: 0.18645846843719482, acc: 0.9375, recall: 0.9672131147540983, prec: 0.9076923076923077, f_beta: 0.9365079365079365\n",
      "train: step: 369, loss: 0.14686384797096252, acc: 0.9375, recall: 0.9242424242424242, prec: 0.953125, f_beta: 0.9384615384615383\n",
      "train: step: 370, loss: 0.2597700357437134, acc: 0.8984375, recall: 0.864406779661017, prec: 0.9107142857142857, f_beta: 0.8869565217391304\n",
      "train: step: 371, loss: 0.19330254197120667, acc: 0.9296875, recall: 0.8679245283018868, prec: 0.9583333333333334, f_beta: 0.910891089108911\n",
      "train: step: 372, loss: 0.18249459564685822, acc: 0.9453125, recall: 0.9423076923076923, prec: 0.9245283018867925, f_beta: 0.9333333333333333\n",
      "train: step: 373, loss: 0.20224067568778992, acc: 0.9375, recall: 0.9824561403508771, prec: 0.8888888888888888, f_beta: 0.9333333333333333\n",
      "train: step: 374, loss: 0.15346696972846985, acc: 0.9609375, recall: 0.9705882352941176, prec: 0.9565217391304348, f_beta: 0.9635036496350365\n",
      "train: step: 375, loss: 0.09122990071773529, acc: 0.984375, recall: 0.971830985915493, prec: 1.0, f_beta: 0.9857142857142858\n",
      "train: step: 376, loss: 0.1304987072944641, acc: 0.953125, recall: 0.9333333333333333, prec: 0.9655172413793104, f_beta: 0.9491525423728815\n",
      "train: step: 377, loss: 0.11953248083591461, acc: 0.9453125, recall: 0.9384615384615385, prec: 0.953125, f_beta: 0.9457364341085271\n",
      "train: step: 378, loss: 0.14512744545936584, acc: 0.9296875, recall: 0.9375, prec: 0.9230769230769231, f_beta: 0.9302325581395349\n",
      "train: step: 379, loss: 0.13262467086315155, acc: 0.9453125, recall: 0.9180327868852459, prec: 0.9655172413793104, f_beta: 0.9411764705882353\n",
      "train: step: 380, loss: 0.15160590410232544, acc: 0.9453125, recall: 0.9666666666666667, prec: 0.9206349206349206, f_beta: 0.943089430894309\n",
      "train: step: 381, loss: 0.1542794555425644, acc: 0.9453125, recall: 0.9420289855072463, prec: 0.9558823529411765, f_beta: 0.9489051094890512\n",
      "train: step: 382, loss: 0.11721838265657425, acc: 0.96875, recall: 0.9852941176470589, prec: 0.9571428571428572, f_beta: 0.9710144927536232\n",
      "train: step: 383, loss: 0.14561191201210022, acc: 0.9375, recall: 0.9672131147540983, prec: 0.9076923076923077, f_beta: 0.9365079365079365\n",
      "train: step: 384, loss: 0.08584987372159958, acc: 0.9765625, recall: 1.0, prec: 0.9538461538461539, f_beta: 0.9763779527559054\n",
      "train: step: 385, loss: 0.12928076088428497, acc: 0.9609375, recall: 0.9722222222222222, prec: 0.958904109589041, f_beta: 0.9655172413793104\n",
      "train: step: 386, loss: 0.21856093406677246, acc: 0.953125, recall: 0.9444444444444444, prec: 0.9444444444444444, f_beta: 0.9444444444444444\n",
      "train: step: 387, loss: 0.07965219020843506, acc: 0.984375, recall: 1.0, prec: 0.9642857142857143, f_beta: 0.9818181818181818\n",
      "train: step: 388, loss: 0.2261931598186493, acc: 0.9296875, recall: 0.9076923076923077, prec: 0.9516129032258065, f_beta: 0.9291338582677167\n",
      "train: step: 389, loss: 0.1673334687948227, acc: 0.921875, recall: 0.8852459016393442, prec: 0.9473684210526315, f_beta: 0.9152542372881356\n",
      "train: step: 390, loss: 0.24417302012443542, acc: 0.921875, recall: 0.9117647058823529, prec: 0.9393939393939394, f_beta: 0.9253731343283583\n",
      "train: step: 391, loss: 0.179268479347229, acc: 0.921875, recall: 0.9285714285714286, prec: 0.9285714285714286, f_beta: 0.9285714285714286\n",
      "train: step: 392, loss: 0.13253401219844818, acc: 0.953125, recall: 0.9538461538461539, prec: 0.9538461538461539, f_beta: 0.9538461538461539\n",
      "train: step: 393, loss: 0.1961536407470703, acc: 0.9296875, recall: 0.9016393442622951, prec: 0.9482758620689655, f_beta: 0.9243697478991596\n",
      "train: step: 394, loss: 0.19369997084140778, acc: 0.9296875, recall: 0.984375, prec: 0.8873239436619719, f_beta: 0.9333333333333333\n",
      "train: step: 395, loss: 0.1328686773777008, acc: 0.96875, recall: 1.0, prec: 0.9402985074626866, f_beta: 0.9692307692307692\n",
      "train: step: 396, loss: 0.15866060554981232, acc: 0.9453125, recall: 0.9444444444444444, prec: 0.9577464788732394, f_beta: 0.951048951048951\n",
      "train: step: 397, loss: 0.1622745841741562, acc: 0.9453125, recall: 1.0, prec: 0.8939393939393939, f_beta: 0.944\n",
      "train: step: 398, loss: 0.1405068039894104, acc: 0.953125, recall: 0.9583333333333334, prec: 0.9583333333333334, f_beta: 0.9583333333333334\n",
      "train: step: 399, loss: 0.15294748544692993, acc: 0.9609375, recall: 0.9310344827586207, prec: 0.9818181818181818, f_beta: 0.9557522123893805\n",
      "train: step: 400, loss: 0.1114569753408432, acc: 0.96875, recall: 0.9272727272727272, prec: 1.0, f_beta: 0.9622641509433962\n",
      "start to evaluate...\n",
      "2019-09-25T21:27:55.803915, step: 400, loss: 0.3304500564550742, acc: 0.8685897435897436, precision: 0.9139930970445965, recall: 0.8163318604606733, f_beta: 0.8613918635856097\n",
      "train: step: 401, loss: 0.08515100181102753, acc: 0.9765625, recall: 0.9682539682539683, prec: 0.9838709677419355, f_beta: 0.976\n",
      "train: step: 402, loss: 0.22596192359924316, acc: 0.8984375, recall: 0.8135593220338984, prec: 0.96, f_beta: 0.8807339449541285\n",
      "train: step: 403, loss: 0.19197526574134827, acc: 0.9375, recall: 0.9305555555555556, prec: 0.9571428571428572, f_beta: 0.943661971830986\n",
      "train: step: 404, loss: 0.08578219264745712, acc: 0.96875, recall: 0.9848484848484849, prec: 0.9558823529411765, f_beta: 0.9701492537313432\n",
      "train: step: 405, loss: 0.21676219999790192, acc: 0.9140625, recall: 0.9736842105263158, prec: 0.891566265060241, f_beta: 0.930817610062893\n",
      "train: step: 406, loss: 0.16086873412132263, acc: 0.9375, recall: 0.9696969696969697, prec: 0.9142857142857143, f_beta: 0.9411764705882354\n",
      "train: step: 407, loss: 0.14433234930038452, acc: 0.9609375, recall: 1.0, prec: 0.9230769230769231, f_beta: 0.9600000000000001\n",
      "train: step: 408, loss: 0.1633872389793396, acc: 0.953125, recall: 0.9516129032258065, prec: 0.9516129032258065, f_beta: 0.9516129032258065\n",
      "train: step: 409, loss: 0.20154719054698944, acc: 0.921875, recall: 0.8873239436619719, prec: 0.9692307692307692, f_beta: 0.9264705882352942\n",
      "train: step: 410, loss: 0.22912898659706116, acc: 0.90625, recall: 0.8333333333333334, prec: 0.9821428571428571, f_beta: 0.9016393442622951\n",
      "train: step: 411, loss: 0.20965707302093506, acc: 0.90625, recall: 0.8405797101449275, prec: 0.9830508474576272, f_beta: 0.9062499999999999\n",
      "train: step: 412, loss: 0.26702433824539185, acc: 0.9140625, recall: 0.9298245614035088, prec: 0.8833333333333333, f_beta: 0.905982905982906\n",
      "train: step: 413, loss: 0.23137125372886658, acc: 0.875, recall: 0.9552238805970149, prec: 0.8311688311688312, f_beta: 0.888888888888889\n",
      "train: step: 414, loss: 0.203851580619812, acc: 0.9296875, recall: 1.0, prec: 0.8714285714285714, f_beta: 0.9312977099236641\n",
      "train: step: 415, loss: 0.18633292615413666, acc: 0.953125, recall: 0.96875, prec: 0.9393939393939394, f_beta: 0.9538461538461539\n",
      "train: step: 416, loss: 0.16392500698566437, acc: 0.9296875, recall: 0.8985507246376812, prec: 0.96875, f_beta: 0.9323308270676692\n",
      "train: step: 417, loss: 0.17440173029899597, acc: 0.9375, recall: 0.9130434782608695, prec: 0.9692307692307692, f_beta: 0.9402985074626865\n",
      "train: step: 418, loss: 0.11716454476118088, acc: 0.953125, recall: 0.9152542372881356, prec: 0.9818181818181818, f_beta: 0.9473684210526316\n",
      "train: step: 419, loss: 0.12776347994804382, acc: 0.9453125, recall: 0.9523809523809523, prec: 0.9375, f_beta: 0.9448818897637795\n",
      "train: step: 420, loss: 0.13742949068546295, acc: 0.9375, recall: 0.9552238805970149, prec: 0.927536231884058, f_beta: 0.9411764705882353\n",
      "train: step: 421, loss: 0.15843240916728973, acc: 0.9453125, recall: 0.9555555555555556, prec: 0.8958333333333334, f_beta: 0.924731182795699\n",
      "train: step: 422, loss: 0.11541706323623657, acc: 0.953125, recall: 0.9692307692307692, prec: 0.9402985074626866, f_beta: 0.9545454545454547\n",
      "train: step: 423, loss: 0.11079269647598267, acc: 0.953125, recall: 0.9253731343283582, prec: 0.9841269841269841, f_beta: 0.9538461538461538\n",
      "train: step: 424, loss: 0.1222914606332779, acc: 0.9765625, recall: 0.9841269841269841, prec: 0.96875, f_beta: 0.9763779527559054\n",
      "train: step: 425, loss: 0.10628275573253632, acc: 0.96875, recall: 0.953125, prec: 0.9838709677419355, f_beta: 0.9682539682539683\n",
      "train: step: 426, loss: 0.134281188249588, acc: 0.953125, recall: 0.9545454545454546, prec: 0.9545454545454546, f_beta: 0.9545454545454546\n",
      "train: step: 427, loss: 0.11308041960000992, acc: 0.953125, recall: 1.0, prec: 0.9090909090909091, f_beta: 0.9523809523809523\n",
      "train: step: 428, loss: 0.06958767026662827, acc: 0.9765625, recall: 0.9714285714285714, prec: 0.9855072463768116, f_beta: 0.9784172661870504\n",
      "train: step: 429, loss: 0.12916597723960876, acc: 0.953125, recall: 0.9545454545454546, prec: 0.9545454545454546, f_beta: 0.9545454545454546\n",
      "train: step: 430, loss: 0.0994168370962143, acc: 0.9765625, recall: 0.9743589743589743, prec: 0.987012987012987, f_beta: 0.9806451612903225\n",
      "train: step: 431, loss: 0.0859939455986023, acc: 0.984375, recall: 0.9836065573770492, prec: 0.9836065573770492, f_beta: 0.9836065573770492\n",
      "train: step: 432, loss: 0.08877754211425781, acc: 0.9765625, recall: 0.9649122807017544, prec: 0.9821428571428571, f_beta: 0.9734513274336283\n",
      "train: step: 433, loss: 0.09930168092250824, acc: 0.984375, recall: 0.967741935483871, prec: 1.0, f_beta: 0.9836065573770492\n",
      "train: step: 434, loss: 0.06358538568019867, acc: 0.9765625, recall: 0.9661016949152542, prec: 0.9827586206896551, f_beta: 0.9743589743589743\n",
      "train: step: 435, loss: 0.31797003746032715, acc: 0.90625, recall: 0.8363636363636363, prec: 0.9387755102040817, f_beta: 0.8846153846153846\n",
      "train: step: 436, loss: 0.06370106339454651, acc: 0.9765625, recall: 0.967741935483871, prec: 0.9836065573770492, f_beta: 0.975609756097561\n",
      "train: step: 437, loss: 0.2471364140510559, acc: 0.921875, recall: 0.9836065573770492, prec: 0.8695652173913043, f_beta: 0.923076923076923\n",
      "train: step: 438, loss: 0.13394081592559814, acc: 0.953125, recall: 0.9428571428571428, prec: 0.9705882352941176, f_beta: 0.9565217391304348\n",
      "train: step: 439, loss: 0.1532074213027954, acc: 0.96875, recall: 0.9848484848484849, prec: 0.9558823529411765, f_beta: 0.9701492537313432\n",
      "train: step: 440, loss: 0.17379644513130188, acc: 0.953125, recall: 0.9714285714285714, prec: 0.9444444444444444, f_beta: 0.9577464788732395\n",
      "train: step: 441, loss: 0.10666489601135254, acc: 0.9609375, recall: 0.9672131147540983, prec: 0.9516129032258065, f_beta: 0.959349593495935\n",
      "train: step: 442, loss: 0.14633163809776306, acc: 0.9375, recall: 0.8679245283018868, prec: 0.9787234042553191, f_beta: 0.9199999999999999\n",
      "train: step: 443, loss: 0.16725385189056396, acc: 0.9296875, recall: 0.9193548387096774, prec: 0.9344262295081968, f_beta: 0.9268292682926829\n",
      "train: step: 444, loss: 0.14831872284412384, acc: 0.9453125, recall: 0.8867924528301887, prec: 0.9791666666666666, f_beta: 0.9306930693069307\n",
      "train: step: 445, loss: 0.261114239692688, acc: 0.921875, recall: 0.875, prec: 0.9423076923076923, f_beta: 0.9074074074074073\n",
      "train: step: 446, loss: 0.13223205506801605, acc: 0.9453125, recall: 0.9846153846153847, prec: 0.9142857142857143, f_beta: 0.9481481481481482\n",
      "train: step: 447, loss: 0.09144015610218048, acc: 0.96875, recall: 0.9855072463768116, prec: 0.9577464788732394, f_beta: 0.9714285714285714\n",
      "train: step: 448, loss: 0.1911650449037552, acc: 0.9453125, recall: 0.9859154929577465, prec: 0.9210526315789473, f_beta: 0.9523809523809524\n",
      "train: step: 449, loss: 0.19637702405452728, acc: 0.9140625, recall: 0.9193548387096774, prec: 0.9047619047619048, f_beta: 0.912\n",
      "train: step: 450, loss: 0.16063472628593445, acc: 0.9375, recall: 0.9558823529411765, prec: 0.9285714285714286, f_beta: 0.9420289855072465\n",
      "train: step: 451, loss: 0.1569780856370926, acc: 0.9375, recall: 0.9661016949152542, prec: 0.9047619047619048, f_beta: 0.9344262295081968\n",
      "train: step: 452, loss: 0.2064935714006424, acc: 0.9375, recall: 0.8939393939393939, prec: 0.9833333333333333, f_beta: 0.9365079365079364\n",
      "train: step: 453, loss: 0.19027207791805267, acc: 0.90625, recall: 0.8305084745762712, prec: 0.9607843137254902, f_beta: 0.890909090909091\n",
      "train: step: 454, loss: 0.17680707573890686, acc: 0.921875, recall: 0.8888888888888888, prec: 0.9491525423728814, f_beta: 0.9180327868852458\n",
      "train: step: 455, loss: 0.13442757725715637, acc: 0.9453125, recall: 0.9354838709677419, prec: 0.9508196721311475, f_beta: 0.943089430894309\n",
      "train: step: 456, loss: 0.1832796037197113, acc: 0.921875, recall: 0.9393939393939394, prec: 0.9117647058823529, f_beta: 0.9253731343283583\n",
      "train: step: 457, loss: 0.13217675685882568, acc: 0.9609375, recall: 1.0, prec: 0.9285714285714286, f_beta: 0.962962962962963\n",
      "train: step: 458, loss: 0.18723013997077942, acc: 0.9453125, recall: 1.0, prec: 0.8923076923076924, f_beta: 0.9430894308943091\n",
      "train: step: 459, loss: 0.1810353845357895, acc: 0.9375, recall: 0.9464285714285714, prec: 0.9137931034482759, f_beta: 0.9298245614035087\n",
      "train: step: 460, loss: 0.18471476435661316, acc: 0.9140625, recall: 0.8676470588235294, prec: 0.9672131147540983, f_beta: 0.9147286821705426\n",
      "train: step: 461, loss: 0.11992625147104263, acc: 0.9609375, recall: 0.9428571428571428, prec: 0.9850746268656716, f_beta: 0.9635036496350364\n",
      "train: step: 462, loss: 0.18211981654167175, acc: 0.9375, recall: 0.9242424242424242, prec: 0.953125, f_beta: 0.9384615384615383\n",
      "train: step: 463, loss: 0.20418623089790344, acc: 0.90625, recall: 0.8656716417910447, prec: 0.9508196721311475, f_beta: 0.9062499999999999\n",
      "train: step: 464, loss: 0.1998685896396637, acc: 0.9375, recall: 0.9411764705882353, prec: 0.9411764705882353, f_beta: 0.9411764705882353\n",
      "train: step: 465, loss: 0.17620502412319183, acc: 0.953125, recall: 0.984375, prec: 0.9264705882352942, f_beta: 0.9545454545454545\n",
      "train: step: 466, loss: 0.12881740927696228, acc: 0.953125, recall: 0.9850746268656716, prec: 0.9295774647887324, f_beta: 0.9565217391304348\n",
      "train: step: 467, loss: 0.2485131323337555, acc: 0.9140625, recall: 0.9354838709677419, prec: 0.8923076923076924, f_beta: 0.9133858267716536\n",
      "train: step: 468, loss: 0.09147148579359055, acc: 0.9609375, recall: 0.9705882352941176, prec: 0.9565217391304348, f_beta: 0.9635036496350365\n",
      "start to train model, 3 epoch / epochs 4\n",
      "train: step: 469, loss: 0.04937872290611267, acc: 1.0, recall: 1.0, prec: 1.0, f_beta: 1.0\n",
      "train: step: 470, loss: 0.05679765343666077, acc: 0.984375, recall: 0.9827586206896551, prec: 0.9827586206896551, f_beta: 0.9827586206896551\n",
      "train: step: 471, loss: 0.07422059774398804, acc: 0.9765625, recall: 0.9846153846153847, prec: 0.9696969696969697, f_beta: 0.9770992366412214\n",
      "train: step: 472, loss: 0.1471337080001831, acc: 0.953125, recall: 0.9264705882352942, prec: 0.984375, f_beta: 0.9545454545454545\n",
      "train: step: 473, loss: 0.04204954952001572, acc: 0.9921875, recall: 0.9830508474576272, prec: 1.0, f_beta: 0.9914529914529915\n",
      "train: step: 474, loss: 0.03995080292224884, acc: 1.0, recall: 1.0, prec: 1.0, f_beta: 1.0\n",
      "train: step: 475, loss: 0.03039509616792202, acc: 1.0, recall: 1.0, prec: 1.0, f_beta: 1.0\n",
      "train: step: 476, loss: 0.03874644637107849, acc: 1.0, recall: 1.0, prec: 1.0, f_beta: 1.0\n",
      "train: step: 477, loss: 0.061384543776512146, acc: 0.984375, recall: 0.9850746268656716, prec: 0.9850746268656716, f_beta: 0.9850746268656716\n",
      "train: step: 478, loss: 0.03540411591529846, acc: 1.0, recall: 1.0, prec: 1.0, f_beta: 1.0\n",
      "train: step: 479, loss: 0.12146703153848648, acc: 0.96875, recall: 0.9846153846153847, prec: 0.9552238805970149, f_beta: 0.9696969696969696\n",
      "train: step: 480, loss: 0.06292416900396347, acc: 0.9921875, recall: 1.0, prec: 0.9848484848484849, f_beta: 0.9923664122137404\n",
      "train: step: 481, loss: 0.0708300769329071, acc: 0.9765625, recall: 0.9852941176470589, prec: 0.9710144927536232, f_beta: 0.9781021897810219\n",
      "train: step: 482, loss: 0.08295907080173492, acc: 0.984375, recall: 0.9833333333333333, prec: 0.9833333333333333, f_beta: 0.9833333333333333\n",
      "train: step: 483, loss: 0.11492259800434113, acc: 0.9765625, recall: 0.9705882352941176, prec: 0.9850746268656716, f_beta: 0.9777777777777777\n",
      "train: step: 484, loss: 0.05962364375591278, acc: 0.984375, recall: 0.9666666666666667, prec: 1.0, f_beta: 0.983050847457627\n",
      "train: step: 485, loss: 0.06873228400945663, acc: 0.9765625, recall: 0.9696969696969697, prec: 0.9846153846153847, f_beta: 0.9770992366412214\n",
      "train: step: 486, loss: 0.035602912306785583, acc: 0.984375, recall: 0.9710144927536232, prec: 1.0, f_beta: 0.9852941176470589\n",
      "train: step: 487, loss: 0.043670546263456345, acc: 0.9921875, recall: 1.0, prec: 0.9830508474576272, f_beta: 0.9914529914529915\n",
      "train: step: 488, loss: 0.083754763007164, acc: 0.9765625, recall: 1.0, prec: 0.9538461538461539, f_beta: 0.9763779527559054\n",
      "train: step: 489, loss: 0.038690224289894104, acc: 0.984375, recall: 0.9841269841269841, prec: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 490, loss: 0.01835937798023224, acc: 1.0, recall: 1.0, prec: 1.0, f_beta: 1.0\n",
      "train: step: 491, loss: 0.051040951162576675, acc: 0.9765625, recall: 0.9473684210526315, prec: 1.0, f_beta: 0.972972972972973\n",
      "train: step: 492, loss: 0.037489429116249084, acc: 0.9921875, recall: 1.0, prec: 0.9848484848484849, f_beta: 0.9923664122137404\n",
      "train: step: 493, loss: 0.029642459005117416, acc: 0.9921875, recall: 1.0, prec: 0.9838709677419355, f_beta: 0.991869918699187\n",
      "train: step: 494, loss: 0.12160035967826843, acc: 0.96875, recall: 0.9846153846153847, prec: 0.9552238805970149, f_beta: 0.9696969696969696\n",
      "train: step: 495, loss: 0.06575916707515717, acc: 0.984375, recall: 0.984375, prec: 0.984375, f_beta: 0.984375\n",
      "train: step: 496, loss: 0.03693143278360367, acc: 0.9921875, recall: 0.9859154929577465, prec: 1.0, f_beta: 0.9929078014184397\n",
      "train: step: 497, loss: 0.0497102253139019, acc: 0.984375, recall: 0.9846153846153847, prec: 0.9846153846153847, f_beta: 0.9846153846153847\n",
      "train: step: 498, loss: 0.020705021917819977, acc: 0.9921875, recall: 0.9846153846153847, prec: 1.0, f_beta: 0.9922480620155039\n",
      "train: step: 499, loss: 0.027095776051282883, acc: 0.9921875, recall: 0.9852941176470589, prec: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 500, loss: 0.031815625727176666, acc: 0.9765625, recall: 0.984375, prec: 0.9692307692307692, f_beta: 0.9767441860465116\n",
      "start to evaluate...\n",
      "2019-09-25T21:43:35.612688, step: 500, loss: 0.47354040084741056, acc: 0.8778044871794872, precision: 0.8682585502250291, recall: 0.8930226435184616, f_beta: 0.8797891851434317\n",
      "train: step: 501, loss: 0.02881050482392311, acc: 0.9921875, recall: 1.0, prec: 0.9859154929577465, f_beta: 0.9929078014184397\n",
      "train: step: 502, loss: 0.019559061154723167, acc: 0.9921875, recall: 0.9846153846153847, prec: 1.0, f_beta: 0.9922480620155039\n",
      "train: step: 503, loss: 0.06676645576953888, acc: 0.984375, recall: 1.0, prec: 0.9714285714285714, f_beta: 0.9855072463768115\n",
      "train: step: 504, loss: 0.05369345471262932, acc: 0.9921875, recall: 0.9846153846153847, prec: 1.0, f_beta: 0.9922480620155039\n",
      "train: step: 505, loss: 0.01683869957923889, acc: 0.9921875, recall: 0.9852941176470589, prec: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 506, loss: 0.028295887634158134, acc: 0.9921875, recall: 0.9861111111111112, prec: 1.0, f_beta: 0.993006993006993\n",
      "train: step: 507, loss: 0.0915614441037178, acc: 0.9609375, recall: 0.9393939393939394, prec: 0.9841269841269841, f_beta: 0.9612403100775193\n",
      "train: step: 508, loss: 0.06488209962844849, acc: 0.984375, recall: 0.984375, prec: 0.984375, f_beta: 0.984375\n",
      "train: step: 509, loss: 0.12760800123214722, acc: 0.984375, recall: 0.9852941176470589, prec: 0.9852941176470589, f_beta: 0.9852941176470589\n",
      "train: step: 510, loss: 0.06789525598287582, acc: 0.984375, recall: 1.0, prec: 0.9705882352941176, f_beta: 0.9850746268656716\n",
      "train: step: 511, loss: 0.08894851058721542, acc: 0.9609375, recall: 0.9850746268656716, prec: 0.9428571428571428, f_beta: 0.9635036496350364\n",
      "train: step: 512, loss: 0.05297211930155754, acc: 0.9921875, recall: 0.9863013698630136, prec: 1.0, f_beta: 0.993103448275862\n",
      "train: step: 513, loss: 0.08101807534694672, acc: 0.96875, recall: 0.9672131147540983, prec: 0.9672131147540983, f_beta: 0.9672131147540983\n",
      "train: step: 514, loss: 0.11589887738227844, acc: 0.96875, recall: 0.9545454545454546, prec: 0.984375, f_beta: 0.9692307692307692\n",
      "train: step: 515, loss: 0.06605397164821625, acc: 0.9765625, recall: 0.9552238805970149, prec: 1.0, f_beta: 0.9770992366412213\n",
      "train: step: 516, loss: 0.019979719072580338, acc: 0.9921875, recall: 0.9827586206896551, prec: 1.0, f_beta: 0.9913043478260869\n",
      "train: step: 517, loss: 0.030257247388362885, acc: 0.984375, recall: 1.0, prec: 0.9696969696969697, f_beta: 0.9846153846153847\n",
      "train: step: 518, loss: 0.08759614825248718, acc: 0.984375, recall: 0.9682539682539683, prec: 1.0, f_beta: 0.9838709677419354\n",
      "train: step: 519, loss: 0.05544799938797951, acc: 0.9765625, recall: 1.0, prec: 0.9508196721311475, f_beta: 0.9747899159663865\n",
      "train: step: 520, loss: 0.06991828978061676, acc: 0.9765625, recall: 0.9841269841269841, prec: 0.96875, f_beta: 0.9763779527559054\n",
      "train: step: 521, loss: 0.093854621052742, acc: 0.9765625, recall: 0.9552238805970149, prec: 1.0, f_beta: 0.9770992366412213\n",
      "train: step: 522, loss: 0.03011377342045307, acc: 0.984375, recall: 1.0, prec: 0.9714285714285714, f_beta: 0.9855072463768115\n",
      "train: step: 523, loss: 0.040603820234537125, acc: 0.984375, recall: 1.0, prec: 0.9733333333333334, f_beta: 0.9864864864864865\n",
      "train: step: 524, loss: 0.036075495183467865, acc: 0.9921875, recall: 0.9863013698630136, prec: 1.0, f_beta: 0.993103448275862\n",
      "train: step: 525, loss: 0.13463418185710907, acc: 0.9609375, recall: 0.9655172413793104, prec: 0.9491525423728814, f_beta: 0.9572649572649573\n",
      "train: step: 526, loss: 0.10498054325580597, acc: 0.96875, recall: 0.9692307692307692, prec: 0.9692307692307692, f_beta: 0.9692307692307692\n",
      "train: step: 527, loss: 0.033404141664505005, acc: 0.9921875, recall: 0.9861111111111112, prec: 1.0, f_beta: 0.993006993006993\n",
      "train: step: 528, loss: 0.1097428947687149, acc: 0.9765625, recall: 0.9538461538461539, prec: 1.0, f_beta: 0.9763779527559054\n",
      "train: step: 529, loss: 0.02338287979364395, acc: 1.0, recall: 1.0, prec: 1.0, f_beta: 1.0\n",
      "train: step: 530, loss: 0.06337577849626541, acc: 0.9765625, recall: 0.9827586206896551, prec: 0.9661016949152542, f_beta: 0.9743589743589743\n",
      "train: step: 531, loss: 0.08939032256603241, acc: 0.96875, recall: 1.0, prec: 0.9436619718309859, f_beta: 0.9710144927536231\n",
      "train: step: 532, loss: 0.07093749940395355, acc: 0.96875, recall: 0.9859154929577465, prec: 0.958904109589041, f_beta: 0.9722222222222222\n",
      "train: step: 533, loss: 0.06295917928218842, acc: 0.9765625, recall: 0.9571428571428572, prec: 1.0, f_beta: 0.9781021897810218\n",
      "train: step: 534, loss: 0.04085245355963707, acc: 0.9765625, recall: 0.9433962264150944, prec: 1.0, f_beta: 0.970873786407767\n",
      "train: step: 535, loss: 0.08894915878772736, acc: 0.9765625, recall: 0.9655172413793104, prec: 0.9824561403508771, f_beta: 0.9739130434782608\n",
      "train: step: 536, loss: 0.06112835928797722, acc: 0.984375, recall: 0.9846153846153847, prec: 0.9846153846153847, f_beta: 0.9846153846153847\n",
      "train: step: 537, loss: 0.19089320302009583, acc: 0.953125, recall: 0.9642857142857143, prec: 0.9310344827586207, f_beta: 0.9473684210526316\n",
      "train: step: 538, loss: 0.07047619670629501, acc: 0.9765625, recall: 1.0, prec: 0.9552238805970149, f_beta: 0.9770992366412213\n",
      "train: step: 539, loss: 0.12017809599637985, acc: 0.9609375, recall: 0.9672131147540983, prec: 0.9516129032258065, f_beta: 0.959349593495935\n",
      "train: step: 540, loss: 0.07441996783018112, acc: 0.9765625, recall: 0.9636363636363636, prec: 0.9814814814814815, f_beta: 0.9724770642201834\n",
      "train: step: 541, loss: 0.043559834361076355, acc: 0.984375, recall: 1.0, prec: 0.96875, f_beta: 0.9841269841269841\n",
      "train: step: 542, loss: 0.11114006489515305, acc: 0.953125, recall: 0.9666666666666667, prec: 0.9354838709677419, f_beta: 0.9508196721311476\n",
      "train: step: 543, loss: 0.05739287659525871, acc: 0.96875, recall: 0.9375, prec: 1.0, f_beta: 0.967741935483871\n",
      "train: step: 544, loss: 0.03331133723258972, acc: 0.9921875, recall: 1.0, prec: 0.9848484848484849, f_beta: 0.9923664122137404\n",
      "train: step: 545, loss: 0.08130507916212082, acc: 0.96875, recall: 0.9365079365079365, prec: 1.0, f_beta: 0.9672131147540983\n",
      "train: step: 546, loss: 0.06816521286964417, acc: 0.9921875, recall: 1.0, prec: 0.9824561403508771, f_beta: 0.9911504424778761\n",
      "train: step: 547, loss: 0.024343151599168777, acc: 0.9921875, recall: 1.0, prec: 0.9814814814814815, f_beta: 0.9906542056074767\n",
      "train: step: 548, loss: 0.042176589369773865, acc: 1.0, recall: 1.0, prec: 1.0, f_beta: 1.0\n",
      "train: step: 549, loss: 0.04364098235964775, acc: 0.984375, recall: 0.984375, prec: 0.984375, f_beta: 0.984375\n",
      "train: step: 550, loss: 0.11364154517650604, acc: 0.9765625, recall: 0.9861111111111112, prec: 0.9726027397260274, f_beta: 0.9793103448275863\n",
      "train: step: 551, loss: 0.03052327036857605, acc: 0.9921875, recall: 1.0, prec: 0.9882352941176471, f_beta: 0.9940828402366864\n",
      "train: step: 552, loss: 0.06258749216794968, acc: 0.96875, recall: 0.9861111111111112, prec: 0.9594594594594594, f_beta: 0.9726027397260274\n",
      "train: step: 553, loss: 0.0882117822766304, acc: 0.96875, recall: 0.9577464788732394, prec: 0.9855072463768116, f_beta: 0.9714285714285714\n",
      "train: step: 554, loss: 0.052121568471193314, acc: 0.984375, recall: 0.9850746268656716, prec: 0.9850746268656716, f_beta: 0.9850746268656716\n",
      "train: step: 555, loss: 0.02396305277943611, acc: 0.9921875, recall: 1.0, prec: 0.9846153846153847, f_beta: 0.9922480620155039\n",
      "train: step: 556, loss: 0.036716487258672714, acc: 0.984375, recall: 0.9830508474576272, prec: 0.9830508474576272, f_beta: 0.9830508474576272\n",
      "train: step: 557, loss: 0.033401478081941605, acc: 0.984375, recall: 0.9818181818181818, prec: 0.9818181818181818, f_beta: 0.9818181818181818\n",
      "train: step: 558, loss: 0.07390798628330231, acc: 0.96875, recall: 0.9552238805970149, prec: 0.9846153846153847, f_beta: 0.9696969696969696\n",
      "train: step: 559, loss: 0.08871043473482132, acc: 0.953125, recall: 0.9538461538461539, prec: 0.9538461538461539, f_beta: 0.9538461538461539\n",
      "train: step: 560, loss: 0.03771907836198807, acc: 0.984375, recall: 1.0, prec: 0.9636363636363636, f_beta: 0.9814814814814815\n",
      "train: step: 561, loss: 0.07837268710136414, acc: 0.96875, recall: 0.9824561403508771, prec: 0.9491525423728814, f_beta: 0.9655172413793103\n",
      "train: step: 562, loss: 0.06583452969789505, acc: 0.9765625, recall: 0.9672131147540983, prec: 0.9833333333333333, f_beta: 0.9752066115702478\n",
      "train: step: 563, loss: 0.044222522526979446, acc: 0.9765625, recall: 0.9565217391304348, prec: 1.0, f_beta: 0.9777777777777777\n",
      "train: step: 564, loss: 0.18516117334365845, acc: 0.9375, recall: 0.8793103448275862, prec: 0.9807692307692307, f_beta: 0.9272727272727272\n",
      "train: step: 565, loss: 0.06666359305381775, acc: 0.984375, recall: 0.9833333333333333, prec: 0.9833333333333333, f_beta: 0.9833333333333333\n",
      "train: step: 566, loss: 0.051962800323963165, acc: 0.9765625, recall: 0.984375, prec: 0.9692307692307692, f_beta: 0.9767441860465116\n",
      "train: step: 567, loss: 0.13512639701366425, acc: 0.96875, recall: 0.9672131147540983, prec: 0.9672131147540983, f_beta: 0.9672131147540983\n",
      "train: step: 568, loss: 0.06763607263565063, acc: 0.9765625, recall: 0.9864864864864865, prec: 0.9733333333333334, f_beta: 0.9798657718120806\n",
      "train: step: 569, loss: 0.1389237344264984, acc: 0.953125, recall: 0.9838709677419355, prec: 0.9242424242424242, f_beta: 0.9531249999999999\n",
      "train: step: 570, loss: 0.07289130240678787, acc: 0.96875, recall: 0.9538461538461539, prec: 0.9841269841269841, f_beta: 0.96875\n",
      "train: step: 571, loss: 0.0813436359167099, acc: 0.9765625, recall: 0.971830985915493, prec: 0.9857142857142858, f_beta: 0.9787234042553192\n",
      "train: step: 572, loss: 0.03484528139233589, acc: 0.9921875, recall: 0.9859154929577465, prec: 1.0, f_beta: 0.9929078014184397\n",
      "train: step: 573, loss: 0.058054178953170776, acc: 0.984375, recall: 0.9841269841269841, prec: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 574, loss: 0.02671767771244049, acc: 0.9921875, recall: 1.0, prec: 0.9824561403508771, f_beta: 0.9911504424778761\n",
      "train: step: 575, loss: 0.04689192771911621, acc: 0.9921875, recall: 0.9807692307692307, prec: 1.0, f_beta: 0.9902912621359222\n",
      "train: step: 576, loss: 0.08599895238876343, acc: 0.9765625, recall: 0.9811320754716981, prec: 0.9629629629629629, f_beta: 0.9719626168224299\n",
      "train: step: 577, loss: 0.023320995271205902, acc: 0.9921875, recall: 0.9846153846153847, prec: 1.0, f_beta: 0.9922480620155039\n",
      "train: step: 578, loss: 0.09258066862821579, acc: 0.96875, recall: 0.9850746268656716, prec: 0.9565217391304348, f_beta: 0.9705882352941176\n",
      "train: step: 579, loss: 0.11925386637449265, acc: 0.96875, recall: 1.0, prec: 0.9420289855072463, f_beta: 0.9701492537313433\n",
      "train: step: 580, loss: 0.054541803896427155, acc: 0.9765625, recall: 0.9672131147540983, prec: 0.9833333333333333, f_beta: 0.9752066115702478\n",
      "train: step: 581, loss: 0.09443821012973785, acc: 0.9609375, recall: 0.9714285714285714, prec: 0.9577464788732394, f_beta: 0.9645390070921985\n",
      "train: step: 582, loss: 0.08958014845848083, acc: 0.9609375, recall: 0.9393939393939394, prec: 0.9841269841269841, f_beta: 0.9612403100775193\n",
      "train: step: 583, loss: 0.16433194279670715, acc: 0.953125, recall: 0.9253731343283582, prec: 0.9841269841269841, f_beta: 0.9538461538461538\n",
      "train: step: 584, loss: 0.07908247411251068, acc: 0.9921875, recall: 0.9852941176470589, prec: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 585, loss: 0.049912966787815094, acc: 0.96875, recall: 1.0, prec: 0.9466666666666667, f_beta: 0.9726027397260273\n",
      "train: step: 586, loss: 0.098084956407547, acc: 0.9453125, recall: 0.9696969696969697, prec: 0.927536231884058, f_beta: 0.9481481481481481\n",
      "train: step: 587, loss: 0.03387203440070152, acc: 0.9921875, recall: 1.0, prec: 0.9848484848484849, f_beta: 0.9923664122137404\n",
      "train: step: 588, loss: 0.0806187093257904, acc: 0.984375, recall: 0.9824561403508771, prec: 0.9824561403508771, f_beta: 0.9824561403508771\n",
      "train: step: 589, loss: 0.11751455068588257, acc: 0.9765625, recall: 0.9666666666666667, prec: 0.9830508474576272, f_beta: 0.9747899159663865\n",
      "train: step: 590, loss: 0.13670319318771362, acc: 0.9609375, recall: 0.9107142857142857, prec: 1.0, f_beta: 0.9532710280373832\n",
      "train: step: 591, loss: 0.06521572172641754, acc: 0.9921875, recall: 0.9821428571428571, prec: 1.0, f_beta: 0.9909909909909909\n",
      "train: step: 592, loss: 0.10534000396728516, acc: 0.9765625, recall: 0.96, prec: 1.0, f_beta: 0.9795918367346939\n",
      "train: step: 593, loss: 0.10526926815509796, acc: 0.96875, recall: 0.9841269841269841, prec: 0.9538461538461539, f_beta: 0.96875\n",
      "train: step: 594, loss: 0.07287438958883286, acc: 0.9609375, recall: 1.0, prec: 0.9230769230769231, f_beta: 0.9600000000000001\n",
      "train: step: 595, loss: 0.06469578295946121, acc: 0.984375, recall: 1.0, prec: 0.9710144927536232, f_beta: 0.9852941176470589\n",
      "train: step: 596, loss: 0.06563190370798111, acc: 0.984375, recall: 0.984375, prec: 0.984375, f_beta: 0.984375\n",
      "train: step: 597, loss: 0.07883602380752563, acc: 0.96875, recall: 0.9622641509433962, prec: 0.9622641509433962, f_beta: 0.9622641509433962\n",
      "train: step: 598, loss: 0.10464588552713394, acc: 0.9609375, recall: 0.9545454545454546, prec: 0.9692307692307692, f_beta: 0.9618320610687022\n",
      "train: step: 599, loss: 0.10561367869377136, acc: 0.9609375, recall: 0.9375, prec: 0.9836065573770492, f_beta: 0.96\n",
      "train: step: 600, loss: 0.11596598476171494, acc: 0.9609375, recall: 0.9846153846153847, prec: 0.9411764705882353, f_beta: 0.962406015037594\n",
      "start to evaluate...\n",
      "2019-09-25T21:58:56.575211, step: 600, loss: 0.4281402948575142, acc: 0.8703926282051282, precision: 0.8809359982167768, recall: 0.859940947779829, f_beta: 0.8691783580457325\n",
      "train: step: 601, loss: 0.044338613748550415, acc: 0.984375, recall: 0.9863013698630136, prec: 0.9863013698630136, f_beta: 0.9863013698630136\n",
      "train: step: 602, loss: 0.05292944610118866, acc: 0.984375, recall: 1.0, prec: 0.9722222222222222, f_beta: 0.9859154929577464\n",
      "train: step: 603, loss: 0.08106234669685364, acc: 0.96875, recall: 1.0, prec: 0.9411764705882353, f_beta: 0.9696969696969697\n",
      "train: step: 604, loss: 0.05301215872168541, acc: 0.96875, recall: 0.9846153846153847, prec: 0.9552238805970149, f_beta: 0.9696969696969696\n",
      "train: step: 605, loss: 0.06402131915092468, acc: 0.96875, recall: 1.0, prec: 0.9365079365079365, f_beta: 0.9672131147540983\n",
      "train: step: 606, loss: 0.13027378916740417, acc: 0.9609375, recall: 0.9310344827586207, prec: 0.9818181818181818, f_beta: 0.9557522123893805\n",
      "train: step: 607, loss: 0.09399468451738358, acc: 0.96875, recall: 0.9354838709677419, prec: 1.0, f_beta: 0.9666666666666666\n",
      "train: step: 608, loss: 0.10592946410179138, acc: 0.96875, recall: 0.9365079365079365, prec: 1.0, f_beta: 0.9672131147540983\n",
      "train: step: 609, loss: 0.07825813442468643, acc: 0.9765625, recall: 0.9411764705882353, prec: 1.0, f_beta: 0.9696969696969697\n",
      "train: step: 610, loss: 0.043876901268959045, acc: 0.984375, recall: 1.0, prec: 0.9655172413793104, f_beta: 0.9824561403508771\n",
      "train: step: 611, loss: 0.054756857454776764, acc: 0.984375, recall: 1.0, prec: 0.9672131147540983, f_beta: 0.9833333333333333\n",
      "train: step: 612, loss: 0.07757309824228287, acc: 0.9765625, recall: 0.9864864864864865, prec: 0.9733333333333334, f_beta: 0.9798657718120806\n",
      "train: step: 613, loss: 0.07786905765533447, acc: 0.9765625, recall: 0.9818181818181818, prec: 0.9642857142857143, f_beta: 0.972972972972973\n",
      "train: step: 614, loss: 0.06623721867799759, acc: 0.96875, recall: 1.0, prec: 0.9344262295081968, f_beta: 0.9661016949152543\n",
      "train: step: 615, loss: 0.18694323301315308, acc: 0.9375, recall: 0.9054054054054054, prec: 0.9852941176470589, f_beta: 0.943661971830986\n",
      "train: step: 616, loss: 0.07449281215667725, acc: 0.984375, recall: 0.9682539682539683, prec: 1.0, f_beta: 0.9838709677419354\n",
      "train: step: 617, loss: 0.04220417141914368, acc: 0.9921875, recall: 1.0, prec: 0.9836065573770492, f_beta: 0.9917355371900827\n",
      "train: step: 618, loss: 0.03725876659154892, acc: 0.984375, recall: 1.0, prec: 0.9692307692307692, f_beta: 0.9843749999999999\n",
      "train: step: 619, loss: 0.0690336525440216, acc: 0.96875, recall: 1.0, prec: 0.9365079365079365, f_beta: 0.9672131147540983\n",
      "train: step: 620, loss: 0.07453599572181702, acc: 0.984375, recall: 0.9705882352941176, prec: 1.0, f_beta: 0.9850746268656716\n",
      "train: step: 621, loss: 0.1024133563041687, acc: 0.9609375, recall: 0.9636363636363636, prec: 0.9464285714285714, f_beta: 0.9549549549549549\n",
      "train: step: 622, loss: 0.05116228386759758, acc: 0.9765625, recall: 0.984375, prec: 0.9692307692307692, f_beta: 0.9767441860465116\n",
      "train: step: 623, loss: 0.08011598140001297, acc: 0.984375, recall: 0.9814814814814815, prec: 0.9814814814814815, f_beta: 0.9814814814814815\n",
      "train: step: 624, loss: 0.07822465896606445, acc: 0.96875, recall: 0.9402985074626866, prec: 1.0, f_beta: 0.9692307692307692\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=sess_config)\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTMAttention(config, wordEmbedding)\n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def trainStep(batchX, batchY):\n",
    "        feed_dict = {\n",
    "            lstm.inputX: batchX, \n",
    "            lstm.inputY: batchY, \n",
    "            lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "        }\n",
    "        _, step, loss, predictions = sess.run([trainOp, globalStep, lstm.loss, lstm.predictions], feed_dict)\n",
    "        timeStr = datetime.datetime.now().isoformat()\n",
    "        \n",
    "        if config.numClasses == 1:\n",
    "            acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "        elif config.numClasses > 1:\n",
    "            acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "        \n",
    "        return loss, acc, prec, recall, f_beta\n",
    "    \n",
    "    \n",
    "    def devStep(batchX, batchY):\n",
    "        feed_dict = {\n",
    "            lstm.inputX: batchX, \n",
    "            lstm.inputY: batchY, \n",
    "            lstm.dropoutKeepProb: 1.0\n",
    "        }\n",
    "        step, loss, predictions = sess.run([globalStep, lstm.loss, lstm.predictions], feed_dict)\n",
    "        if config.numClasses == 1:\n",
    "            acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "        elif config.numClasses > 1:\n",
    "            acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "        \n",
    "        return loss, acc, prec, recall, f_beta\n",
    "    \n",
    "    \n",
    "    for index in range(config.training.epochs):\n",
    "        print(\"start to train model, {} epoch / epochs {}\".format(str(index), str(config.training.epochs)))\n",
    "        for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "            loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "            currentStep = tf.train.global_step(sess, globalStep)\n",
    "            print(\"train: step: {}, loss: {}, acc: {}, recall: {}, prec: {}, f_beta: {}\".format(\n",
    "                currentStep, loss, acc, recall, prec, f_beta))\n",
    "            \n",
    "            if currentStep % config.training.evaluateEvery == 0:\n",
    "                print(\"start to evaluate...\")\n",
    "                losses, accs, precisions, recalls, f_betas = [], [], [], [], []\n",
    "                for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                    loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
    "                    losses.append(loss)\n",
    "                    accs.append(acc)\n",
    "                    precisions.append(precision)\n",
    "                    recalls.append(recall)\n",
    "                    f_betas.append(f_beta)\n",
    "                \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}, step: {}, loss: {}, acc: {}, precision: {}, recall: {}, f_beta: {}\".format(\n",
    "                    time_str, currentStep, mean(losses), mean(accs), mean(precisions), mean(recalls), mean(f_betas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
