{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os, csv, time, datetime, random, json\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:-1]) + \"\\\\data\"\n",
    "os.path.exists(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 超参配置\n",
    "class TrainingConfig(object):\n",
    "    epochs = 5\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate= 0.001\n",
    "\n",
    "\n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    numFilters = 128\n",
    "    filterSizes = [2, 3, 4, 5]\n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200\n",
    "    batchSize = 128\n",
    "    dataSource = directory_path + \"\\\\preProcess\\\\labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = directory_path + \"\\\\english\"\n",
    "    \n",
    "    # 二分类时设置为 1 ，对分类时设置为其他数字\n",
    "    numClasses = 1\n",
    "    rate = 0.8         # 训练集占比\n",
    "    training = TrainingConfig()\n",
    "    model = ModelConfig()\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Config at 0x1eeef8b5358>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:94: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource\n",
    "        self._sequenceLength = config.sequenceLength\n",
    "        \n",
    "        \n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        \n",
    "        self._stopWordDict = dict()\n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        self.wordEmbedding = None\n",
    "        self.labelList = []\n",
    "        \n",
    "    def _readData(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if self.config.numClasses == 1:\n",
    "            labels = df[\"sentiment\"].tolist()\n",
    "        elif self.config.numClasses > 1:\n",
    "            labels = df[\"rate\"].tolist()\n",
    "        \n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "        \n",
    "        return reviews, labels\n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        \"\"\"\n",
    "            本函数旨在将类别标签转换成索引号表示\n",
    "        \"\"\"\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        \n",
    "        return labelIds\n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        \"\"\"\n",
    "            将词汇转换成为索引\n",
    "        \"\"\"\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "    \n",
    "    \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        reviews = []\n",
    "        for review in x:\n",
    "            if len(review) >= self._sequenceLength:\n",
    "                # contruncate\n",
    "                reviews.append(review[: self._sequenceLength])\n",
    "            else:\n",
    "                # PADDING\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "        \n",
    "        trainIndex = int(len(x) * rate)\n",
    "\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[: trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[: trainIndex], dtype=\"float32\")\n",
    "        evalReviews = np.asarray(reviews[trainIndex :], dtype=\"int64\")\n",
    "        evalLabels = np.array(y[trainIndex : ], dtype=\"float32\")\n",
    "        \n",
    "        \n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "    \n",
    "    \n",
    "    def _readStopWord(self, stop_word_path):\n",
    "        with open(stop_word_path, \"r\") as f:\n",
    "            stop_words = f.read()\n",
    "            stop_word_list = stop_words.splitlines()\n",
    "            self.stopWordDict = dict(zip(stop_word_list, list(range(len(stop_word_list)))))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _getWordEmbedding(self, words):\n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(directory_path + \"\\\\word2vec\\\\word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 新增 \"UNK\" 和 \"PAD\"\n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        \n",
    "        # “PAD”为全0，\"UNK\"为随机初始化值\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(\"{} is not exist in vocabulary...\".format(word))\n",
    "        \n",
    "        \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    \n",
    "    def _genVocabulary(self, reviews, labels):\n",
    "        \"\"\"\n",
    "            生成词汇向量、以及建立 词汇-索引 映射字典\n",
    "        \"\"\"\n",
    "        # 去掉停用词哦\n",
    "        all_words = [word for review in reviews for word in review]\n",
    "        subWords = [word for word in all_words if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]     # 过滤低频词汇\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        with open(directory_path + \"\\\\wordJson\\\\word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        \n",
    "        with open(directory_path + \"\\\\wordJson\\\\label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "    \n",
    "    \n",
    "    def dataGen(self):\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
    "        \n",
    "        # 将标签与句子数据化\n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "    \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training data shape:  (20000, 200)\n",
      "the training label shape:  (20000,)\n",
      "the evaluate data shape:  (5000, 200)\n",
      "the evaluate label shape:  (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"the training data shape: \", data.trainReviews.shape)\n",
    "print(\"the training label shape: \", data.trainLabels.shape)\n",
    "print(\"the evaluate data shape: \", data.evalReviews.shape)\n",
    "print(\"the evaluate label shape: \", data.evalLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextBatch(x, y, batchSize):\n",
    "    perm = np.arange(len(x))\n",
    "    np.random.shuffle(perm)\n",
    "    x = x[perm]\n",
    "    y = y[perm]\n",
    "    \n",
    "    num_batches = len(x) // batchSize\n",
    "    for index in range(num_batches):\n",
    "        start = index * batchSize\n",
    "        end = start + batchSize\n",
    "        batchX = np.array(x[start : end], dtype=\"int64\")\n",
    "        batchY = np.array(y[start : end], dtype=\"float32\")\n",
    "    \n",
    "        yield batchX, batchY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        l2Loss = tf.constant(0.0)\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            self.embeddedWordsExpanded = tf.expand_dims(self.embeddedWords, -1)\n",
    "        \n",
    "        pooledOutputs = []\n",
    "        # 卷积池化层：有三种 size 大小的filter，分别为3、4、5；TextCNN是个多通道的单层卷积model，可当作是三个单层的卷积model的融合\n",
    "        for index, filterSize in enumerate(config.model.filterSizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filterSize):\n",
    "                # 卷积层，size为filterSize * embeddingSize\n",
    "                filterShape = [filterSize, config.model.embeddingSize, 1, config.model.numFilters]\n",
    "                W = tf.Variable(tf.truncated_normal(filterShape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[config.model.numFilters]), name=\"b\")\n",
    "                \n",
    "                conv = tf.nn.conv2d(self.embeddedWordsExpanded, \n",
    "                                   W, \n",
    "                                   strides=[1, 1, 1, 1], \n",
    "                                   padding=\"VALID\", \n",
    "                                   name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                \n",
    "                pooled = tf.nn.max_pool(h, \n",
    "                                       ksize=[1, config.sequenceLength - filterSize + 1, 1, 1], \n",
    "                                       strides=[1, 1, 1, 1], \n",
    "                                       padding=\"VALID\", \n",
    "                                       name=\"pool\")\n",
    "                pooledOutputs.append(pooled)\n",
    "        \n",
    "        #获取到CNN model的输出长度\n",
    "        numFiltersTotal= config.model.numFilters * len(config.model.filterSizes)\n",
    "        # 池化后维度不变、按照最后的维度进行concat\n",
    "        self.hPool = tf.concat(pooledOutputs, 3)\n",
    "        # 展开成二维的，然后送入全连接层\n",
    "        self.hPoolFlat = tf.reshape(self.hPool, [-1, numFiltersTotal])\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.hDrop = tf.nn.dropout(self.hPoolFlat, self.dropoutKeepProb)\n",
    "        \n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\"outputW\", shape=[numFiltersTotal, config.numClasses], \n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB = tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            \n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(self.hDrop, outputW, outputB, name=\"logits\")\n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.int32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axsi=-1, name=\"predictions\")\n",
    "\n",
    "#             print(\"the current predictions are: \", self.predictions)\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                              dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_sotfmax_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                        labels=self.inputY)\n",
    "\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean(item):\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    \n",
    "    corr = 0\n",
    "    for index in range(len(pred_y)):\n",
    "        if pred_y[index] == true_y[index]:\n",
    "            corr += 1\n",
    "    \n",
    "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "def binary_precision(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    :param positive: 正例的索引表示\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    pred_corr = 0\n",
    "    for index in range(len(pred_y)):\n",
    "        if pred_y[index] == positive:\n",
    "            pred_corr += 1\n",
    "            if pred_y[index] == true_y[index]:\n",
    "                corr += 1\n",
    "    \n",
    "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "    \n",
    "    return prec\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    corr = 0\n",
    "    true_corr = 0\n",
    "    for index in range(len(pred_y)):\n",
    "        if true_y[index] == positive:\n",
    "            true_corr += 1\n",
    "            if true_y[index] == pred_y[index]:\n",
    "                corr += 1\n",
    "    \n",
    "    rec = corr / true_corr if true_corr > 0 else 0\n",
    "    \n",
    "    return rec\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    try:\n",
    "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    except:\n",
    "        f_b = 0\n",
    "    \n",
    "    return f_b\n",
    "\n",
    "\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    \n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    \n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    \n",
    "    f_beats =[binary_f_bate(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    \n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = mulyi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    \n",
    "    return acc, recall, precision, f_beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current predictions are: self.predictions\n",
      "writing to C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model_code\\summarys \n",
      "\n",
      "start to train models...\n",
      "train: step: 1, loss: 3.054974317550659, acc: 0.453125, recall: 0.0945945945945946, precision: 0.7, f_beta: 0.16666666666666669\n",
      "train: step: 2, loss: 2.5258941650390625, acc: 0.390625, recall: 0.7, precision: 0.35714285714285715, f_beta: 0.47297297297297297\n",
      "train: step: 3, loss: 2.1744542121887207, acc: 0.4921875, recall: 0.828125, precision: 0.4953271028037383, f_beta: 0.6198830409356726\n",
      "train: step: 4, loss: 1.8002183437347412, acc: 0.515625, recall: 0.6440677966101694, precision: 0.4810126582278481, f_beta: 0.5507246376811594\n",
      "train: step: 5, loss: 1.6515357494354248, acc: 0.5390625, recall: 0.5483870967741935, precision: 0.5230769230769231, f_beta: 0.5354330708661418\n",
      "train: step: 6, loss: 1.6065058708190918, acc: 0.5078125, recall: 0.2037037037037037, precision: 0.3548387096774194, f_beta: 0.2588235294117647\n",
      "train: step: 7, loss: 2.0597681999206543, acc: 0.5, recall: 0.14516129032258066, precision: 0.45, f_beta: 0.2195121951219512\n",
      "train: step: 8, loss: 2.163203477859497, acc: 0.421875, recall: 0.2465753424657534, precision: 0.4864864864864865, f_beta: 0.32727272727272727\n",
      "train: step: 9, loss: 1.5859756469726562, acc: 0.5, recall: 0.36666666666666664, precision: 0.4583333333333333, f_beta: 0.4074074074074074\n",
      "train: step: 10, loss: 1.6724414825439453, acc: 0.53125, recall: 0.7619047619047619, precision: 0.5161290322580645, f_beta: 0.6153846153846153\n",
      "train: step: 11, loss: 1.867998480796814, acc: 0.4765625, recall: 0.796875, precision: 0.4857142857142857, f_beta: 0.6035502958579881\n",
      "train: step: 12, loss: 1.7196640968322754, acc: 0.4609375, recall: 0.7413793103448276, precision: 0.44329896907216493, f_beta: 0.5548387096774193\n",
      "train: step: 13, loss: 1.6058063507080078, acc: 0.4609375, recall: 0.5645161290322581, precision: 0.45454545454545453, f_beta: 0.5035971223021583\n",
      "train: step: 14, loss: 1.4657652378082275, acc: 0.5078125, recall: 0.4266666666666667, precision: 0.6153846153846154, f_beta: 0.5039370078740159\n",
      "train: step: 15, loss: 1.440349817276001, acc: 0.5703125, recall: 0.39344262295081966, precision: 0.5714285714285714, f_beta: 0.4660194174757281\n",
      "train: step: 16, loss: 1.4885393381118774, acc: 0.515625, recall: 0.296875, precision: 0.5277777777777778, f_beta: 0.38\n",
      "train: step: 17, loss: 1.572889804840088, acc: 0.5390625, recall: 0.36486486486486486, precision: 0.6923076923076923, f_beta: 0.47787610619469023\n",
      "train: step: 18, loss: 1.3221511840820312, acc: 0.5625, recall: 0.47058823529411764, precision: 0.4528301886792453, f_beta: 0.4615384615384615\n",
      "train: step: 19, loss: 1.6072163581848145, acc: 0.4609375, recall: 0.6428571428571429, precision: 0.4235294117647059, f_beta: 0.5106382978723405\n",
      "train: step: 20, loss: 1.4430973529815674, acc: 0.5390625, recall: 0.6029411764705882, precision: 0.5616438356164384, f_beta: 0.5815602836879432\n",
      "train: step: 21, loss: 1.529447317123413, acc: 0.5, recall: 0.5409836065573771, precision: 0.4782608695652174, f_beta: 0.5076923076923077\n",
      "train: step: 22, loss: 1.3610303401947021, acc: 0.5, recall: 0.5573770491803278, precision: 0.4788732394366197, f_beta: 0.5151515151515151\n",
      "train: step: 23, loss: 1.439276933670044, acc: 0.515625, recall: 0.5692307692307692, precision: 0.5211267605633803, f_beta: 0.5441176470588235\n",
      "train: step: 24, loss: 1.6832141876220703, acc: 0.4609375, recall: 0.43548387096774194, precision: 0.4426229508196721, f_beta: 0.43902439024390244\n",
      "train: step: 25, loss: 1.3875538110733032, acc: 0.515625, recall: 0.43283582089552236, precision: 0.5471698113207547, f_beta: 0.4833333333333333\n",
      "train: step: 26, loss: 1.3947595357894897, acc: 0.515625, recall: 0.375, precision: 0.5217391304347826, f_beta: 0.43636363636363634\n",
      "train: step: 27, loss: 1.2545127868652344, acc: 0.5390625, recall: 0.5352112676056338, precision: 0.59375, f_beta: 0.562962962962963\n",
      "train: step: 28, loss: 1.2575184106826782, acc: 0.515625, recall: 0.5666666666666667, precision: 0.4857142857142857, f_beta: 0.523076923076923\n",
      "train: step: 29, loss: 1.2109580039978027, acc: 0.6015625, recall: 0.5967741935483871, precision: 0.5873015873015873, f_beta: 0.592\n",
      "train: step: 30, loss: 1.0841279029846191, acc: 0.5390625, recall: 0.5844155844155844, precision: 0.625, f_beta: 0.6040268456375839\n",
      "train: step: 31, loss: 1.4636809825897217, acc: 0.4765625, recall: 0.5757575757575758, precision: 0.4935064935064935, f_beta: 0.5314685314685315\n",
      "train: step: 32, loss: 1.011317253112793, acc: 0.5390625, recall: 0.5507246376811594, precision: 0.5757575757575758, f_beta: 0.562962962962963\n",
      "train: step: 33, loss: 0.9793628454208374, acc: 0.625, recall: 0.6666666666666666, precision: 0.6666666666666666, f_beta: 0.6666666666666666\n",
      "train: step: 34, loss: 1.2935572862625122, acc: 0.5, recall: 0.5625, precision: 0.5, f_beta: 0.5294117647058824\n",
      "train: step: 35, loss: 1.103999137878418, acc: 0.5703125, recall: 0.6578947368421053, precision: 0.6329113924050633, f_beta: 0.6451612903225806\n",
      "train: step: 36, loss: 0.9308688640594482, acc: 0.5703125, recall: 0.6029411764705882, precision: 0.5942028985507246, f_beta: 0.5985401459854013\n",
      "train: step: 37, loss: 1.092313289642334, acc: 0.5390625, recall: 0.5342465753424658, precision: 0.609375, f_beta: 0.5693430656934307\n",
      "train: step: 38, loss: 1.0080935955047607, acc: 0.546875, recall: 0.49206349206349204, precision: 0.543859649122807, f_beta: 0.5166666666666667\n",
      "train: step: 39, loss: 1.32197904586792, acc: 0.515625, recall: 0.5714285714285714, precision: 0.5070422535211268, f_beta: 0.5373134328358208\n",
      "train: step: 40, loss: 1.3527616262435913, acc: 0.453125, recall: 0.5, precision: 0.5, f_beta: 0.5\n",
      "train: step: 41, loss: 1.1783764362335205, acc: 0.4921875, recall: 0.46153846153846156, precision: 0.5, f_beta: 0.48000000000000004\n",
      "train: step: 42, loss: 1.1834988594055176, acc: 0.5, recall: 0.5396825396825397, precision: 0.4927536231884058, f_beta: 0.515151515151515\n",
      "train: step: 43, loss: 1.0202312469482422, acc: 0.5625, recall: 0.45714285714285713, precision: 0.64, f_beta: 0.5333333333333333\n",
      "train: step: 44, loss: 0.9040287733078003, acc: 0.59375, recall: 0.6417910447761194, precision: 0.6056338028169014, f_beta: 0.6231884057971014\n",
      "train: step: 45, loss: 0.9021353125572205, acc: 0.5625, recall: 0.6896551724137931, precision: 0.5128205128205128, f_beta: 0.5882352941176471\n",
      "train: step: 46, loss: 0.9751083850860596, acc: 0.5390625, recall: 0.6376811594202898, precision: 0.5641025641025641, f_beta: 0.5986394557823129\n",
      "train: step: 47, loss: 1.0991148948669434, acc: 0.515625, recall: 0.44642857142857145, precision: 0.44642857142857145, f_beta: 0.44642857142857145\n",
      "train: step: 48, loss: 0.8979048728942871, acc: 0.578125, recall: 0.5396825396825397, precision: 0.576271186440678, f_beta: 0.5573770491803278\n",
      "train: step: 49, loss: 1.080927848815918, acc: 0.546875, recall: 0.4117647058823529, precision: 0.42857142857142855, f_beta: 0.42\n",
      "train: step: 50, loss: 0.841616153717041, acc: 0.6640625, recall: 0.609375, precision: 0.6842105263157895, f_beta: 0.6446280991735538\n",
      "train: step: 51, loss: 0.8919256925582886, acc: 0.6171875, recall: 0.5483870967741935, precision: 0.6181818181818182, f_beta: 0.5811965811965812\n",
      "train: step: 52, loss: 0.9298882484436035, acc: 0.59375, recall: 0.6551724137931034, precision: 0.5428571428571428, f_beta: 0.5937499999999999\n",
      "train: step: 53, loss: 0.8888117074966431, acc: 0.578125, recall: 0.47368421052631576, precision: 0.5294117647058824, f_beta: 0.5\n",
      "train: step: 54, loss: 0.8241629004478455, acc: 0.5546875, recall: 0.4626865671641791, precision: 0.5961538461538461, f_beta: 0.5210084033613446\n",
      "train: step: 55, loss: 0.7875196933746338, acc: 0.640625, recall: 0.6323529411764706, precision: 0.671875, f_beta: 0.6515151515151515\n",
      "train: step: 56, loss: 0.7249054312705994, acc: 0.703125, recall: 0.6911764705882353, precision: 0.734375, f_beta: 0.7121212121212122\n",
      "train: step: 57, loss: 0.7232438921928406, acc: 0.6640625, recall: 0.7424242424242424, precision: 0.6533333333333333, f_beta: 0.6950354609929078\n",
      "train: step: 58, loss: 1.0450303554534912, acc: 0.515625, recall: 0.7580645161290323, precision: 0.5, f_beta: 0.6025641025641025\n",
      "train: step: 59, loss: 1.0425870418548584, acc: 0.5234375, recall: 0.6666666666666666, precision: 0.49382716049382713, f_beta: 0.5673758865248227\n",
      "train: step: 60, loss: 0.8943456411361694, acc: 0.5859375, recall: 0.5238095238095238, precision: 0.5892857142857143, f_beta: 0.5546218487394958\n",
      "train: step: 61, loss: 0.8837100267410278, acc: 0.625, recall: 0.463768115942029, precision: 0.7441860465116279, f_beta: 0.5714285714285714\n",
      "train: step: 62, loss: 0.7840983271598816, acc: 0.625, recall: 0.45, precision: 0.6428571428571429, f_beta: 0.5294117647058824\n",
      "train: step: 63, loss: 0.8453985452651978, acc: 0.6484375, recall: 0.5909090909090909, precision: 0.6842105263157895, f_beta: 0.6341463414634148\n",
      "train: step: 64, loss: 0.7304803133010864, acc: 0.625, recall: 0.6451612903225806, precision: 0.6060606060606061, f_beta: 0.625\n",
      "train: step: 65, loss: 0.6916665434837341, acc: 0.6875, recall: 0.7058823529411765, precision: 0.7058823529411765, f_beta: 0.7058823529411765\n",
      "train: step: 66, loss: 0.8147311806678772, acc: 0.6328125, recall: 0.7611940298507462, precision: 0.6219512195121951, f_beta: 0.6845637583892618\n",
      "train: step: 67, loss: 0.7638386487960815, acc: 0.640625, recall: 0.7611940298507462, precision: 0.6296296296296297, f_beta: 0.6891891891891893\n",
      "train: step: 68, loss: 0.7671009302139282, acc: 0.6484375, recall: 0.7142857142857143, precision: 0.625, f_beta: 0.6666666666666666\n",
      "train: step: 69, loss: 0.8483675718307495, acc: 0.6171875, recall: 0.703125, precision: 0.6, f_beta: 0.6474820143884892\n",
      "train: step: 70, loss: 0.6124457716941833, acc: 0.6640625, recall: 0.6428571428571429, precision: 0.6101694915254238, f_beta: 0.6260869565217392\n",
      "train: step: 71, loss: 0.8165825605392456, acc: 0.6171875, recall: 0.4339622641509434, precision: 0.5476190476190477, f_beta: 0.48421052631578954\n",
      "train: step: 72, loss: 0.7475721836090088, acc: 0.7109375, recall: 0.532258064516129, precision: 0.8048780487804879, f_beta: 0.6407766990291263\n",
      "train: step: 73, loss: 0.578983724117279, acc: 0.71875, recall: 0.6031746031746031, precision: 0.7755102040816326, f_beta: 0.6785714285714285\n",
      "train: step: 74, loss: 0.658916711807251, acc: 0.65625, recall: 0.5396825396825397, precision: 0.6938775510204082, f_beta: 0.6071428571428572\n",
      "train: step: 75, loss: 0.7094728946685791, acc: 0.640625, recall: 0.6351351351351351, precision: 0.7121212121212122, f_beta: 0.6714285714285715\n",
      "train: step: 76, loss: 0.6528038382530212, acc: 0.6875, recall: 0.7532467532467533, precision: 0.7341772151898734, f_beta: 0.7435897435897437\n",
      "train: step: 77, loss: 0.7090848684310913, acc: 0.65625, recall: 0.8714285714285714, precision: 0.6354166666666666, f_beta: 0.7349397590361446\n",
      "train: step: 78, loss: 0.8899316191673279, acc: 0.609375, recall: 0.8840579710144928, precision: 0.5922330097087378, f_beta: 0.7093023255813954\n",
      "train: step: 79, loss: 0.7215063571929932, acc: 0.671875, recall: 0.8387096774193549, precision: 0.6190476190476191, f_beta: 0.7123287671232876\n",
      "train: step: 80, loss: 0.8623824119567871, acc: 0.5859375, recall: 0.6507936507936508, precision: 0.5694444444444444, f_beta: 0.6074074074074073\n",
      "train: step: 81, loss: 0.8259493708610535, acc: 0.640625, recall: 0.5342465753424658, precision: 0.7647058823529411, f_beta: 0.6290322580645161\n",
      "train: step: 82, loss: 0.7106121182441711, acc: 0.6796875, recall: 0.47058823529411764, precision: 0.8648648648648649, f_beta: 0.6095238095238095\n",
      "train: step: 83, loss: 0.7156452536582947, acc: 0.6875, recall: 0.5735294117647058, precision: 0.78, f_beta: 0.6610169491525424\n",
      "train: step: 84, loss: 0.7379583120346069, acc: 0.625, recall: 0.578125, precision: 0.6379310344827587, f_beta: 0.6065573770491803\n",
      "train: step: 85, loss: 0.5368503928184509, acc: 0.7265625, recall: 0.7666666666666667, precision: 0.6865671641791045, f_beta: 0.7244094488188977\n",
      "train: step: 86, loss: 0.7754491567611694, acc: 0.6171875, recall: 0.7457627118644068, precision: 0.5641025641025641, f_beta: 0.6423357664233577\n",
      "train: step: 87, loss: 0.6382261514663696, acc: 0.65625, recall: 0.704225352112676, precision: 0.684931506849315, f_beta: 0.6944444444444443\n",
      "train: step: 88, loss: 0.5979481935501099, acc: 0.6953125, recall: 0.6779661016949152, precision: 0.6666666666666666, f_beta: 0.6722689075630253\n",
      "train: step: 89, loss: 0.5818657875061035, acc: 0.703125, recall: 0.7543859649122807, precision: 0.6417910447761194, f_beta: 0.6935483870967741\n",
      "train: step: 90, loss: 0.5520738363265991, acc: 0.765625, recall: 0.6984126984126984, precision: 0.8, f_beta: 0.7457627118644068\n",
      "train: step: 91, loss: 0.6838402152061462, acc: 0.6796875, recall: 0.603448275862069, precision: 0.660377358490566, f_beta: 0.6306306306306305\n",
      "train: step: 92, loss: 0.6967161297798157, acc: 0.6796875, recall: 0.5294117647058824, precision: 0.8, f_beta: 0.6371681415929203\n",
      "train: step: 93, loss: 0.6737829446792603, acc: 0.65625, recall: 0.576271186440678, precision: 0.6415094339622641, f_beta: 0.6071428571428571\n",
      "train: step: 94, loss: 0.7187691330909729, acc: 0.6640625, recall: 0.7068965517241379, precision: 0.6119402985074627, f_beta: 0.6559999999999999\n",
      "train: step: 95, loss: 0.6928606033325195, acc: 0.6875, recall: 0.5666666666666667, precision: 0.7083333333333334, f_beta: 0.6296296296296297\n",
      "train: step: 96, loss: 0.5899126529693604, acc: 0.7109375, recall: 0.6911764705882353, precision: 0.746031746031746, f_beta: 0.717557251908397\n",
      "train: step: 97, loss: 0.7491549253463745, acc: 0.65625, recall: 0.6470588235294118, precision: 0.559322033898305, f_beta: 0.6\n",
      "train: step: 98, loss: 0.5029106140136719, acc: 0.75, recall: 0.7733333333333333, precision: 0.7945205479452054, f_beta: 0.7837837837837838\n",
      "train: step: 99, loss: 0.6210033297538757, acc: 0.6640625, recall: 0.6896551724137931, precision: 0.6153846153846154, f_beta: 0.6504065040650407\n",
      "train: step: 100, loss: 0.6498633623123169, acc: 0.6875, recall: 0.6307692307692307, precision: 0.7192982456140351, f_beta: 0.6721311475409837\n",
      "\n",
      " evaluate model...\n",
      "2019-09-24T11:53:08.244228, step: 100, loss: 0.47066003924761063, acc: 0.7852564102564102, precision: 0.7460001029380923, recall: 0.870442738282826, f_beta: 0.8026098853687258\n",
      "saved mdoel checkpoint tp C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\textCNN\\my_model-100\n",
      "\n",
      "train: step: 101, loss: 0.5703612565994263, acc: 0.703125, recall: 0.8181818181818182, precision: 0.675, f_beta: 0.7397260273972603\n",
      "train: step: 102, loss: 0.6862325668334961, acc: 0.6328125, recall: 0.7321428571428571, precision: 0.5616438356164384, f_beta: 0.6356589147286821\n",
      "train: step: 103, loss: 0.676904559135437, acc: 0.65625, recall: 0.5789473684210527, precision: 0.6226415094339622, f_beta: 0.6\n",
      "train: step: 104, loss: 0.6435345411300659, acc: 0.6796875, recall: 0.5901639344262295, precision: 0.6923076923076923, f_beta: 0.6371681415929203\n",
      "train: step: 105, loss: 0.5554155707359314, acc: 0.7734375, recall: 0.746031746031746, precision: 0.7833333333333333, f_beta: 0.7642276422764228\n",
      "train: step: 106, loss: 0.7758846282958984, acc: 0.6640625, recall: 0.5, precision: 0.7209302325581395, f_beta: 0.5904761904761905\n",
      "train: step: 107, loss: 0.5500823259353638, acc: 0.7265625, recall: 0.71875, precision: 0.7301587301587301, f_beta: 0.7244094488188977\n",
      "train: step: 108, loss: 0.5646848678588867, acc: 0.6875, recall: 0.6376811594202898, precision: 0.7457627118644068, f_beta: 0.6875\n",
      "train: step: 109, loss: 0.4632333517074585, acc: 0.7578125, recall: 0.8088235294117647, precision: 0.7534246575342466, f_beta: 0.7801418439716311\n",
      "train: step: 110, loss: 0.6295998096466064, acc: 0.6875, recall: 0.7704918032786885, precision: 0.6438356164383562, f_beta: 0.7014925373134328\n",
      "train: step: 111, loss: 0.5754238367080688, acc: 0.734375, recall: 0.7313432835820896, precision: 0.7538461538461538, f_beta: 0.7424242424242424\n",
      "train: step: 112, loss: 0.5488879680633545, acc: 0.734375, recall: 0.7692307692307693, precision: 0.7246376811594203, f_beta: 0.7462686567164178\n",
      "train: step: 113, loss: 0.470876008272171, acc: 0.796875, recall: 0.8571428571428571, precision: 0.7894736842105263, f_beta: 0.8219178082191781\n",
      "train: step: 114, loss: 0.555130124092102, acc: 0.765625, recall: 0.8095238095238095, precision: 0.7391304347826086, f_beta: 0.7727272727272727\n",
      "train: step: 115, loss: 0.6073746085166931, acc: 0.6953125, recall: 0.75, precision: 0.676056338028169, f_beta: 0.7111111111111111\n",
      "train: step: 116, loss: 0.5814287662506104, acc: 0.6796875, recall: 0.6363636363636364, precision: 0.5283018867924528, f_beta: 0.5773195876288659\n",
      "train: step: 117, loss: 0.6169471740722656, acc: 0.7109375, recall: 0.6615384615384615, precision: 0.7413793103448276, f_beta: 0.6991869918699186\n",
      "train: step: 118, loss: 0.6483566761016846, acc: 0.71875, recall: 0.6125, precision: 0.9074074074074074, f_beta: 0.7313432835820897\n",
      "train: step: 119, loss: 0.6540772914886475, acc: 0.6875, recall: 0.6567164179104478, precision: 0.7213114754098361, f_beta: 0.6875\n",
      "train: step: 120, loss: 0.4665384292602539, acc: 0.8046875, recall: 0.8, precision: 0.7868852459016393, f_beta: 0.7933884297520661\n",
      "train: step: 121, loss: 0.5506709814071655, acc: 0.7265625, recall: 0.7627118644067796, precision: 0.6818181818181818, f_beta: 0.7199999999999999\n",
      "train: step: 122, loss: 0.5411930680274963, acc: 0.734375, recall: 0.9107142857142857, precision: 0.6375, f_beta: 0.7499999999999999\n",
      "train: step: 123, loss: 0.6189436912536621, acc: 0.703125, recall: 0.7903225806451613, precision: 0.6621621621621622, f_beta: 0.7205882352941176\n",
      "train: step: 124, loss: 0.6079564690589905, acc: 0.6953125, recall: 0.7205882352941176, precision: 0.7101449275362319, f_beta: 0.7153284671532847\n",
      "train: step: 125, loss: 0.48907846212387085, acc: 0.7578125, recall: 0.7049180327868853, precision: 0.7678571428571429, f_beta: 0.7350427350427351\n",
      "train: step: 126, loss: 0.5690795183181763, acc: 0.7265625, recall: 0.65, precision: 0.7358490566037735, f_beta: 0.6902654867256638\n",
      "train: step: 127, loss: 0.4718680679798126, acc: 0.7890625, recall: 0.75, precision: 0.7358490566037735, f_beta: 0.7428571428571428\n",
      "train: step: 128, loss: 0.6720603704452515, acc: 0.6796875, recall: 0.5166666666666667, precision: 0.7209302325581395, f_beta: 0.6019417475728156\n",
      "train: step: 129, loss: 0.5569705963134766, acc: 0.71875, recall: 0.6666666666666666, precision: 0.7142857142857143, f_beta: 0.689655172413793\n",
      "train: step: 130, loss: 0.4399068057537079, acc: 0.78125, recall: 0.6935483870967742, precision: 0.8269230769230769, f_beta: 0.7543859649122807\n",
      "train: step: 131, loss: 0.465497761964798, acc: 0.765625, recall: 0.7090909090909091, precision: 0.7358490566037735, f_beta: 0.7222222222222221\n",
      "train: step: 132, loss: 0.5005383491516113, acc: 0.75, recall: 0.7580645161290323, precision: 0.734375, f_beta: 0.7460317460317459\n",
      "train: step: 133, loss: 0.5046586990356445, acc: 0.7734375, recall: 0.8055555555555556, precision: 0.7945205479452054, f_beta: 0.8\n",
      "train: step: 134, loss: 0.5688380002975464, acc: 0.7421875, recall: 0.8412698412698413, precision: 0.6973684210526315, f_beta: 0.7625899280575539\n",
      "train: step: 135, loss: 0.4675113558769226, acc: 0.75, recall: 0.8852459016393442, precision: 0.6835443037974683, f_beta: 0.7714285714285715\n",
      "train: step: 136, loss: 0.5954307317733765, acc: 0.734375, recall: 0.7903225806451613, precision: 0.7, f_beta: 0.7424242424242423\n",
      "train: step: 137, loss: 0.599716067314148, acc: 0.7109375, recall: 0.6268656716417911, precision: 0.7777777777777778, f_beta: 0.694214876033058\n",
      "train: step: 138, loss: 0.5445787906646729, acc: 0.6953125, recall: 0.6617647058823529, precision: 0.7377049180327869, f_beta: 0.6976744186046512\n",
      "train: step: 139, loss: 0.6065683364868164, acc: 0.6875, recall: 0.7101449275362319, precision: 0.7101449275362319, f_beta: 0.7101449275362319\n",
      "train: step: 140, loss: 0.6036829352378845, acc: 0.6875, recall: 0.7166666666666667, precision: 0.6515151515151515, f_beta: 0.6825396825396826\n",
      "train: step: 141, loss: 0.43021079897880554, acc: 0.8046875, recall: 0.8333333333333334, precision: 0.7692307692307693, f_beta: 0.8\n",
      "train: step: 142, loss: 0.4852698743343353, acc: 0.7578125, recall: 0.78125, precision: 0.746268656716418, f_beta: 0.7633587786259542\n",
      "train: step: 143, loss: 0.5920473337173462, acc: 0.6953125, recall: 0.6607142857142857, precision: 0.6491228070175439, f_beta: 0.6548672566371682\n",
      "train: step: 144, loss: 0.4773958921432495, acc: 0.75, recall: 0.6612903225806451, precision: 0.7884615384615384, f_beta: 0.719298245614035\n",
      "train: step: 145, loss: 0.560229480266571, acc: 0.734375, recall: 0.6533333333333333, precision: 0.8596491228070176, f_beta: 0.7424242424242424\n",
      "train: step: 146, loss: 0.43116000294685364, acc: 0.7890625, recall: 0.75, precision: 0.8135593220338984, f_beta: 0.7804878048780488\n",
      "train: step: 147, loss: 0.5300273895263672, acc: 0.765625, recall: 0.8, precision: 0.7536231884057971, f_beta: 0.7761194029850746\n",
      "train: step: 148, loss: 0.5069620609283447, acc: 0.703125, recall: 0.8382352941176471, precision: 0.6785714285714286, f_beta: 0.7500000000000001\n",
      "train: step: 149, loss: 0.523196816444397, acc: 0.7265625, recall: 0.78125, precision: 0.704225352112676, f_beta: 0.7407407407407407\n",
      "train: step: 150, loss: 0.5466927289962769, acc: 0.7890625, recall: 0.9047619047619048, precision: 0.7307692307692307, f_beta: 0.8085106382978723\n",
      "train: step: 151, loss: 0.5623326301574707, acc: 0.765625, recall: 0.8035714285714286, precision: 0.703125, f_beta: 0.75\n",
      "train: step: 152, loss: 0.4935091435909271, acc: 0.8046875, recall: 0.8125, precision: 0.8, f_beta: 0.8062015503875969\n",
      "train: step: 153, loss: 0.5812526941299438, acc: 0.71875, recall: 0.6571428571428571, precision: 0.7931034482758621, f_beta: 0.71875\n",
      "train: step: 154, loss: 0.5447703003883362, acc: 0.7109375, recall: 0.6323529411764706, precision: 0.7818181818181819, f_beta: 0.6991869918699187\n",
      "train: step: 155, loss: 0.5230110883712769, acc: 0.7421875, recall: 0.6290322580645161, precision: 0.7959183673469388, f_beta: 0.7027027027027026\n",
      "train: step: 156, loss: 0.5478648543357849, acc: 0.7109375, recall: 0.7088607594936709, precision: 0.8, f_beta: 0.7516778523489933\n",
      "start to train models...\n",
      "train: step: 157, loss: 0.5105487108230591, acc: 0.7890625, recall: 0.8260869565217391, precision: 0.7916666666666666, f_beta: 0.8085106382978724\n",
      "train: step: 158, loss: 0.5221531391143799, acc: 0.7734375, recall: 0.90625, precision: 0.7160493827160493, f_beta: 0.7999999999999999\n",
      "train: step: 159, loss: 0.5262079238891602, acc: 0.7578125, recall: 0.9344262295081968, precision: 0.6785714285714286, f_beta: 0.7862068965517242\n",
      "train: step: 160, loss: 0.48970019817352295, acc: 0.796875, recall: 0.835820895522388, precision: 0.7887323943661971, f_beta: 0.8115942028985507\n",
      "train: step: 161, loss: 0.5046069025993347, acc: 0.7421875, recall: 0.8148148148148148, precision: 0.6567164179104478, f_beta: 0.7272727272727273\n",
      "train: step: 162, loss: 0.3507910370826721, acc: 0.8828125, recall: 0.8225806451612904, precision: 0.9272727272727272, f_beta: 0.8717948717948718\n",
      "train: step: 163, loss: 0.4338010549545288, acc: 0.7890625, recall: 0.7205882352941176, precision: 0.8596491228070176, f_beta: 0.7839999999999999\n",
      "train: step: 164, loss: 0.5631966590881348, acc: 0.75, recall: 0.6031746031746031, precision: 0.8444444444444444, f_beta: 0.7037037037037037\n",
      "train: step: 165, loss: 0.44741755723953247, acc: 0.8046875, recall: 0.7313432835820896, precision: 0.875, f_beta: 0.7967479674796748\n",
      "train: step: 166, loss: 0.339145302772522, acc: 0.875, recall: 0.8507462686567164, precision: 0.9047619047619048, f_beta: 0.8769230769230769\n",
      "train: step: 167, loss: 0.3607124090194702, acc: 0.8125, recall: 0.7878787878787878, precision: 0.8387096774193549, f_beta: 0.8125\n",
      "train: step: 168, loss: 0.4492601454257965, acc: 0.7578125, recall: 0.8382352941176471, precision: 0.7402597402597403, f_beta: 0.7862068965517242\n",
      "train: step: 169, loss: 0.4257356822490692, acc: 0.8125, recall: 0.8823529411764706, precision: 0.7894736842105263, f_beta: 0.8333333333333333\n",
      "train: step: 170, loss: 0.5064915418624878, acc: 0.734375, recall: 0.875, precision: 0.6447368421052632, f_beta: 0.7424242424242424\n",
      "train: step: 171, loss: 0.3674734830856323, acc: 0.84375, recall: 0.9253731343283582, precision: 0.8051948051948052, f_beta: 0.8611111111111112\n",
      "train: step: 172, loss: 0.3150695264339447, acc: 0.875, recall: 0.9428571428571428, precision: 0.8461538461538461, f_beta: 0.8918918918918919\n",
      "train: step: 173, loss: 0.5186505913734436, acc: 0.7734375, recall: 0.7432432432432432, precision: 0.8461538461538461, f_beta: 0.7913669064748202\n",
      "train: step: 174, loss: 0.3456810712814331, acc: 0.8515625, recall: 0.8305084745762712, precision: 0.8448275862068966, f_beta: 0.8376068376068375\n",
      "train: step: 175, loss: 0.4368310868740082, acc: 0.8125, recall: 0.7692307692307693, precision: 0.847457627118644, f_beta: 0.8064516129032259\n",
      "train: step: 176, loss: 0.4703235626220703, acc: 0.7578125, recall: 0.7333333333333333, precision: 0.7457627118644068, f_beta: 0.7394957983193278\n",
      "train: step: 177, loss: 0.5004541277885437, acc: 0.6953125, recall: 0.6551724137931034, precision: 0.6666666666666666, f_beta: 0.6608695652173913\n",
      "train: step: 178, loss: 0.4072067141532898, acc: 0.828125, recall: 0.8076923076923077, precision: 0.9, f_beta: 0.8513513513513514\n",
      "train: step: 179, loss: 0.41449612379074097, acc: 0.8046875, recall: 0.828125, precision: 0.7910447761194029, f_beta: 0.8091603053435115\n",
      "train: step: 180, loss: 0.30111944675445557, acc: 0.875, recall: 0.9014084507042254, precision: 0.8767123287671232, f_beta: 0.8888888888888888\n",
      "train: step: 181, loss: 0.38699647784233093, acc: 0.828125, recall: 0.9, precision: 0.7714285714285715, f_beta: 0.8307692307692307\n",
      "train: step: 182, loss: 0.4607405662536621, acc: 0.765625, recall: 0.8923076923076924, precision: 0.7160493827160493, f_beta: 0.7945205479452054\n",
      "train: step: 183, loss: 0.4290095567703247, acc: 0.78125, recall: 0.9137931034482759, precision: 0.6973684210526315, f_beta: 0.7910447761194029\n",
      "train: step: 184, loss: 0.40087705850601196, acc: 0.828125, recall: 0.859375, precision: 0.8088235294117647, f_beta: 0.8333333333333333\n",
      "train: step: 185, loss: 0.44018134474754333, acc: 0.7421875, recall: 0.7391304347826086, precision: 0.7727272727272727, f_beta: 0.7555555555555555\n",
      "train: step: 186, loss: 0.3792074918746948, acc: 0.84375, recall: 0.7966101694915254, precision: 0.8545454545454545, f_beta: 0.8245614035087718\n",
      "train: step: 187, loss: 0.46050480008125305, acc: 0.7421875, recall: 0.7272727272727273, precision: 0.8235294117647058, f_beta: 0.7724137931034483\n",
      "train: step: 188, loss: 0.35556450486183167, acc: 0.8125, recall: 0.7301587301587301, precision: 0.8679245283018868, f_beta: 0.7931034482758621\n",
      "train: step: 189, loss: 0.3753191828727722, acc: 0.8671875, recall: 0.8392857142857143, precision: 0.8545454545454545, f_beta: 0.8468468468468467\n",
      "train: step: 190, loss: 0.512711763381958, acc: 0.7734375, recall: 0.7704918032786885, precision: 0.7580645161290323, f_beta: 0.7642276422764228\n",
      "train: step: 191, loss: 0.3851447105407715, acc: 0.84375, recall: 0.859375, precision: 0.8333333333333334, f_beta: 0.8461538461538461\n",
      "train: step: 192, loss: 0.44407010078430176, acc: 0.78125, recall: 0.8548387096774194, precision: 0.7361111111111112, f_beta: 0.791044776119403\n",
      "train: step: 193, loss: 0.40172576904296875, acc: 0.859375, recall: 0.8611111111111112, precision: 0.8857142857142857, f_beta: 0.8732394366197184\n",
      "train: step: 194, loss: 0.4023982286453247, acc: 0.859375, recall: 0.8571428571428571, precision: 0.8823529411764706, f_beta: 0.8695652173913043\n",
      "train: step: 195, loss: 0.38269156217575073, acc: 0.8125, recall: 0.8309859154929577, precision: 0.8309859154929577, f_beta: 0.8309859154929577\n",
      "train: step: 196, loss: 0.2890596389770508, acc: 0.875, recall: 0.8961038961038961, precision: 0.8961038961038961, f_beta: 0.8961038961038962\n",
      "train: step: 197, loss: 0.3862827718257904, acc: 0.8515625, recall: 0.8888888888888888, precision: 0.7868852459016393, f_beta: 0.8347826086956522\n",
      "train: step: 198, loss: 0.4533381462097168, acc: 0.828125, recall: 0.9104477611940298, precision: 0.7922077922077922, f_beta: 0.8472222222222222\n",
      "train: step: 199, loss: 0.4097178280353546, acc: 0.8125, recall: 0.8064516129032258, precision: 0.8064516129032258, f_beta: 0.8064516129032258\n",
      "train: step: 200, loss: 0.4183720350265503, acc: 0.796875, recall: 0.8169014084507042, precision: 0.8169014084507042, f_beta: 0.8169014084507042\n",
      "\n",
      " evaluate model...\n",
      "2019-09-24T11:55:05.614759, step: 200, loss: 0.3753956693869371, acc: 0.8411458333333334, precision: 0.8364433048115782, recall: 0.852561297302006, f_beta: 0.84344045303352\n",
      "saved mdoel checkpoint tp C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\textCNN\\my_model-200\n",
      "\n",
      "train: step: 201, loss: 0.36196091771125793, acc: 0.8359375, recall: 0.7894736842105263, precision: 0.8333333333333334, f_beta: 0.8108108108108109\n",
      "train: step: 202, loss: 0.40800386667251587, acc: 0.8359375, recall: 0.819672131147541, precision: 0.8333333333333334, f_beta: 0.8264462809917356\n",
      "train: step: 203, loss: 0.36416199803352356, acc: 0.8515625, recall: 0.825, precision: 0.9295774647887324, f_beta: 0.8741721854304636\n",
      "train: step: 204, loss: 0.3795854151248932, acc: 0.8046875, recall: 0.8225806451612904, precision: 0.7846153846153846, f_beta: 0.8031496062992126\n",
      "train: step: 205, loss: 0.4160590171813965, acc: 0.796875, recall: 0.8333333333333334, precision: 0.7575757575757576, f_beta: 0.7936507936507938\n",
      "train: step: 206, loss: 0.30864450335502625, acc: 0.8671875, recall: 0.8787878787878788, precision: 0.8656716417910447, f_beta: 0.8721804511278195\n",
      "train: step: 207, loss: 0.3888709545135498, acc: 0.8203125, recall: 0.8225806451612904, precision: 0.8095238095238095, f_beta: 0.8160000000000001\n",
      "train: step: 208, loss: 0.3473971486091614, acc: 0.84375, recall: 0.9047619047619048, precision: 0.8028169014084507, f_beta: 0.8507462686567164\n",
      "train: step: 209, loss: 0.42235878109931946, acc: 0.796875, recall: 0.8648648648648649, precision: 0.8, f_beta: 0.8311688311688312\n",
      "train: step: 210, loss: 0.28498929738998413, acc: 0.8828125, recall: 0.9027777777777778, precision: 0.8904109589041096, f_beta: 0.896551724137931\n",
      "train: step: 211, loss: 0.40470874309539795, acc: 0.84375, recall: 0.85, precision: 0.8947368421052632, f_beta: 0.8717948717948718\n",
      "train: step: 212, loss: 0.4027274549007416, acc: 0.8125, recall: 0.8153846153846154, precision: 0.8153846153846154, f_beta: 0.8153846153846154\n",
      "train: step: 213, loss: 0.3910876512527466, acc: 0.8046875, recall: 0.8833333333333333, precision: 0.7464788732394366, f_beta: 0.8091603053435115\n",
      "train: step: 214, loss: 0.561296820640564, acc: 0.7421875, recall: 0.8524590163934426, precision: 0.6842105263157895, f_beta: 0.7591240875912408\n",
      "train: step: 215, loss: 0.45480355620384216, acc: 0.8125, recall: 0.78125, precision: 0.8333333333333334, f_beta: 0.8064516129032259\n",
      "train: step: 216, loss: 0.38064995408058167, acc: 0.84375, recall: 0.84375, precision: 0.84375, f_beta: 0.84375\n",
      "train: step: 217, loss: 0.3797948956489563, acc: 0.796875, recall: 0.6557377049180327, precision: 0.8888888888888888, f_beta: 0.7547169811320755\n",
      "train: step: 218, loss: 0.3275258541107178, acc: 0.859375, recall: 0.8, precision: 0.8333333333333334, f_beta: 0.816326530612245\n",
      "train: step: 219, loss: 0.3802165389060974, acc: 0.859375, recall: 0.85, precision: 0.85, f_beta: 0.85\n",
      "train: step: 220, loss: 0.3449612855911255, acc: 0.84375, recall: 0.8571428571428571, precision: 0.8307692307692308, f_beta: 0.84375\n",
      "train: step: 221, loss: 0.3950299024581909, acc: 0.828125, recall: 0.8615384615384616, precision: 0.8115942028985508, f_beta: 0.835820895522388\n",
      "train: step: 222, loss: 0.351438045501709, acc: 0.8515625, recall: 0.8596491228070176, precision: 0.8166666666666667, f_beta: 0.8376068376068376\n",
      "train: step: 223, loss: 0.40394192934036255, acc: 0.8203125, recall: 0.7894736842105263, precision: 0.8035714285714286, f_beta: 0.7964601769911505\n",
      "train: step: 224, loss: 0.43200308084487915, acc: 0.8203125, recall: 0.8115942028985508, precision: 0.8484848484848485, f_beta: 0.8296296296296296\n",
      "train: step: 225, loss: 0.4204307794570923, acc: 0.8125, recall: 0.8571428571428571, precision: 0.782608695652174, f_beta: 0.8181818181818182\n",
      "train: step: 226, loss: 0.36112773418426514, acc: 0.84375, recall: 0.8648648648648649, precision: 0.8648648648648649, f_beta: 0.8648648648648649\n",
      "train: step: 227, loss: 0.4051681160926819, acc: 0.8203125, recall: 0.8947368421052632, precision: 0.75, f_beta: 0.816\n",
      "train: step: 228, loss: 0.4051348567008972, acc: 0.8125, recall: 0.7580645161290323, precision: 0.8392857142857143, f_beta: 0.7966101694915255\n",
      "train: step: 229, loss: 0.34563887119293213, acc: 0.8515625, recall: 0.8709677419354839, precision: 0.8307692307692308, f_beta: 0.8503937007874016\n",
      "train: step: 230, loss: 0.42652368545532227, acc: 0.78125, recall: 0.7301587301587301, precision: 0.8070175438596491, f_beta: 0.7666666666666667\n",
      "train: step: 231, loss: 0.43043336272239685, acc: 0.828125, recall: 0.8431372549019608, precision: 0.7543859649122807, f_beta: 0.7962962962962964\n",
      "train: step: 232, loss: 0.4356476962566376, acc: 0.8125, recall: 0.7884615384615384, precision: 0.7592592592592593, f_beta: 0.7735849056603773\n",
      "train: step: 233, loss: 0.3660959005355835, acc: 0.8125, recall: 0.7777777777777778, precision: 0.8305084745762712, f_beta: 0.8032786885245902\n",
      "train: step: 234, loss: 0.38255053758621216, acc: 0.8359375, recall: 0.819672131147541, precision: 0.8333333333333334, f_beta: 0.8264462809917356\n",
      "train: step: 235, loss: 0.3344002664089203, acc: 0.8671875, recall: 0.875, precision: 0.8615384615384616, f_beta: 0.8682170542635659\n",
      "train: step: 236, loss: 0.44366687536239624, acc: 0.828125, recall: 0.7903225806451613, precision: 0.8448275862068966, f_beta: 0.8166666666666667\n",
      "train: step: 237, loss: 0.3842146396636963, acc: 0.8046875, recall: 0.746268656716418, precision: 0.8620689655172413, f_beta: 0.7999999999999999\n",
      "train: step: 238, loss: 0.366199254989624, acc: 0.8515625, recall: 0.875, precision: 0.835820895522388, f_beta: 0.8549618320610687\n",
      "train: step: 239, loss: 0.3092595934867859, acc: 0.875, recall: 0.9230769230769231, precision: 0.8450704225352113, f_beta: 0.8823529411764706\n",
      "train: step: 240, loss: 0.45224612951278687, acc: 0.7890625, recall: 0.8055555555555556, precision: 0.8169014084507042, f_beta: 0.8111888111888113\n",
      "train: step: 241, loss: 0.363232284784317, acc: 0.84375, recall: 0.90625, precision: 0.8055555555555556, f_beta: 0.8529411764705882\n",
      "train: step: 242, loss: 0.3415335714817047, acc: 0.8671875, recall: 0.918918918918919, precision: 0.8607594936708861, f_beta: 0.8888888888888888\n",
      "train: step: 243, loss: 0.3703439235687256, acc: 0.8046875, recall: 0.85, precision: 0.7611940298507462, f_beta: 0.8031496062992126\n",
      "train: step: 244, loss: 0.39805227518081665, acc: 0.8046875, recall: 0.8070175438596491, precision: 0.7666666666666667, f_beta: 0.7863247863247863\n",
      "train: step: 245, loss: 0.3436734080314636, acc: 0.8515625, recall: 0.7971014492753623, precision: 0.9166666666666666, f_beta: 0.8527131782945736\n",
      "train: step: 246, loss: 0.3635966181755066, acc: 0.8203125, recall: 0.7878787878787878, precision: 0.8524590163934426, f_beta: 0.8188976377952757\n",
      "train: step: 247, loss: 0.3391408324241638, acc: 0.8515625, recall: 0.8461538461538461, precision: 0.859375, f_beta: 0.8527131782945736\n",
      "train: step: 248, loss: 0.3566350042819977, acc: 0.8671875, recall: 0.8305084745762712, precision: 0.875, f_beta: 0.8521739130434782\n",
      "train: step: 249, loss: 0.48984813690185547, acc: 0.7734375, recall: 0.7903225806451613, precision: 0.7538461538461538, f_beta: 0.7716535433070866\n",
      "train: step: 250, loss: 0.4307723939418793, acc: 0.8359375, recall: 0.8260869565217391, precision: 0.8636363636363636, f_beta: 0.8444444444444444\n",
      "train: step: 251, loss: 0.4400767683982849, acc: 0.78125, recall: 0.7727272727272727, precision: 0.796875, f_beta: 0.7846153846153846\n",
      "train: step: 252, loss: 0.43493181467056274, acc: 0.8046875, recall: 0.7777777777777778, precision: 0.8166666666666667, f_beta: 0.7967479674796747\n",
      "train: step: 253, loss: 0.25493013858795166, acc: 0.890625, recall: 0.9655172413793104, precision: 0.8235294117647058, f_beta: 0.888888888888889\n",
      "train: step: 254, loss: 0.34761109948158264, acc: 0.8515625, recall: 0.8507462686567164, precision: 0.8636363636363636, f_beta: 0.8571428571428571\n",
      "train: step: 255, loss: 0.3010374903678894, acc: 0.8671875, recall: 0.9402985074626866, precision: 0.8289473684210527, f_beta: 0.8811188811188811\n",
      "train: step: 256, loss: 0.41420653462409973, acc: 0.8203125, recall: 0.8269230769230769, precision: 0.7543859649122807, f_beta: 0.7889908256880734\n",
      "train: step: 257, loss: 0.36324411630630493, acc: 0.8203125, recall: 0.8666666666666667, precision: 0.7761194029850746, f_beta: 0.8188976377952756\n",
      "train: step: 258, loss: 0.2941979169845581, acc: 0.875, recall: 0.835820895522388, precision: 0.9180327868852459, f_beta: 0.875\n",
      "train: step: 259, loss: 0.33620142936706543, acc: 0.8515625, recall: 0.8135593220338984, precision: 0.8571428571428571, f_beta: 0.8347826086956522\n",
      "train: step: 260, loss: 0.34685128927230835, acc: 0.8515625, recall: 0.8095238095238095, precision: 0.8793103448275862, f_beta: 0.8429752066115702\n",
      "train: step: 261, loss: 0.32194891571998596, acc: 0.8828125, recall: 0.8, precision: 0.9411764705882353, f_beta: 0.8648648648648648\n",
      "train: step: 262, loss: 0.3432793915271759, acc: 0.859375, recall: 0.8636363636363636, precision: 0.8636363636363636, f_beta: 0.8636363636363636\n",
      "train: step: 263, loss: 0.3664383292198181, acc: 0.8515625, recall: 0.9122807017543859, precision: 0.7878787878787878, f_beta: 0.8455284552845528\n",
      "train: step: 264, loss: 0.35099801421165466, acc: 0.859375, recall: 0.9242424242424242, precision: 0.8243243243243243, f_beta: 0.8714285714285714\n",
      "train: step: 265, loss: 0.39952516555786133, acc: 0.828125, recall: 0.8507462686567164, precision: 0.8260869565217391, f_beta: 0.838235294117647\n",
      "train: step: 266, loss: 0.4627761244773865, acc: 0.7734375, recall: 0.8166666666666667, precision: 0.7313432835820896, f_beta: 0.7716535433070867\n",
      "train: step: 267, loss: 0.41585832834243774, acc: 0.8046875, recall: 0.875, precision: 0.7671232876712328, f_beta: 0.8175182481751825\n",
      "train: step: 268, loss: 0.34733855724334717, acc: 0.8515625, recall: 0.84375, precision: 0.8571428571428571, f_beta: 0.8503937007874015\n",
      "train: step: 269, loss: 0.2941210865974426, acc: 0.890625, recall: 0.9322033898305084, precision: 0.8461538461538461, f_beta: 0.8870967741935484\n",
      "train: step: 270, loss: 0.3969559669494629, acc: 0.8046875, recall: 0.8867924528301887, precision: 0.7121212121212122, f_beta: 0.7899159663865547\n",
      "train: step: 271, loss: 0.40145546197891235, acc: 0.828125, recall: 0.8450704225352113, precision: 0.8450704225352113, f_beta: 0.8450704225352113\n",
      "train: step: 272, loss: 0.5188891887664795, acc: 0.7421875, recall: 0.7611940298507462, precision: 0.75, f_beta: 0.7555555555555554\n",
      "train: step: 273, loss: 0.4199178218841553, acc: 0.78125, recall: 0.6875, precision: 0.8461538461538461, f_beta: 0.7586206896551724\n",
      "train: step: 274, loss: 0.3292670249938965, acc: 0.8359375, recall: 0.7777777777777778, precision: 0.8235294117647058, f_beta: 0.7999999999999999\n",
      "train: step: 275, loss: 0.32757481932640076, acc: 0.84375, recall: 0.8333333333333334, precision: 0.8823529411764706, f_beta: 0.8571428571428571\n",
      "train: step: 276, loss: 0.29527485370635986, acc: 0.8671875, recall: 0.9375, precision: 0.821917808219178, f_beta: 0.8759124087591241\n",
      "train: step: 277, loss: 0.4014517068862915, acc: 0.8203125, recall: 0.9032258064516129, precision: 0.7671232876712328, f_beta: 0.8296296296296296\n",
      "train: step: 278, loss: 0.33322960138320923, acc: 0.859375, recall: 0.8571428571428571, precision: 0.8823529411764706, f_beta: 0.8695652173913043\n",
      "train: step: 279, loss: 0.36301547288894653, acc: 0.8359375, recall: 0.8709677419354839, precision: 0.8059701492537313, f_beta: 0.8372093023255813\n",
      "train: step: 280, loss: 0.32568880915641785, acc: 0.8671875, recall: 0.8405797101449275, precision: 0.90625, f_beta: 0.8721804511278196\n",
      "train: step: 281, loss: 0.38584381341934204, acc: 0.84375, recall: 0.8636363636363636, precision: 0.8382352941176471, f_beta: 0.8507462686567164\n",
      "train: step: 282, loss: 0.4271884858608246, acc: 0.8046875, recall: 0.7230769230769231, precision: 0.8703703703703703, f_beta: 0.7899159663865546\n",
      "train: step: 283, loss: 0.33632129430770874, acc: 0.8828125, recall: 0.8507462686567164, precision: 0.9193548387096774, f_beta: 0.883720930232558\n",
      "train: step: 284, loss: 0.3240012526512146, acc: 0.8359375, recall: 0.8666666666666667, precision: 0.8, f_beta: 0.832\n",
      "train: step: 285, loss: 0.30052798986434937, acc: 0.8828125, recall: 0.9545454545454546, precision: 0.84, f_beta: 0.8936170212765958\n",
      "train: step: 286, loss: 0.34389713406562805, acc: 0.8671875, recall: 0.8793103448275862, precision: 0.8360655737704918, f_beta: 0.8571428571428572\n",
      "train: step: 287, loss: 0.32081878185272217, acc: 0.8359375, recall: 0.9444444444444444, precision: 0.7391304347826086, f_beta: 0.8292682926829269\n",
      "train: step: 288, loss: 0.2943715453147888, acc: 0.890625, recall: 0.9047619047619048, precision: 0.8769230769230769, f_beta: 0.890625\n",
      "train: step: 289, loss: 0.5211031436920166, acc: 0.765625, recall: 0.65625, precision: 0.84, f_beta: 0.736842105263158\n",
      "train: step: 290, loss: 0.33617621660232544, acc: 0.875, recall: 0.7758620689655172, precision: 0.9375, f_beta: 0.8490566037735848\n",
      "train: step: 291, loss: 0.3671344220638275, acc: 0.84375, recall: 0.796875, precision: 0.8793103448275862, f_beta: 0.8360655737704917\n",
      "train: step: 292, loss: 0.38940367102622986, acc: 0.796875, recall: 0.7761194029850746, precision: 0.8253968253968254, f_beta: 0.8\n",
      "train: step: 293, loss: 0.3238682150840759, acc: 0.8828125, recall: 0.9130434782608695, precision: 0.875, f_beta: 0.8936170212765957\n",
      "train: step: 294, loss: 0.2969664931297302, acc: 0.8671875, recall: 0.9577464788732394, precision: 0.8292682926829268, f_beta: 0.8888888888888888\n",
      "train: step: 295, loss: 0.3744734525680542, acc: 0.828125, recall: 0.9836065573770492, precision: 0.7407407407407407, f_beta: 0.8450704225352113\n",
      "train: step: 296, loss: 0.3452211916446686, acc: 0.828125, recall: 0.921875, precision: 0.7763157894736842, f_beta: 0.842857142857143\n",
      "train: step: 297, loss: 0.3900684714317322, acc: 0.8359375, recall: 0.9375, precision: 0.7792207792207793, f_beta: 0.851063829787234\n",
      "train: step: 298, loss: 0.3681686520576477, acc: 0.8203125, recall: 0.84375, precision: 0.8059701492537313, f_beta: 0.8244274809160305\n",
      "train: step: 299, loss: 0.45101046562194824, acc: 0.796875, recall: 0.6774193548387096, precision: 0.875, f_beta: 0.7636363636363636\n",
      "train: step: 300, loss: 0.4222603440284729, acc: 0.8046875, recall: 0.6724137931034483, precision: 0.8666666666666667, f_beta: 0.7572815533980582\n",
      "\n",
      " evaluate model...\n",
      "2019-09-24T11:57:03.557564, step: 300, loss: 0.3699264006736951, acc: 0.8409455128205128, precision: 0.8934113114329437, recall: 0.778083516777526, f_beta: 0.8305736693215088\n",
      "saved mdoel checkpoint tp C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\textCNN\\my_model-300\n",
      "\n",
      "train: step: 301, loss: 0.39764803647994995, acc: 0.8203125, recall: 0.7313432835820896, precision: 0.9074074074074074, f_beta: 0.8099173553719008\n",
      "train: step: 302, loss: 0.350445032119751, acc: 0.8515625, recall: 0.8253968253968254, precision: 0.8666666666666667, f_beta: 0.8455284552845528\n",
      "train: step: 303, loss: 0.28266650438308716, acc: 0.8671875, recall: 0.8928571428571429, precision: 0.819672131147541, f_beta: 0.8547008547008548\n",
      "train: step: 304, loss: 0.3272486925125122, acc: 0.859375, recall: 0.8636363636363636, precision: 0.8636363636363636, f_beta: 0.8636363636363636\n",
      "train: step: 305, loss: 0.3800721764564514, acc: 0.7890625, recall: 0.8448275862068966, precision: 0.7313432835820896, f_beta: 0.784\n",
      "train: step: 306, loss: 0.41612324118614197, acc: 0.8046875, recall: 0.8656716417910447, precision: 0.7837837837837838, f_beta: 0.8226950354609929\n",
      "train: step: 307, loss: 0.42661941051483154, acc: 0.7734375, recall: 0.9117647058823529, precision: 0.7294117647058823, f_beta: 0.8104575163398693\n",
      "train: step: 308, loss: 0.35685545206069946, acc: 0.890625, recall: 0.9242424242424242, precision: 0.8714285714285714, f_beta: 0.8970588235294117\n",
      "train: step: 309, loss: 0.3566935062408447, acc: 0.8515625, recall: 0.6862745098039216, precision: 0.9210526315789473, f_beta: 0.7865168539325842\n",
      "train: step: 310, loss: 0.36180368065834045, acc: 0.8203125, recall: 0.75, precision: 0.8235294117647058, f_beta: 0.7850467289719627\n",
      "train: step: 311, loss: 0.379728227853775, acc: 0.8359375, recall: 0.7945205479452054, precision: 0.90625, f_beta: 0.8467153284671532\n",
      "train: step: 312, loss: 0.3395361006259918, acc: 0.8203125, recall: 0.8135593220338984, precision: 0.8, f_beta: 0.8067226890756303\n",
      "start to train models...\n",
      "train: step: 313, loss: 0.22858524322509766, acc: 0.8984375, recall: 0.9047619047619048, precision: 0.890625, f_beta: 0.8976377952755906\n",
      "train: step: 314, loss: 0.2515726089477539, acc: 0.875, recall: 0.8305084745762712, precision: 0.8909090909090909, f_beta: 0.8596491228070176\n",
      "train: step: 315, loss: 0.3336729407310486, acc: 0.8359375, recall: 0.9074074074074074, precision: 0.7538461538461538, f_beta: 0.8235294117647058\n",
      "train: step: 316, loss: 0.21550846099853516, acc: 0.9296875, recall: 0.953125, precision: 0.9104477611940298, f_beta: 0.931297709923664\n",
      "train: step: 317, loss: 0.328851580619812, acc: 0.859375, recall: 0.9047619047619048, precision: 0.8260869565217391, f_beta: 0.8636363636363636\n",
      "train: step: 318, loss: 0.28387650847435, acc: 0.90625, recall: 0.9178082191780822, precision: 0.9178082191780822, f_beta: 0.9178082191780822\n",
      "train: step: 319, loss: 0.3076704144477844, acc: 0.8671875, recall: 0.8769230769230769, precision: 0.8636363636363636, f_beta: 0.8702290076335878\n",
      "train: step: 320, loss: 0.2751898765563965, acc: 0.890625, recall: 0.9152542372881356, precision: 0.8571428571428571, f_beta: 0.8852459016393444\n",
      "train: step: 321, loss: 0.3102867007255554, acc: 0.84375, recall: 0.835820895522388, precision: 0.8615384615384616, f_beta: 0.8484848484848485\n",
      "train: step: 322, loss: 0.2289169281721115, acc: 0.921875, recall: 0.9032258064516129, precision: 0.9333333333333333, f_beta: 0.9180327868852459\n",
      "train: step: 323, loss: 0.3255480229854584, acc: 0.8671875, recall: 0.8135593220338984, precision: 0.8888888888888888, f_beta: 0.8495575221238938\n",
      "train: step: 324, loss: 0.2246365249156952, acc: 0.9140625, recall: 0.9516129032258065, precision: 0.8805970149253731, f_beta: 0.9147286821705426\n",
      "train: step: 325, loss: 0.3221302926540375, acc: 0.875, recall: 0.8852459016393442, precision: 0.8571428571428571, f_beta: 0.8709677419354839\n",
      "train: step: 326, loss: 0.24270841479301453, acc: 0.8984375, recall: 0.9047619047619048, precision: 0.890625, f_beta: 0.8976377952755906\n",
      "train: step: 327, loss: 0.3156789243221283, acc: 0.8828125, recall: 0.8591549295774648, precision: 0.9242424242424242, f_beta: 0.8905109489051095\n",
      "train: step: 328, loss: 0.3020045757293701, acc: 0.859375, recall: 0.8805970149253731, precision: 0.855072463768116, f_beta: 0.8676470588235295\n",
      "train: step: 329, loss: 0.24488762021064758, acc: 0.8984375, recall: 0.9253731343283582, precision: 0.8857142857142857, f_beta: 0.9051094890510949\n",
      "train: step: 330, loss: 0.34220391511917114, acc: 0.890625, recall: 0.9333333333333333, precision: 0.8484848484848485, f_beta: 0.888888888888889\n",
      "train: step: 331, loss: 0.3438190221786499, acc: 0.84375, recall: 0.8955223880597015, precision: 0.821917808219178, f_beta: 0.8571428571428571\n",
      "train: step: 332, loss: 0.2778169512748718, acc: 0.90625, recall: 0.9253731343283582, precision: 0.8985507246376812, f_beta: 0.9117647058823529\n",
      "train: step: 333, loss: 0.2290714979171753, acc: 0.8828125, recall: 0.863013698630137, precision: 0.9264705882352942, f_beta: 0.8936170212765958\n",
      "train: step: 334, loss: 0.22641299664974213, acc: 0.9375, recall: 0.9672131147540983, precision: 0.9076923076923077, f_beta: 0.9365079365079365\n",
      "train: step: 335, loss: 0.32414141297340393, acc: 0.859375, recall: 0.8793103448275862, precision: 0.8225806451612904, f_beta: 0.8500000000000001\n",
      "train: step: 336, loss: 0.19964946806430817, acc: 0.90625, recall: 0.9310344827586207, precision: 0.8709677419354839, f_beta: 0.9\n",
      "train: step: 337, loss: 0.35349738597869873, acc: 0.84375, recall: 0.7794117647058824, precision: 0.9137931034482759, f_beta: 0.8412698412698414\n",
      "train: step: 338, loss: 0.2027985155582428, acc: 0.921875, recall: 0.875, precision: 0.9655172413793104, f_beta: 0.9180327868852458\n",
      "train: step: 339, loss: 0.28813204169273376, acc: 0.8515625, recall: 0.8, precision: 0.896551724137931, f_beta: 0.8455284552845529\n",
      "train: step: 340, loss: 0.29758507013320923, acc: 0.8671875, recall: 0.8833333333333333, precision: 0.8412698412698413, f_beta: 0.8617886178861788\n",
      "train: step: 341, loss: 0.32712671160697937, acc: 0.859375, recall: 0.875, precision: 0.8166666666666667, f_beta: 0.8448275862068966\n",
      "train: step: 342, loss: 0.3104640245437622, acc: 0.8515625, recall: 0.8852459016393442, precision: 0.8181818181818182, f_beta: 0.8503937007874016\n",
      "train: step: 343, loss: 0.28045469522476196, acc: 0.890625, recall: 0.9393939393939394, precision: 0.8611111111111112, f_beta: 0.8985507246376813\n",
      "train: step: 344, loss: 0.24121806025505066, acc: 0.890625, recall: 0.967741935483871, precision: 0.8333333333333334, f_beta: 0.8955223880597015\n",
      "train: step: 345, loss: 0.2815435826778412, acc: 0.890625, recall: 0.8904109589041096, precision: 0.9154929577464789, f_beta: 0.9027777777777778\n",
      "train: step: 346, loss: 0.2253681868314743, acc: 0.90625, recall: 0.8421052631578947, precision: 0.9411764705882353, f_beta: 0.8888888888888888\n",
      "train: step: 347, loss: 0.28804680705070496, acc: 0.9140625, recall: 0.8676470588235294, precision: 0.9672131147540983, f_beta: 0.9147286821705426\n",
      "train: step: 348, loss: 0.2410736382007599, acc: 0.890625, recall: 0.9180327868852459, precision: 0.8615384615384616, f_beta: 0.8888888888888888\n",
      "train: step: 349, loss: 0.27371907234191895, acc: 0.8671875, recall: 0.8676470588235294, precision: 0.8805970149253731, f_beta: 0.874074074074074\n",
      "train: step: 350, loss: 0.18401247262954712, acc: 0.9453125, recall: 0.9285714285714286, precision: 0.9701492537313433, f_beta: 0.948905109489051\n",
      "train: step: 351, loss: 0.3015216588973999, acc: 0.859375, recall: 0.8888888888888888, precision: 0.8648648648648649, f_beta: 0.8767123287671232\n",
      "train: step: 352, loss: 0.3735698461532593, acc: 0.8203125, recall: 0.8867924528301887, precision: 0.734375, f_beta: 0.8034188034188035\n",
      "train: step: 353, loss: 0.26243799924850464, acc: 0.921875, recall: 0.9402985074626866, precision: 0.9130434782608695, f_beta: 0.9264705882352942\n",
      "train: step: 354, loss: 0.2252151519060135, acc: 0.8984375, recall: 0.9310344827586207, precision: 0.8571428571428571, f_beta: 0.8925619834710743\n",
      "train: step: 355, loss: 0.3217447102069855, acc: 0.84375, recall: 0.8484848484848485, precision: 0.8484848484848485, f_beta: 0.8484848484848486\n",
      "train: step: 356, loss: 0.3674512803554535, acc: 0.84375, recall: 0.828125, precision: 0.8548387096774194, f_beta: 0.8412698412698412\n",
      "train: step: 357, loss: 0.37085670232772827, acc: 0.796875, recall: 0.7017543859649122, precision: 0.8163265306122449, f_beta: 0.7547169811320754\n",
      "train: step: 358, loss: 0.2444409728050232, acc: 0.921875, recall: 0.890625, precision: 0.95, f_beta: 0.9193548387096774\n",
      "train: step: 359, loss: 0.31801527738571167, acc: 0.875, recall: 0.8769230769230769, precision: 0.8769230769230769, f_beta: 0.8769230769230769\n",
      "train: step: 360, loss: 0.2671661674976349, acc: 0.8828125, recall: 0.9482758620689655, precision: 0.8208955223880597, f_beta: 0.8799999999999999\n",
      "train: step: 361, loss: 0.261677622795105, acc: 0.875, recall: 0.9523809523809523, precision: 0.821917808219178, f_beta: 0.8823529411764706\n",
      "train: step: 362, loss: 0.2890323996543884, acc: 0.8671875, recall: 0.8823529411764706, precision: 0.8695652173913043, f_beta: 0.8759124087591241\n",
      "train: step: 363, loss: 0.32705551385879517, acc: 0.8671875, recall: 0.8695652173913043, precision: 0.8823529411764706, f_beta: 0.8759124087591241\n",
      "train: step: 364, loss: 0.29502320289611816, acc: 0.90625, recall: 0.9245283018867925, precision: 0.8596491228070176, f_beta: 0.8909090909090909\n",
      "train: step: 365, loss: 0.2651084363460541, acc: 0.90625, recall: 0.8709677419354839, precision: 0.9310344827586207, f_beta: 0.9\n",
      "train: step: 366, loss: 0.21851211786270142, acc: 0.9140625, recall: 0.8852459016393442, precision: 0.9310344827586207, f_beta: 0.9075630252100839\n",
      "train: step: 367, loss: 0.2982581555843353, acc: 0.8515625, recall: 0.8378378378378378, precision: 0.8985507246376812, f_beta: 0.8671328671328672\n",
      "train: step: 368, loss: 0.23201987147331238, acc: 0.921875, recall: 0.9152542372881356, precision: 0.9152542372881356, f_beta: 0.9152542372881356\n",
      "train: step: 369, loss: 0.2526731789112091, acc: 0.890625, recall: 0.9253731343283582, precision: 0.8732394366197183, f_beta: 0.8985507246376812\n",
      "train: step: 370, loss: 0.3042331635951996, acc: 0.84375, recall: 0.9365079365079365, precision: 0.7866666666666666, f_beta: 0.8550724637681159\n",
      "train: step: 371, loss: 0.3327949047088623, acc: 0.8515625, recall: 0.7962962962962963, precision: 0.8431372549019608, f_beta: 0.8190476190476189\n",
      "train: step: 372, loss: 0.23920488357543945, acc: 0.875, recall: 0.8928571428571429, precision: 0.8333333333333334, f_beta: 0.8620689655172413\n",
      "train: step: 373, loss: 0.3261955678462982, acc: 0.890625, recall: 0.8666666666666667, precision: 0.896551724137931, f_beta: 0.8813559322033899\n",
      "train: step: 374, loss: 0.2589617073535919, acc: 0.890625, recall: 0.864406779661017, precision: 0.8947368421052632, f_beta: 0.8793103448275862\n",
      "train: step: 375, loss: 0.3396602272987366, acc: 0.8515625, recall: 0.7647058823529411, precision: 0.9454545454545454, f_beta: 0.8455284552845528\n",
      "train: step: 376, loss: 0.2660573422908783, acc: 0.875, recall: 0.9206349206349206, precision: 0.8405797101449275, f_beta: 0.8787878787878787\n",
      "train: step: 377, loss: 0.38659483194351196, acc: 0.8359375, recall: 0.8, precision: 0.8666666666666667, f_beta: 0.832\n",
      "train: step: 378, loss: 0.24059182405471802, acc: 0.90625, recall: 0.8904109589041096, precision: 0.9420289855072463, f_beta: 0.9154929577464788\n",
      "train: step: 379, loss: 0.2901035249233246, acc: 0.90625, recall: 0.9259259259259259, precision: 0.8620689655172413, f_beta: 0.8928571428571429\n",
      "train: step: 380, loss: 0.3548133075237274, acc: 0.8203125, recall: 0.9104477611940298, precision: 0.782051282051282, f_beta: 0.8413793103448275\n",
      "train: step: 381, loss: 0.289762020111084, acc: 0.9140625, recall: 0.9852941176470589, precision: 0.8701298701298701, f_beta: 0.9241379310344828\n",
      "train: step: 382, loss: 0.33981746435165405, acc: 0.84375, recall: 0.8548387096774194, precision: 0.828125, f_beta: 0.8412698412698412\n",
      "train: step: 383, loss: 0.3359968066215515, acc: 0.828125, recall: 0.7808219178082192, precision: 0.9047619047619048, f_beta: 0.8382352941176471\n",
      "train: step: 384, loss: 0.2897706925868988, acc: 0.8671875, recall: 0.828125, precision: 0.8983050847457628, f_beta: 0.8617886178861789\n",
      "train: step: 385, loss: 0.2248319685459137, acc: 0.9375, recall: 0.9242424242424242, precision: 0.953125, f_beta: 0.9384615384615383\n",
      "train: step: 386, loss: 0.2660555839538574, acc: 0.8828125, recall: 0.847457627118644, precision: 0.8928571428571429, f_beta: 0.8695652173913044\n",
      "train: step: 387, loss: 0.2818448543548584, acc: 0.8828125, recall: 0.9344262295081968, precision: 0.8382352941176471, f_beta: 0.8837209302325582\n",
      "train: step: 388, loss: 0.27942711114883423, acc: 0.8671875, recall: 0.9038461538461539, precision: 0.7966101694915254, f_beta: 0.8468468468468469\n",
      "train: step: 389, loss: 0.2827761173248291, acc: 0.8515625, recall: 0.855072463768116, precision: 0.8676470588235294, f_beta: 0.8613138686131386\n",
      "train: step: 390, loss: 0.2306496500968933, acc: 0.9296875, recall: 0.9365079365079365, precision: 0.921875, f_beta: 0.9291338582677166\n",
      "train: step: 391, loss: 0.23874911665916443, acc: 0.921875, recall: 0.8823529411764706, precision: 0.967741935483871, f_beta: 0.923076923076923\n",
      "train: step: 392, loss: 0.3067047894001007, acc: 0.84375, recall: 0.8653846153846154, precision: 0.7758620689655172, f_beta: 0.8181818181818181\n",
      "train: step: 393, loss: 0.246568500995636, acc: 0.90625, recall: 0.9629629629629629, precision: 0.8387096774193549, f_beta: 0.896551724137931\n",
      "train: step: 394, loss: 0.32101354002952576, acc: 0.859375, recall: 0.8688524590163934, precision: 0.8412698412698413, f_beta: 0.8548387096774194\n",
      "train: step: 395, loss: 0.23031093180179596, acc: 0.9140625, recall: 0.8870967741935484, precision: 0.9322033898305084, f_beta: 0.9090909090909092\n",
      "train: step: 396, loss: 0.3615444600582123, acc: 0.8671875, recall: 0.8194444444444444, precision: 0.9365079365079365, f_beta: 0.874074074074074\n",
      "train: step: 397, loss: 0.3286014795303345, acc: 0.8828125, recall: 0.8484848484848485, precision: 0.9180327868852459, f_beta: 0.8818897637795275\n",
      "train: step: 398, loss: 0.3000008463859558, acc: 0.875, recall: 0.9259259259259259, precision: 0.8064516129032258, f_beta: 0.8620689655172414\n",
      "train: step: 399, loss: 0.2215379774570465, acc: 0.8984375, recall: 0.9122807017543859, precision: 0.8666666666666667, f_beta: 0.8888888888888888\n",
      "train: step: 400, loss: 0.3274170756340027, acc: 0.875, recall: 0.8648648648648649, precision: 0.9142857142857143, f_beta: 0.888888888888889\n",
      "\n",
      " evaluate model...\n",
      "2019-09-24T11:59:01.121323, step: 400, loss: 0.337326990870329, acc: 0.8563701923076923, precision: 0.8363528844350139, recall: 0.8887603441822177, f_beta: 0.8610654221412005\n",
      "saved mdoel checkpoint tp C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\textCNN\\my_model-400\n",
      "\n",
      "train: step: 401, loss: 0.21485833823680878, acc: 0.9140625, recall: 0.9473684210526315, precision: 0.8709677419354839, f_beta: 0.9075630252100839\n",
      "train: step: 402, loss: 0.26621267199516296, acc: 0.90625, recall: 0.9142857142857143, precision: 0.9142857142857143, f_beta: 0.9142857142857143\n",
      "train: step: 403, loss: 0.3189062476158142, acc: 0.8828125, recall: 0.9041095890410958, precision: 0.8918918918918919, f_beta: 0.8979591836734694\n",
      "train: step: 404, loss: 0.33210161328315735, acc: 0.875, recall: 0.9137931034482759, precision: 0.828125, f_beta: 0.8688524590163935\n",
      "train: step: 405, loss: 0.2776423692703247, acc: 0.8828125, recall: 0.9666666666666667, precision: 0.8169014084507042, f_beta: 0.8854961832061069\n",
      "train: step: 406, loss: 0.22387489676475525, acc: 0.90625, recall: 0.9285714285714286, precision: 0.8666666666666667, f_beta: 0.896551724137931\n",
      "train: step: 407, loss: 0.2530015707015991, acc: 0.9140625, recall: 0.9193548387096774, precision: 0.9047619047619048, f_beta: 0.912\n",
      "train: step: 408, loss: 0.31194862723350525, acc: 0.84375, recall: 0.7846153846153846, precision: 0.8947368421052632, f_beta: 0.8360655737704918\n",
      "train: step: 409, loss: 0.3276631832122803, acc: 0.875, recall: 0.8194444444444444, precision: 0.9516129032258065, f_beta: 0.8805970149253732\n",
      "train: step: 410, loss: 0.268404096364975, acc: 0.890625, recall: 0.8235294117647058, precision: 0.9655172413793104, f_beta: 0.888888888888889\n",
      "train: step: 411, loss: 0.32284969091415405, acc: 0.828125, recall: 0.8032786885245902, precision: 0.8305084745762712, f_beta: 0.8166666666666667\n",
      "train: step: 412, loss: 0.2770267128944397, acc: 0.8828125, recall: 0.9310344827586207, precision: 0.8307692307692308, f_beta: 0.8780487804878049\n",
      "train: step: 413, loss: 0.236472487449646, acc: 0.9140625, recall: 0.9545454545454546, precision: 0.8873239436619719, f_beta: 0.9197080291970803\n",
      "train: step: 414, loss: 0.26416054368019104, acc: 0.8828125, recall: 0.9411764705882353, precision: 0.8, f_beta: 0.8648648648648648\n",
      "train: step: 415, loss: 0.20978713035583496, acc: 0.9375, recall: 0.9714285714285714, precision: 0.918918918918919, f_beta: 0.9444444444444445\n",
      "train: step: 416, loss: 0.2100289762020111, acc: 0.9453125, recall: 0.9625, precision: 0.9506172839506173, f_beta: 0.9565217391304348\n",
      "train: step: 417, loss: 0.31674420833587646, acc: 0.8671875, recall: 0.8333333333333334, precision: 0.9016393442622951, f_beta: 0.8661417322834646\n",
      "train: step: 418, loss: 0.31117627024650574, acc: 0.859375, recall: 0.9090909090909091, precision: 0.7936507936507936, f_beta: 0.847457627118644\n",
      "train: step: 419, loss: 0.31154492497444153, acc: 0.90625, recall: 0.9466666666666667, precision: 0.8987341772151899, f_beta: 0.9220779220779222\n",
      "train: step: 420, loss: 0.29699161648750305, acc: 0.8828125, recall: 0.8493150684931506, precision: 0.9393939393939394, f_beta: 0.8920863309352518\n",
      "train: step: 421, loss: 0.2949996292591095, acc: 0.859375, recall: 0.8026315789473685, precision: 0.953125, f_beta: 0.8714285714285714\n",
      "train: step: 422, loss: 0.23130324482917786, acc: 0.9140625, recall: 0.9310344827586207, precision: 0.8852459016393442, f_beta: 0.9075630252100839\n",
      "train: step: 423, loss: 0.2583039104938507, acc: 0.890625, recall: 0.9305555555555556, precision: 0.881578947368421, f_beta: 0.9054054054054054\n",
      "train: step: 424, loss: 0.26019051671028137, acc: 0.921875, recall: 0.9672131147540983, precision: 0.8805970149253731, f_beta: 0.9218749999999999\n",
      "train: step: 425, loss: 0.26918309926986694, acc: 0.875, recall: 0.9538461538461539, precision: 0.8266666666666667, f_beta: 0.8857142857142857\n",
      "train: step: 426, loss: 0.23262658715248108, acc: 0.90625, recall: 0.9076923076923077, precision: 0.9076923076923077, f_beta: 0.9076923076923076\n",
      "train: step: 427, loss: 0.24706056714057922, acc: 0.8984375, recall: 0.9253731343283582, precision: 0.8857142857142857, f_beta: 0.9051094890510949\n",
      "train: step: 428, loss: 0.28378909826278687, acc: 0.890625, recall: 0.9166666666666666, precision: 0.859375, f_beta: 0.8870967741935484\n",
      "train: step: 429, loss: 0.24942556023597717, acc: 0.90625, recall: 0.9090909090909091, precision: 0.9090909090909091, f_beta: 0.9090909090909091\n",
      "train: step: 430, loss: 0.342325359582901, acc: 0.875, recall: 0.8787878787878788, precision: 0.8787878787878788, f_beta: 0.8787878787878788\n",
      "train: step: 431, loss: 0.3424583673477173, acc: 0.828125, recall: 0.8387096774193549, precision: 0.8125, f_beta: 0.8253968253968254\n",
      "train: step: 432, loss: 0.24091948568820953, acc: 0.8828125, recall: 0.8615384615384616, precision: 0.9032258064516129, f_beta: 0.8818897637795274\n",
      "train: step: 433, loss: 0.2021060585975647, acc: 0.9375, recall: 0.95, precision: 0.9193548387096774, f_beta: 0.9344262295081968\n",
      "train: step: 434, loss: 0.23815886676311493, acc: 0.921875, recall: 0.90625, precision: 0.9354838709677419, f_beta: 0.9206349206349206\n",
      "train: step: 435, loss: 0.27642449736595154, acc: 0.8828125, recall: 0.9433962264150944, precision: 0.8064516129032258, f_beta: 0.8695652173913043\n",
      "train: step: 436, loss: 0.23408012092113495, acc: 0.921875, recall: 0.8783783783783784, precision: 0.9848484848484849, f_beta: 0.9285714285714285\n",
      "train: step: 437, loss: 0.23350673913955688, acc: 0.921875, recall: 0.927536231884058, precision: 0.927536231884058, f_beta: 0.927536231884058\n",
      "train: step: 438, loss: 0.2823876440525055, acc: 0.875, recall: 0.9180327868852459, precision: 0.835820895522388, f_beta: 0.875\n",
      "train: step: 439, loss: 0.22056740522384644, acc: 0.8984375, recall: 0.9523809523809523, precision: 0.8571428571428571, f_beta: 0.9022556390977443\n",
      "train: step: 440, loss: 0.22336682677268982, acc: 0.921875, recall: 0.9436619718309859, precision: 0.9178082191780822, f_beta: 0.9305555555555556\n",
      "train: step: 441, loss: 0.30045169591903687, acc: 0.875, recall: 0.9272727272727272, precision: 0.8095238095238095, f_beta: 0.864406779661017\n",
      "train: step: 442, loss: 0.2958982586860657, acc: 0.890625, recall: 0.8656716417910447, precision: 0.9206349206349206, f_beta: 0.8923076923076922\n",
      "train: step: 443, loss: 0.2534644305706024, acc: 0.9140625, recall: 0.8676470588235294, precision: 0.9672131147540983, f_beta: 0.9147286821705426\n",
      "train: step: 444, loss: 0.3352953791618347, acc: 0.875, recall: 0.8441558441558441, precision: 0.9420289855072463, f_beta: 0.8904109589041096\n",
      "train: step: 445, loss: 0.28527504205703735, acc: 0.8828125, recall: 0.9558823529411765, precision: 0.8441558441558441, f_beta: 0.8965517241379309\n",
      "train: step: 446, loss: 0.2960596978664398, acc: 0.8515625, recall: 0.8709677419354839, precision: 0.8307692307692308, f_beta: 0.8503937007874016\n",
      "train: step: 447, loss: 0.32462239265441895, acc: 0.8671875, recall: 0.8985507246376812, precision: 0.8611111111111112, f_beta: 0.8794326241134751\n",
      "train: step: 448, loss: 0.2735540270805359, acc: 0.890625, recall: 0.927536231884058, precision: 0.8767123287671232, f_beta: 0.9014084507042253\n",
      "train: step: 449, loss: 0.25932979583740234, acc: 0.921875, recall: 0.971830985915493, precision: 0.8961038961038961, f_beta: 0.9324324324324325\n",
      "train: step: 450, loss: 0.27865660190582275, acc: 0.890625, recall: 0.9074074074074074, precision: 0.8448275862068966, f_beta: 0.875\n",
      "train: step: 451, loss: 0.26820194721221924, acc: 0.875, recall: 0.8529411764705882, precision: 0.90625, f_beta: 0.8787878787878787\n",
      "train: step: 452, loss: 0.23055781424045563, acc: 0.921875, recall: 0.8545454545454545, precision: 0.9591836734693877, f_beta: 0.9038461538461537\n",
      "train: step: 453, loss: 0.3248918056488037, acc: 0.890625, recall: 0.8153846153846154, precision: 0.9636363636363636, f_beta: 0.8833333333333333\n",
      "train: step: 454, loss: 0.34926095604896545, acc: 0.828125, recall: 0.7761194029850746, precision: 0.8813559322033898, f_beta: 0.8253968253968255\n",
      "train: step: 455, loss: 0.3350082337856293, acc: 0.8984375, recall: 0.8666666666666667, precision: 0.9122807017543859, f_beta: 0.8888888888888888\n",
      "train: step: 456, loss: 0.24025669693946838, acc: 0.8984375, recall: 0.9322033898305084, precision: 0.859375, f_beta: 0.8943089430894309\n",
      "train: step: 457, loss: 0.260434627532959, acc: 0.8984375, recall: 0.9041095890410958, precision: 0.9166666666666666, f_beta: 0.9103448275862068\n",
      "train: step: 458, loss: 0.2731173634529114, acc: 0.8984375, recall: 0.9393939393939394, precision: 0.8732394366197183, f_beta: 0.9051094890510948\n",
      "train: step: 459, loss: 0.26651662588119507, acc: 0.90625, recall: 0.8955223880597015, precision: 0.9230769230769231, f_beta: 0.9090909090909091\n",
      "train: step: 460, loss: 0.3075864911079407, acc: 0.859375, recall: 0.8787878787878788, precision: 0.8529411764705882, f_beta: 0.8656716417910447\n",
      "train: step: 461, loss: 0.28520673513412476, acc: 0.890625, recall: 0.9310344827586207, precision: 0.84375, f_beta: 0.8852459016393444\n",
      "train: step: 462, loss: 0.2640547454357147, acc: 0.8984375, recall: 0.9375, precision: 0.8695652173913043, f_beta: 0.9022556390977444\n",
      "train: step: 463, loss: 0.3122120797634125, acc: 0.8984375, recall: 0.8666666666666667, precision: 0.9122807017543859, f_beta: 0.8888888888888888\n",
      "train: step: 464, loss: 0.2685077488422394, acc: 0.8984375, recall: 0.9117647058823529, precision: 0.8985507246376812, f_beta: 0.9051094890510949\n",
      "train: step: 465, loss: 0.29012978076934814, acc: 0.859375, recall: 0.8478260869565217, precision: 0.78, f_beta: 0.8125\n",
      "train: step: 466, loss: 0.24031879007816315, acc: 0.9140625, recall: 0.8787878787878788, precision: 0.9508196721311475, f_beta: 0.9133858267716536\n",
      "train: step: 467, loss: 0.26470476388931274, acc: 0.890625, recall: 0.828125, precision: 0.9464285714285714, f_beta: 0.8833333333333333\n",
      "train: step: 468, loss: 0.2733883857727051, acc: 0.8828125, recall: 0.8656716417910447, precision: 0.90625, f_beta: 0.8854961832061069\n",
      "start to train models...\n",
      "train: step: 469, loss: 0.24114669859409332, acc: 0.921875, recall: 0.9672131147540983, precision: 0.8805970149253731, f_beta: 0.9218749999999999\n",
      "train: step: 470, loss: 0.1745033860206604, acc: 0.9453125, recall: 0.9655172413793104, precision: 0.9180327868852459, f_beta: 0.9411764705882353\n",
      "train: step: 471, loss: 0.26920071244239807, acc: 0.9296875, recall: 0.9117647058823529, precision: 0.9538461538461539, f_beta: 0.9323308270676691\n",
      "train: step: 472, loss: 0.2278057485818863, acc: 0.890625, recall: 0.9661016949152542, precision: 0.8260869565217391, f_beta: 0.890625\n",
      "train: step: 473, loss: 0.2511042356491089, acc: 0.890625, recall: 0.96875, precision: 0.8378378378378378, f_beta: 0.8985507246376812\n",
      "train: step: 474, loss: 0.19037002325057983, acc: 0.9140625, recall: 0.9354838709677419, precision: 0.8923076923076924, f_beta: 0.9133858267716536\n",
      "train: step: 475, loss: 0.186380535364151, acc: 0.921875, recall: 0.8852459016393442, precision: 0.9473684210526315, f_beta: 0.9152542372881356\n",
      "train: step: 476, loss: 0.2363566756248474, acc: 0.90625, recall: 0.864406779661017, precision: 0.9272727272727272, f_beta: 0.8947368421052632\n",
      "train: step: 477, loss: 0.2298237383365631, acc: 0.9453125, recall: 0.9166666666666666, precision: 0.9850746268656716, f_beta: 0.949640287769784\n",
      "train: step: 478, loss: 0.19376453757286072, acc: 0.9375, recall: 0.8888888888888888, precision: 0.9824561403508771, f_beta: 0.9333333333333333\n",
      "train: step: 479, loss: 0.21091008186340332, acc: 0.9296875, recall: 0.9272727272727272, precision: 0.9107142857142857, f_beta: 0.918918918918919\n",
      "train: step: 480, loss: 0.27536773681640625, acc: 0.921875, recall: 0.9102564102564102, precision: 0.9594594594594594, f_beta: 0.9342105263157894\n",
      "train: step: 481, loss: 0.14303269982337952, acc: 0.9609375, recall: 0.9864864864864865, precision: 0.948051948051948, f_beta: 0.9668874172185431\n",
      "train: step: 482, loss: 0.26813805103302, acc: 0.8828125, recall: 0.9508196721311475, precision: 0.8285714285714286, f_beta: 0.885496183206107\n",
      "train: step: 483, loss: 0.24094054102897644, acc: 0.9296875, recall: 0.9516129032258065, precision: 0.9076923076923077, f_beta: 0.9291338582677167\n",
      "train: step: 484, loss: 0.21819768846035004, acc: 0.921875, recall: 0.9076923076923077, precision: 0.9365079365079365, f_beta: 0.9218749999999999\n",
      "train: step: 485, loss: 0.16818460822105408, acc: 0.953125, recall: 0.9558823529411765, precision: 0.9558823529411765, f_beta: 0.9558823529411765\n",
      "train: step: 486, loss: 0.17899394035339355, acc: 0.9296875, recall: 0.95, precision: 0.9047619047619048, f_beta: 0.9268292682926829\n",
      "train: step: 487, loss: 0.2021702527999878, acc: 0.9375, recall: 0.9074074074074074, precision: 0.9423076923076923, f_beta: 0.9245283018867925\n",
      "train: step: 488, loss: 0.179942324757576, acc: 0.953125, recall: 0.9354838709677419, precision: 0.9666666666666667, f_beta: 0.9508196721311476\n",
      "train: step: 489, loss: 0.15306618809700012, acc: 0.953125, recall: 0.9193548387096774, precision: 0.9827586206896551, f_beta: 0.95\n",
      "train: step: 490, loss: 0.19557905197143555, acc: 0.9296875, recall: 0.90625, precision: 0.9508196721311475, f_beta: 0.9279999999999999\n",
      "train: step: 491, loss: 0.17595171928405762, acc: 0.9453125, recall: 0.9523809523809523, precision: 0.9375, f_beta: 0.9448818897637795\n",
      "train: step: 492, loss: 0.22656646370887756, acc: 0.9296875, recall: 0.9552238805970149, precision: 0.9142857142857143, f_beta: 0.9343065693430657\n",
      "train: step: 493, loss: 0.2308158427476883, acc: 0.875, recall: 0.9107142857142857, precision: 0.8225806451612904, f_beta: 0.864406779661017\n",
      "train: step: 494, loss: 0.15008169412612915, acc: 0.9609375, recall: 0.9428571428571428, precision: 0.9850746268656716, f_beta: 0.9635036496350364\n",
      "train: step: 495, loss: 0.2497105598449707, acc: 0.90625, recall: 0.9692307692307692, precision: 0.863013698630137, f_beta: 0.9130434782608695\n",
      "train: step: 496, loss: 0.197494238615036, acc: 0.9140625, recall: 0.9558823529411765, precision: 0.8904109589041096, f_beta: 0.9219858156028369\n",
      "train: step: 497, loss: 0.24903520941734314, acc: 0.90625, recall: 0.8979591836734694, precision: 0.8627450980392157, f_beta: 0.8799999999999999\n",
      "train: step: 498, loss: 0.2219250351190567, acc: 0.9140625, recall: 0.9166666666666666, precision: 0.9295774647887324, f_beta: 0.9230769230769231\n",
      "train: step: 499, loss: 0.20851126313209534, acc: 0.9296875, recall: 0.8923076923076924, precision: 0.9666666666666667, f_beta: 0.928\n",
      "train: step: 500, loss: 0.18933482468128204, acc: 0.9453125, recall: 0.9166666666666666, precision: 0.9649122807017544, f_beta: 0.9401709401709402\n",
      "\n",
      " evaluate model...\n",
      "2019-09-24T12:01:00.939212, step: 500, loss: 0.32993539709311265, acc: 0.8577724358974359, precision: 0.8790500517350025, recall: 0.8333985310473521, f_beta: 0.8546317860780721\n",
      "saved mdoel checkpoint tp C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\textCNN\\my_model-500\n",
      "\n",
      "train: step: 501, loss: 0.19077372550964355, acc: 0.9453125, recall: 0.9264705882352942, precision: 0.9692307692307692, f_beta: 0.9473684210526316\n",
      "train: step: 502, loss: 0.12125444412231445, acc: 0.984375, recall: 0.9722222222222222, precision: 1.0, f_beta: 0.9859154929577464\n",
      "train: step: 503, loss: 0.20276224613189697, acc: 0.90625, recall: 0.8852459016393442, precision: 0.9152542372881356, f_beta: 0.9\n",
      "train: step: 504, loss: 0.24276864528656006, acc: 0.9296875, recall: 0.9310344827586207, precision: 0.9152542372881356, f_beta: 0.923076923076923\n",
      "train: step: 505, loss: 0.2010459303855896, acc: 0.921875, recall: 1.0, precision: 0.8360655737704918, f_beta: 0.9107142857142857\n",
      "train: step: 506, loss: 0.17667396366596222, acc: 0.9296875, recall: 0.9444444444444444, precision: 0.9315068493150684, f_beta: 0.9379310344827586\n",
      "train: step: 507, loss: 0.16363999247550964, acc: 0.9609375, recall: 0.9428571428571428, precision: 0.9850746268656716, f_beta: 0.9635036496350364\n",
      "train: step: 508, loss: 0.20045119524002075, acc: 0.9296875, recall: 0.8918918918918919, precision: 0.9850746268656716, f_beta: 0.9361702127659575\n",
      "train: step: 509, loss: 0.2557876706123352, acc: 0.90625, recall: 0.9090909090909091, precision: 0.9090909090909091, f_beta: 0.9090909090909091\n",
      "train: step: 510, loss: 0.2197854369878769, acc: 0.9140625, recall: 0.8928571428571429, precision: 0.9090909090909091, f_beta: 0.9009009009009009\n",
      "train: step: 511, loss: 0.22428900003433228, acc: 0.90625, recall: 0.9375, precision: 0.8823529411764706, f_beta: 0.9090909090909091\n",
      "train: step: 512, loss: 0.23555457592010498, acc: 0.90625, recall: 0.9, precision: 0.9, f_beta: 0.9\n",
      "train: step: 513, loss: 0.2161514163017273, acc: 0.9296875, recall: 0.9384615384615385, precision: 0.9242424242424242, f_beta: 0.9312977099236641\n",
      "train: step: 514, loss: 0.1775151789188385, acc: 0.9453125, recall: 0.9696969696969697, precision: 0.927536231884058, f_beta: 0.9481481481481481\n",
      "train: step: 515, loss: 0.2145942747592926, acc: 0.9296875, recall: 0.9076923076923077, precision: 0.9516129032258065, f_beta: 0.9291338582677167\n",
      "train: step: 516, loss: 0.20006130635738373, acc: 0.921875, recall: 0.9571428571428572, precision: 0.9054054054054054, f_beta: 0.9305555555555555\n",
      "train: step: 517, loss: 0.17415648698806763, acc: 0.9296875, recall: 0.9696969696969697, precision: 0.9014084507042254, f_beta: 0.9343065693430657\n",
      "train: step: 518, loss: 0.1409071534872055, acc: 0.9609375, recall: 0.9264705882352942, precision: 1.0, f_beta: 0.9618320610687023\n",
      "train: step: 519, loss: 0.2548564672470093, acc: 0.890625, recall: 0.9411764705882353, precision: 0.8648648648648649, f_beta: 0.9014084507042254\n",
      "train: step: 520, loss: 0.16175684332847595, acc: 0.9375, recall: 0.9558823529411765, precision: 0.9285714285714286, f_beta: 0.9420289855072465\n",
      "train: step: 521, loss: 0.16102704405784607, acc: 0.953125, recall: 0.9666666666666667, precision: 0.9354838709677419, f_beta: 0.9508196721311476\n",
      "train: step: 522, loss: 0.2477141171693802, acc: 0.9140625, recall: 0.8888888888888888, precision: 0.9333333333333333, f_beta: 0.9105691056910569\n",
      "train: step: 523, loss: 0.17036360502243042, acc: 0.9453125, recall: 0.9038461538461539, precision: 0.9591836734693877, f_beta: 0.9306930693069307\n",
      "train: step: 524, loss: 0.17229589819908142, acc: 0.9296875, recall: 0.9166666666666666, precision: 0.9322033898305084, f_beta: 0.9243697478991596\n",
      "train: step: 525, loss: 0.18520362675189972, acc: 0.9296875, recall: 0.9230769230769231, precision: 0.9375, f_beta: 0.9302325581395349\n",
      "train: step: 526, loss: 0.15542885661125183, acc: 0.9453125, recall: 0.9714285714285714, precision: 0.9315068493150684, f_beta: 0.9510489510489512\n",
      "train: step: 527, loss: 0.20742756128311157, acc: 0.9453125, recall: 0.9152542372881356, precision: 0.9642857142857143, f_beta: 0.9391304347826087\n",
      "train: step: 528, loss: 0.1900728940963745, acc: 0.9296875, recall: 0.953125, precision: 0.9104477611940298, f_beta: 0.931297709923664\n",
      "train: step: 529, loss: 0.21286362409591675, acc: 0.921875, recall: 0.8923076923076924, precision: 0.9508196721311475, f_beta: 0.9206349206349206\n",
      "train: step: 530, loss: 0.20001783967018127, acc: 0.921875, recall: 0.9354838709677419, precision: 0.90625, f_beta: 0.9206349206349206\n",
      "train: step: 531, loss: 0.22133813798427582, acc: 0.890625, recall: 0.8596491228070176, precision: 0.8909090909090909, f_beta: 0.875\n",
      "train: step: 532, loss: 0.18297487497329712, acc: 0.9375, recall: 0.9649122807017544, precision: 0.9016393442622951, f_beta: 0.9322033898305084\n",
      "train: step: 533, loss: 0.15243008732795715, acc: 0.9765625, recall: 0.9552238805970149, precision: 1.0, f_beta: 0.9770992366412213\n",
      "train: step: 534, loss: 0.1660138964653015, acc: 0.9453125, recall: 0.9047619047619048, precision: 0.9827586206896551, f_beta: 0.9421487603305785\n",
      "train: step: 535, loss: 0.26516690850257874, acc: 0.8828125, recall: 0.8615384615384616, precision: 0.9032258064516129, f_beta: 0.8818897637795274\n",
      "train: step: 536, loss: 0.22206199169158936, acc: 0.9140625, recall: 0.9591836734693877, precision: 0.8392857142857143, f_beta: 0.8952380952380952\n",
      "train: step: 537, loss: 0.23111259937286377, acc: 0.9375, recall: 0.918918918918919, precision: 0.9714285714285714, f_beta: 0.9444444444444445\n",
      "train: step: 538, loss: 0.2255108505487442, acc: 0.9296875, recall: 0.9041095890410958, precision: 0.9705882352941176, f_beta: 0.9361702127659575\n",
      "train: step: 539, loss: 0.24004848301410675, acc: 0.8984375, recall: 0.953125, precision: 0.8591549295774648, f_beta: 0.9037037037037037\n",
      "train: step: 540, loss: 0.18955837190151215, acc: 0.9375, recall: 0.9491525423728814, precision: 0.9180327868852459, f_beta: 0.9333333333333333\n",
      "train: step: 541, loss: 0.21899759769439697, acc: 0.9375, recall: 0.9682539682539683, precision: 0.9104477611940298, f_beta: 0.9384615384615386\n",
      "train: step: 542, loss: 0.21317952871322632, acc: 0.921875, recall: 0.8769230769230769, precision: 0.9661016949152542, f_beta: 0.9193548387096773\n",
      "train: step: 543, loss: 0.15141412615776062, acc: 0.953125, recall: 0.9516129032258065, precision: 0.9516129032258065, f_beta: 0.9516129032258065\n",
      "train: step: 544, loss: 0.1755015254020691, acc: 0.9375, recall: 0.9558823529411765, precision: 0.9285714285714286, f_beta: 0.9420289855072465\n",
      "train: step: 545, loss: 0.24518349766731262, acc: 0.8984375, recall: 0.9384615384615385, precision: 0.8714285714285714, f_beta: 0.9037037037037037\n",
      "train: step: 546, loss: 0.19411666691303253, acc: 0.9375, recall: 0.9491525423728814, precision: 0.9180327868852459, f_beta: 0.9333333333333333\n",
      "train: step: 547, loss: 0.20451486110687256, acc: 0.8984375, recall: 0.8852459016393442, precision: 0.9, f_beta: 0.8925619834710743\n",
      "train: step: 548, loss: 0.17318490147590637, acc: 0.9609375, recall: 0.967741935483871, precision: 0.9523809523809523, f_beta: 0.96\n",
      "train: step: 549, loss: 0.21445982158184052, acc: 0.9140625, recall: 0.8688524590163934, precision: 0.9464285714285714, f_beta: 0.9059829059829059\n",
      "train: step: 550, loss: 0.18460074067115784, acc: 0.9375, recall: 0.9193548387096774, precision: 0.95, f_beta: 0.9344262295081968\n",
      "train: step: 551, loss: 0.20587033033370972, acc: 0.9140625, recall: 0.8923076923076924, precision: 0.9354838709677419, f_beta: 0.9133858267716536\n",
      "train: step: 552, loss: 0.1979048252105713, acc: 0.90625, recall: 0.9130434782608695, precision: 0.9130434782608695, f_beta: 0.9130434782608695\n",
      "train: step: 553, loss: 0.2323414385318756, acc: 0.9375, recall: 0.9117647058823529, precision: 0.96875, f_beta: 0.9393939393939394\n",
      "train: step: 554, loss: 0.20589850842952728, acc: 0.9140625, recall: 0.9423076923076923, precision: 0.8596491228070176, f_beta: 0.8990825688073394\n",
      "train: step: 555, loss: 0.2106897234916687, acc: 0.9140625, recall: 0.9344262295081968, precision: 0.890625, f_beta: 0.9120000000000001\n",
      "train: step: 556, loss: 0.20506656169891357, acc: 0.90625, recall: 0.9117647058823529, precision: 0.9117647058823529, f_beta: 0.9117647058823528\n",
      "train: step: 557, loss: 0.18866005539894104, acc: 0.9453125, recall: 0.9655172413793104, precision: 0.9180327868852459, f_beta: 0.9411764705882353\n",
      "train: step: 558, loss: 0.19891613721847534, acc: 0.953125, recall: 0.9722222222222222, precision: 0.9459459459459459, f_beta: 0.9589041095890412\n",
      "train: step: 559, loss: 0.17411410808563232, acc: 0.9765625, recall: 0.9848484848484849, precision: 0.9701492537313433, f_beta: 0.9774436090225564\n",
      "train: step: 560, loss: 0.16299208998680115, acc: 0.9453125, recall: 0.9324324324324325, precision: 0.971830985915493, f_beta: 0.9517241379310345\n",
      "train: step: 561, loss: 0.18195955455303192, acc: 0.9296875, recall: 0.9298245614035088, precision: 0.9137931034482759, f_beta: 0.9217391304347825\n",
      "train: step: 562, loss: 0.15280777215957642, acc: 0.9453125, recall: 0.9523809523809523, precision: 0.9375, f_beta: 0.9448818897637795\n",
      "train: step: 563, loss: 0.20697283744812012, acc: 0.9296875, recall: 0.8928571428571429, precision: 0.9433962264150944, f_beta: 0.9174311926605505\n",
      "train: step: 564, loss: 0.16366046667099, acc: 0.9296875, recall: 0.9565217391304348, precision: 0.9166666666666666, f_beta: 0.9361702127659574\n",
      "train: step: 565, loss: 0.20250943303108215, acc: 0.90625, recall: 0.9104477611940298, precision: 0.9104477611940298, f_beta: 0.9104477611940298\n",
      "train: step: 566, loss: 0.19148632884025574, acc: 0.90625, recall: 0.9354838709677419, precision: 0.8787878787878788, f_beta: 0.90625\n",
      "train: step: 567, loss: 0.17953276634216309, acc: 0.9296875, recall: 0.9466666666666667, precision: 0.9342105263157895, f_beta: 0.9403973509933775\n",
      "train: step: 568, loss: 0.25118595361709595, acc: 0.921875, recall: 0.9178082191780822, precision: 0.9436619718309859, f_beta: 0.9305555555555556\n",
      "train: step: 569, loss: 0.19101881980895996, acc: 0.9375, recall: 0.9833333333333333, precision: 0.8939393939393939, f_beta: 0.9365079365079364\n",
      "train: step: 570, loss: 0.27009880542755127, acc: 0.8671875, recall: 0.9615384615384616, precision: 0.7692307692307693, f_beta: 0.8547008547008548\n",
      "train: step: 571, loss: 0.22651240229606628, acc: 0.90625, recall: 0.8450704225352113, precision: 0.9836065573770492, f_beta: 0.9090909090909091\n",
      "train: step: 572, loss: 0.1851803958415985, acc: 0.9296875, recall: 0.9107142857142857, precision: 0.9272727272727272, f_beta: 0.918918918918919\n",
      "train: step: 573, loss: 0.1666838526725769, acc: 0.9375, recall: 0.8833333333333333, precision: 0.9814814814814815, f_beta: 0.9298245614035088\n",
      "train: step: 574, loss: 0.18800869584083557, acc: 0.9375, recall: 0.9324324324324325, precision: 0.9583333333333334, f_beta: 0.9452054794520548\n",
      "train: step: 575, loss: 0.2279679775238037, acc: 0.9296875, recall: 0.9821428571428571, precision: 0.873015873015873, f_beta: 0.9243697478991596\n",
      "train: step: 576, loss: 0.18107494711875916, acc: 0.9453125, recall: 0.9459459459459459, precision: 0.958904109589041, f_beta: 0.9523809523809523\n",
      "train: step: 577, loss: 0.20304031670093536, acc: 0.9453125, recall: 0.9565217391304348, precision: 0.9428571428571428, f_beta: 0.9496402877697843\n",
      "train: step: 578, loss: 0.17440003156661987, acc: 0.9140625, recall: 0.9710144927536232, precision: 0.881578947368421, f_beta: 0.9241379310344827\n",
      "train: step: 579, loss: 0.21761783957481384, acc: 0.9140625, recall: 0.9295774647887324, precision: 0.9166666666666666, f_beta: 0.9230769230769231\n",
      "train: step: 580, loss: 0.22007527947425842, acc: 0.921875, recall: 1.0, precision: 0.8591549295774648, f_beta: 0.9242424242424242\n",
      "train: step: 581, loss: 0.17623066902160645, acc: 0.9453125, recall: 0.9180327868852459, precision: 0.9655172413793104, f_beta: 0.9411764705882353\n",
      "train: step: 582, loss: 0.21042069792747498, acc: 0.921875, recall: 0.8771929824561403, precision: 0.9433962264150944, f_beta: 0.9090909090909091\n",
      "train: step: 583, loss: 0.1772899031639099, acc: 0.921875, recall: 0.921875, precision: 0.921875, f_beta: 0.921875\n",
      "train: step: 584, loss: 0.2948512136936188, acc: 0.8984375, recall: 0.8873239436619719, precision: 0.9264705882352942, f_beta: 0.906474820143885\n",
      "train: step: 585, loss: 0.16131135821342468, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9696969696969697, f_beta: 0.955223880597015\n",
      "train: step: 586, loss: 0.183567076921463, acc: 0.9140625, recall: 0.8888888888888888, precision: 0.9056603773584906, f_beta: 0.897196261682243\n",
      "train: step: 587, loss: 0.2131020426750183, acc: 0.9296875, recall: 0.9180327868852459, precision: 0.9333333333333333, f_beta: 0.9256198347107439\n",
      "train: step: 588, loss: 0.16055381298065186, acc: 0.9453125, recall: 0.9636363636363636, precision: 0.9137931034482759, f_beta: 0.9380530973451328\n",
      "train: step: 589, loss: 0.22460299730300903, acc: 0.8828125, recall: 0.8545454545454545, precision: 0.8703703703703703, f_beta: 0.8623853211009175\n",
      "train: step: 590, loss: 0.17667144536972046, acc: 0.9296875, recall: 0.9384615384615385, precision: 0.9242424242424242, f_beta: 0.9312977099236641\n",
      "train: step: 591, loss: 0.19242945313453674, acc: 0.9453125, recall: 0.9855072463768116, precision: 0.918918918918919, f_beta: 0.951048951048951\n",
      "train: step: 592, loss: 0.1876133680343628, acc: 0.9296875, recall: 0.9473684210526315, precision: 0.9, f_beta: 0.9230769230769231\n",
      "train: step: 593, loss: 0.2211739718914032, acc: 0.90625, recall: 0.9315068493150684, precision: 0.9066666666666666, f_beta: 0.918918918918919\n",
      "train: step: 594, loss: 0.1441383957862854, acc: 0.96875, recall: 0.95, precision: 0.9827586206896551, f_beta: 0.9661016949152542\n",
      "train: step: 595, loss: 0.15701700747013092, acc: 0.9609375, recall: 0.967741935483871, precision: 0.9523809523809523, f_beta: 0.96\n",
      "train: step: 596, loss: 0.21647560596466064, acc: 0.9296875, recall: 0.890625, precision: 0.9661016949152542, f_beta: 0.9268292682926829\n",
      "train: step: 597, loss: 0.17586439847946167, acc: 0.9296875, recall: 0.9154929577464789, precision: 0.9558823529411765, f_beta: 0.9352517985611511\n",
      "train: step: 598, loss: 0.2106885015964508, acc: 0.90625, recall: 0.9180327868852459, precision: 0.8888888888888888, f_beta: 0.9032258064516128\n",
      "train: step: 599, loss: 0.17333173751831055, acc: 0.9375, recall: 0.95, precision: 0.9193548387096774, f_beta: 0.9344262295081968\n",
      "train: step: 600, loss: 0.22070620954036713, acc: 0.921875, recall: 0.9666666666666667, precision: 0.8787878787878788, f_beta: 0.9206349206349207\n",
      "\n",
      " evaluate model...\n",
      "2019-09-24T12:02:59.716999, step: 600, loss: 0.31822098256685794, acc: 0.8643830128205128, precision: 0.8569183071804084, recall: 0.8773521927091691, f_beta: 0.8664569171960145\n",
      "saved mdoel checkpoint tp C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\textCNN\\my_model-600\n",
      "\n",
      "train: step: 601, loss: 0.19078359007835388, acc: 0.9296875, recall: 0.9622641509433962, precision: 0.8793103448275862, f_beta: 0.9189189189189189\n",
      "train: step: 602, loss: 0.171655535697937, acc: 0.9296875, recall: 0.8771929824561403, precision: 0.9615384615384616, f_beta: 0.9174311926605504\n",
      "train: step: 603, loss: 0.21154187619686127, acc: 0.921875, recall: 0.8596491228070176, precision: 0.9607843137254902, f_beta: 0.9074074074074074\n",
      "train: step: 604, loss: 0.1795894205570221, acc: 0.921875, recall: 0.8823529411764706, precision: 0.967741935483871, f_beta: 0.923076923076923\n",
      "train: step: 605, loss: 0.20790798962116241, acc: 0.921875, recall: 0.9180327868852459, precision: 0.9180327868852459, f_beta: 0.9180327868852459\n",
      "train: step: 606, loss: 0.2189474254846573, acc: 0.8828125, recall: 0.8805970149253731, precision: 0.8939393939393939, f_beta: 0.887218045112782\n",
      "train: step: 607, loss: 0.17908504605293274, acc: 0.9140625, recall: 0.9558823529411765, precision: 0.8904109589041096, f_beta: 0.9219858156028369\n",
      "train: step: 608, loss: 0.1999712437391281, acc: 0.90625, recall: 0.8923076923076924, precision: 0.9206349206349206, f_beta: 0.90625\n",
      "train: step: 609, loss: 0.2227695733308792, acc: 0.9140625, recall: 0.9827586206896551, precision: 0.8507462686567164, f_beta: 0.9119999999999999\n",
      "train: step: 610, loss: 0.18159210681915283, acc: 0.953125, recall: 0.9545454545454546, precision: 0.9545454545454546, f_beta: 0.9545454545454546\n",
      "train: step: 611, loss: 0.20975257456302643, acc: 0.8984375, recall: 0.8985507246376812, precision: 0.9117647058823529, f_beta: 0.9051094890510949\n",
      "train: step: 612, loss: 0.20436953008174896, acc: 0.9375, recall: 0.9848484848484849, precision: 0.9027777777777778, f_beta: 0.9420289855072465\n",
      "train: step: 613, loss: 0.1563970297574997, acc: 0.9296875, recall: 0.9272727272727272, precision: 0.9107142857142857, f_beta: 0.918918918918919\n",
      "train: step: 614, loss: 0.18177571892738342, acc: 0.921875, recall: 0.9, precision: 0.9310344827586207, f_beta: 0.9152542372881356\n",
      "train: step: 615, loss: 0.24641621112823486, acc: 0.90625, recall: 0.8666666666666667, precision: 0.9701492537313433, f_beta: 0.915492957746479\n",
      "train: step: 616, loss: 0.1991317868232727, acc: 0.90625, recall: 0.875, precision: 0.9545454545454546, f_beta: 0.9130434782608695\n",
      "train: step: 617, loss: 0.1735084503889084, acc: 0.9375, recall: 0.9482758620689655, precision: 0.9166666666666666, f_beta: 0.9322033898305084\n",
      "train: step: 618, loss: 0.24312755465507507, acc: 0.921875, recall: 0.9333333333333333, precision: 0.9333333333333333, f_beta: 0.9333333333333333\n",
      "train: step: 619, loss: 0.20017379522323608, acc: 0.9296875, recall: 0.9464285714285714, precision: 0.8983050847457628, f_beta: 0.9217391304347826\n",
      "train: step: 620, loss: 0.14148283004760742, acc: 0.953125, recall: 0.9538461538461539, precision: 0.9538461538461539, f_beta: 0.9538461538461539\n",
      "train: step: 621, loss: 0.18371543288230896, acc: 0.9296875, recall: 0.953125, precision: 0.9104477611940298, f_beta: 0.931297709923664\n",
      "train: step: 622, loss: 0.22615543007850647, acc: 0.9140625, recall: 0.9436619718309859, precision: 0.9054054054054054, f_beta: 0.9241379310344827\n",
      "train: step: 623, loss: 0.1899224817752838, acc: 0.90625, recall: 0.9402985074626866, precision: 0.8873239436619719, f_beta: 0.9130434782608696\n",
      "train: step: 624, loss: 0.19236618280410767, acc: 0.921875, recall: 0.9066666666666666, precision: 0.9577464788732394, f_beta: 0.9315068493150686\n",
      "start to train models...\n",
      "train: step: 625, loss: 0.17404508590698242, acc: 0.9296875, recall: 0.9354838709677419, precision: 0.9206349206349206, f_beta: 0.9279999999999999\n",
      "train: step: 626, loss: 0.14155814051628113, acc: 0.953125, recall: 0.9130434782608695, precision: 1.0, f_beta: 0.9545454545454545\n",
      "train: step: 627, loss: 0.1745276153087616, acc: 0.9375, recall: 0.9117647058823529, precision: 0.96875, f_beta: 0.9393939393939394\n",
      "train: step: 628, loss: 0.13043615221977234, acc: 0.953125, recall: 0.9803921568627451, precision: 0.9090909090909091, f_beta: 0.9433962264150944\n",
      "train: step: 629, loss: 0.1363256275653839, acc: 0.9609375, recall: 0.9714285714285714, precision: 0.9577464788732394, f_beta: 0.9645390070921985\n",
      "train: step: 630, loss: 0.1733798235654831, acc: 0.9453125, recall: 0.9342105263157895, precision: 0.9726027397260274, f_beta: 0.9530201342281879\n",
      "train: step: 631, loss: 0.11565034836530685, acc: 0.9765625, recall: 0.9821428571428571, precision: 0.9649122807017544, f_beta: 0.9734513274336283\n",
      "train: step: 632, loss: 0.13976430892944336, acc: 0.96875, recall: 0.9838709677419355, precision: 0.953125, f_beta: 0.9682539682539683\n",
      "train: step: 633, loss: 0.1595163643360138, acc: 0.9375, recall: 0.9682539682539683, precision: 0.9104477611940298, f_beta: 0.9384615384615386\n",
      "train: step: 634, loss: 0.11455153673887253, acc: 0.9765625, recall: 0.9516129032258065, precision: 1.0, f_beta: 0.9752066115702479\n",
      "train: step: 635, loss: 0.15923374891281128, acc: 0.953125, recall: 0.9428571428571428, precision: 0.9705882352941176, f_beta: 0.9565217391304348\n",
      "train: step: 636, loss: 0.1261122226715088, acc: 0.953125, recall: 0.9245283018867925, precision: 0.9607843137254902, f_beta: 0.9423076923076923\n",
      "train: step: 637, loss: 0.11735114455223083, acc: 0.9765625, recall: 0.9615384615384616, precision: 1.0, f_beta: 0.9803921568627451\n",
      "train: step: 638, loss: 0.10543350875377655, acc: 0.96875, recall: 0.9701492537313433, precision: 0.9701492537313433, f_beta: 0.9701492537313433\n",
      "train: step: 639, loss: 0.14943262934684753, acc: 0.96875, recall: 0.9571428571428572, precision: 0.9852941176470589, f_beta: 0.9710144927536232\n",
      "train: step: 640, loss: 0.16615620255470276, acc: 0.953125, recall: 0.987012987012987, precision: 0.9382716049382716, f_beta: 0.9620253164556961\n",
      "train: step: 641, loss: 0.17152325809001923, acc: 0.921875, recall: 0.9836065573770492, precision: 0.8695652173913043, f_beta: 0.923076923076923\n",
      "train: step: 642, loss: 0.19680601358413696, acc: 0.9296875, recall: 0.9672131147540983, precision: 0.8939393939393939, f_beta: 0.9291338582677166\n",
      "train: step: 643, loss: 0.178798109292984, acc: 0.9296875, recall: 0.9310344827586207, precision: 0.9152542372881356, f_beta: 0.923076923076923\n",
      "train: step: 644, loss: 0.14615266025066376, acc: 0.9453125, recall: 0.9672131147540983, precision: 0.921875, f_beta: 0.944\n",
      "train: step: 645, loss: 0.13161584734916687, acc: 0.953125, recall: 0.9090909090909091, precision: 0.9803921568627451, f_beta: 0.9433962264150944\n",
      "train: step: 646, loss: 0.15251128375530243, acc: 0.953125, recall: 0.927536231884058, precision: 0.9846153846153847, f_beta: 0.9552238805970149\n",
      "train: step: 647, loss: 0.16432487964630127, acc: 0.9375, recall: 0.9178082191780822, precision: 0.9710144927536232, f_beta: 0.943661971830986\n",
      "train: step: 648, loss: 0.20032596588134766, acc: 0.9296875, recall: 0.9016393442622951, precision: 0.9482758620689655, f_beta: 0.9243697478991596\n",
      "train: step: 649, loss: 0.19467146694660187, acc: 0.921875, recall: 0.8846153846153846, precision: 0.92, f_beta: 0.9019607843137256\n",
      "train: step: 650, loss: 0.18156230449676514, acc: 0.9296875, recall: 0.921875, precision: 0.9365079365079365, f_beta: 0.9291338582677166\n",
      "train: step: 651, loss: 0.15226826071739197, acc: 0.9453125, recall: 0.9516129032258065, precision: 0.9365079365079365, f_beta: 0.944\n",
      "train: step: 652, loss: 0.13243058323860168, acc: 0.953125, recall: 0.9482758620689655, precision: 0.9482758620689655, f_beta: 0.9482758620689655\n",
      "train: step: 653, loss: 0.15943661332130432, acc: 0.953125, recall: 0.9821428571428571, precision: 0.9166666666666666, f_beta: 0.9482758620689654\n",
      "train: step: 654, loss: 0.1726781278848648, acc: 0.921875, recall: 0.9253731343283582, precision: 0.9253731343283582, f_beta: 0.9253731343283582\n",
      "train: step: 655, loss: 0.16262511909008026, acc: 0.9296875, recall: 0.9365079365079365, precision: 0.921875, f_beta: 0.9291338582677166\n",
      "train: step: 656, loss: 0.16532555222511292, acc: 0.9453125, recall: 0.9464285714285714, precision: 0.9298245614035088, f_beta: 0.9380530973451328\n",
      "train: step: 657, loss: 0.19849278032779694, acc: 0.9296875, recall: 0.9180327868852459, precision: 0.9333333333333333, f_beta: 0.9256198347107439\n",
      "train: step: 658, loss: 0.1469327062368393, acc: 0.9609375, recall: 0.9365079365079365, precision: 0.9833333333333333, f_beta: 0.9593495934959351\n",
      "train: step: 659, loss: 0.1314443200826645, acc: 0.953125, recall: 0.9333333333333333, precision: 0.9655172413793104, f_beta: 0.9491525423728815\n",
      "train: step: 660, loss: 0.16118451952934265, acc: 0.953125, recall: 0.9402985074626866, precision: 0.9692307692307692, f_beta: 0.9545454545454547\n",
      "train: step: 661, loss: 0.1285606175661087, acc: 0.953125, recall: 0.953125, precision: 0.953125, f_beta: 0.953125\n",
      "train: step: 662, loss: 0.15627239644527435, acc: 0.953125, recall: 0.9365079365079365, precision: 0.9672131147540983, f_beta: 0.9516129032258064\n",
      "train: step: 663, loss: 0.13392305374145508, acc: 0.96875, recall: 1.0, precision: 0.9285714285714286, f_beta: 0.962962962962963\n",
      "train: step: 664, loss: 0.12831975519657135, acc: 0.9765625, recall: 0.9692307692307692, precision: 0.984375, f_beta: 0.9767441860465116\n",
      "train: step: 665, loss: 0.17748378217220306, acc: 0.9296875, recall: 0.9242424242424242, precision: 0.9384615384615385, f_beta: 0.9312977099236641\n",
      "train: step: 666, loss: 0.15108346939086914, acc: 0.9609375, recall: 0.972972972972973, precision: 0.96, f_beta: 0.9664429530201343\n",
      "train: step: 667, loss: 0.12195117026567459, acc: 0.953125, recall: 0.9322033898305084, precision: 0.9649122807017544, f_beta: 0.9482758620689654\n",
      "train: step: 668, loss: 0.15234512090682983, acc: 0.9453125, recall: 0.96875, precision: 0.9253731343283582, f_beta: 0.9465648854961832\n",
      "train: step: 669, loss: 0.17724934220314026, acc: 0.9453125, recall: 0.967741935483871, precision: 0.9230769230769231, f_beta: 0.9448818897637796\n",
      "train: step: 670, loss: 0.1703837811946869, acc: 0.921875, recall: 0.8923076923076924, precision: 0.9508196721311475, f_beta: 0.9206349206349206\n",
      "train: step: 671, loss: 0.17946740984916687, acc: 0.9453125, recall: 0.9384615384615385, precision: 0.953125, f_beta: 0.9457364341085271\n",
      "train: step: 672, loss: 0.10627171397209167, acc: 0.96875, recall: 0.953125, precision: 0.9838709677419355, f_beta: 0.9682539682539683\n",
      "train: step: 673, loss: 0.10381923615932465, acc: 0.96875, recall: 0.9814814814814815, precision: 0.9464285714285714, f_beta: 0.9636363636363636\n",
      "train: step: 674, loss: 0.1372440755367279, acc: 0.96875, recall: 0.9558823529411765, precision: 0.9848484848484849, f_beta: 0.9701492537313432\n",
      "train: step: 675, loss: 0.15987758338451385, acc: 0.96875, recall: 1.0, precision: 0.9375, f_beta: 0.967741935483871\n",
      "train: step: 676, loss: 0.14728549122810364, acc: 0.9375, recall: 0.9552238805970149, precision: 0.927536231884058, f_beta: 0.9411764705882353\n",
      "train: step: 677, loss: 0.12118489295244217, acc: 0.96875, recall: 0.9393939393939394, precision: 1.0, f_beta: 0.96875\n",
      "train: step: 678, loss: 0.13521137833595276, acc: 0.953125, recall: 0.9649122807017544, precision: 0.9322033898305084, f_beta: 0.9482758620689654\n",
      "train: step: 679, loss: 0.25514858961105347, acc: 0.8828125, recall: 0.8333333333333334, precision: 0.9090909090909091, f_beta: 0.8695652173913043\n",
      "train: step: 680, loss: 0.11583757400512695, acc: 0.9765625, recall: 0.9710144927536232, precision: 0.9852941176470589, f_beta: 0.9781021897810219\n",
      "train: step: 681, loss: 0.11657275259494781, acc: 0.96875, recall: 0.9692307692307692, precision: 0.9692307692307692, f_beta: 0.9692307692307692\n",
      "train: step: 682, loss: 0.16690614819526672, acc: 0.953125, recall: 0.9821428571428571, precision: 0.9166666666666666, f_beta: 0.9482758620689654\n",
      "train: step: 683, loss: 0.12779569625854492, acc: 0.96875, recall: 1.0, precision: 0.9298245614035088, f_beta: 0.9636363636363636\n",
      "train: step: 684, loss: 0.13105586171150208, acc: 0.9453125, recall: 0.9193548387096774, precision: 0.9661016949152542, f_beta: 0.9421487603305785\n",
      "train: step: 685, loss: 0.12871426343917847, acc: 0.953125, recall: 0.9859154929577465, precision: 0.9333333333333333, f_beta: 0.9589041095890412\n",
      "train: step: 686, loss: 0.15131649374961853, acc: 0.953125, recall: 0.9558823529411765, precision: 0.9558823529411765, f_beta: 0.9558823529411765\n",
      "train: step: 687, loss: 0.10170592367649078, acc: 0.984375, recall: 0.9864864864864865, precision: 0.9864864864864865, f_beta: 0.9864864864864865\n",
      "train: step: 688, loss: 0.1526264250278473, acc: 0.9609375, recall: 0.9538461538461539, precision: 0.96875, f_beta: 0.9612403100775193\n",
      "train: step: 689, loss: 0.12767815589904785, acc: 0.9453125, recall: 0.9393939393939394, precision: 0.9538461538461539, f_beta: 0.9465648854961831\n",
      "train: step: 690, loss: 0.11657766997814178, acc: 0.96875, recall: 0.9710144927536232, precision: 0.9710144927536232, f_beta: 0.9710144927536232\n",
      "train: step: 691, loss: 0.12306618690490723, acc: 0.953125, recall: 0.9523809523809523, precision: 0.9523809523809523, f_beta: 0.9523809523809523\n",
      "train: step: 692, loss: 0.13123542070388794, acc: 0.953125, recall: 0.9692307692307692, precision: 0.9402985074626866, f_beta: 0.9545454545454547\n",
      "train: step: 693, loss: 0.1514265239238739, acc: 0.9453125, recall: 0.9516129032258065, precision: 0.9365079365079365, f_beta: 0.944\n",
      "train: step: 694, loss: 0.10290844738483429, acc: 0.96875, recall: 1.0, precision: 0.9344262295081968, f_beta: 0.9661016949152543\n",
      "train: step: 695, loss: 0.14333723485469818, acc: 0.9609375, recall: 0.9846153846153847, precision: 0.9411764705882353, f_beta: 0.962406015037594\n",
      "train: step: 696, loss: 0.1500444859266281, acc: 0.953125, recall: 0.9649122807017544, precision: 0.9322033898305084, f_beta: 0.9482758620689654\n",
      "train: step: 697, loss: 0.17403441667556763, acc: 0.9375, recall: 0.896551724137931, precision: 0.9629629629629629, f_beta: 0.9285714285714286\n",
      "train: step: 698, loss: 0.12694469094276428, acc: 0.9765625, recall: 0.9565217391304348, precision: 1.0, f_beta: 0.9777777777777777\n",
      "train: step: 699, loss: 0.1448308825492859, acc: 0.9296875, recall: 0.890625, precision: 0.9661016949152542, f_beta: 0.9268292682926829\n",
      "train: step: 700, loss: 0.13289770483970642, acc: 0.96875, recall: 0.9666666666666667, precision: 0.9666666666666667, f_beta: 0.9666666666666667\n",
      "\n",
      " evaluate model...\n",
      "2019-09-24T12:04:59.534377, step: 700, loss: 0.32214770599817616, acc: 0.8649839743589743, precision: 0.8633966891226524, recall: 0.8702397219265265, f_beta: 0.8659541040728831\n",
      "saved mdoel checkpoint tp C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\textCNN\\my_model-700\n",
      "\n",
      "train: step: 701, loss: 0.18876269459724426, acc: 0.9140625, recall: 0.873015873015873, precision: 0.9482758620689655, f_beta: 0.9090909090909091\n",
      "train: step: 702, loss: 0.16385230422019958, acc: 0.9296875, recall: 0.9838709677419355, precision: 0.8840579710144928, f_beta: 0.9312977099236642\n",
      "train: step: 703, loss: 0.1595955491065979, acc: 0.9375, recall: 0.984375, precision: 0.9, f_beta: 0.9402985074626866\n",
      "train: step: 704, loss: 0.1691439300775528, acc: 0.9375, recall: 0.9710144927536232, precision: 0.9178082191780822, f_beta: 0.943661971830986\n",
      "train: step: 705, loss: 0.16902995109558105, acc: 0.96875, recall: 0.9661016949152542, precision: 0.9661016949152542, f_beta: 0.9661016949152542\n",
      "train: step: 706, loss: 0.15387126803398132, acc: 0.9375, recall: 0.9253731343283582, precision: 0.9538461538461539, f_beta: 0.9393939393939394\n",
      "train: step: 707, loss: 0.22218015789985657, acc: 0.9140625, recall: 0.9130434782608695, precision: 0.9264705882352942, f_beta: 0.9197080291970804\n",
      "train: step: 708, loss: 0.14809370040893555, acc: 0.9296875, recall: 0.9178082191780822, precision: 0.9571428571428572, f_beta: 0.9370629370629371\n",
      "train: step: 709, loss: 0.11014081537723541, acc: 0.96875, recall: 0.9795918367346939, precision: 0.9411764705882353, f_beta: 0.96\n",
      "train: step: 710, loss: 0.15582214295864105, acc: 0.953125, recall: 0.9836065573770492, precision: 0.9230769230769231, f_beta: 0.9523809523809524\n",
      "train: step: 711, loss: 0.13368305563926697, acc: 0.9453125, recall: 0.9436619718309859, precision: 0.9571428571428572, f_beta: 0.9503546099290779\n",
      "train: step: 712, loss: 0.15932929515838623, acc: 0.9609375, recall: 0.9358974358974359, precision: 1.0, f_beta: 0.9668874172185431\n",
      "train: step: 713, loss: 0.14603647589683533, acc: 0.96875, recall: 0.9705882352941176, precision: 0.9705882352941176, f_beta: 0.9705882352941176\n",
      "train: step: 714, loss: 0.1776318997144699, acc: 0.9453125, recall: 0.9571428571428572, precision: 0.9436619718309859, f_beta: 0.9503546099290779\n",
      "train: step: 715, loss: 0.13611619174480438, acc: 0.953125, recall: 1.0, precision: 0.9130434782608695, f_beta: 0.9545454545454545\n",
      "train: step: 716, loss: 0.13853013515472412, acc: 0.953125, recall: 1.0, precision: 0.9230769230769231, f_beta: 0.9600000000000001\n",
      "train: step: 717, loss: 0.14234575629234314, acc: 0.9453125, recall: 0.9285714285714286, precision: 0.9454545454545454, f_beta: 0.9369369369369368\n",
      "train: step: 718, loss: 0.10941439867019653, acc: 0.9453125, recall: 0.9649122807017544, precision: 0.9166666666666666, f_beta: 0.9401709401709402\n",
      "train: step: 719, loss: 0.13187864422798157, acc: 0.9765625, recall: 0.9836065573770492, precision: 0.967741935483871, f_beta: 0.975609756097561\n",
      "train: step: 720, loss: 0.1779901087284088, acc: 0.9375, recall: 0.9491525423728814, precision: 0.9180327868852459, f_beta: 0.9333333333333333\n",
      "train: step: 721, loss: 0.12118099629878998, acc: 0.9609375, recall: 0.9420289855072463, precision: 0.9848484848484849, f_beta: 0.962962962962963\n",
      "train: step: 722, loss: 0.13371366262435913, acc: 0.9609375, recall: 0.9558823529411765, precision: 0.9701492537313433, f_beta: 0.962962962962963\n",
      "train: step: 723, loss: 0.17224085330963135, acc: 0.9453125, recall: 0.9482758620689655, precision: 0.9322033898305084, f_beta: 0.94017094017094\n",
      "train: step: 724, loss: 0.16337072849273682, acc: 0.953125, recall: 0.9577464788732394, precision: 0.9577464788732394, f_beta: 0.9577464788732394\n",
      "train: step: 725, loss: 0.14326326549053192, acc: 0.9609375, recall: 0.9848484848484849, precision: 0.9420289855072463, f_beta: 0.962962962962963\n",
      "train: step: 726, loss: 0.14115455746650696, acc: 0.9609375, recall: 0.9692307692307692, precision: 0.9545454545454546, f_beta: 0.9618320610687022\n",
      "train: step: 727, loss: 0.14793631434440613, acc: 0.953125, recall: 0.9508196721311475, precision: 0.9508196721311475, f_beta: 0.9508196721311475\n",
      "train: step: 728, loss: 0.1614721566438675, acc: 0.9375, recall: 0.9830508474576272, precision: 0.8923076923076924, f_beta: 0.9354838709677421\n",
      "train: step: 729, loss: 0.12153510749340057, acc: 0.9453125, recall: 0.9814814814814815, precision: 0.8983050847457628, f_beta: 0.9380530973451328\n",
      "train: step: 730, loss: 0.11064848303794861, acc: 0.953125, recall: 0.9701492537313433, precision: 0.9420289855072463, f_beta: 0.9558823529411764\n",
      "train: step: 731, loss: 0.26657798886299133, acc: 0.90625, recall: 0.8450704225352113, precision: 0.9836065573770492, f_beta: 0.9090909090909091\n",
      "train: step: 732, loss: 0.14216840267181396, acc: 0.953125, recall: 0.9152542372881356, precision: 0.9818181818181818, f_beta: 0.9473684210526316\n",
      "train: step: 733, loss: 0.10810376703739166, acc: 0.984375, recall: 1.0, precision: 0.9710144927536232, f_beta: 0.9852941176470589\n",
      "train: step: 734, loss: 0.14008650183677673, acc: 0.953125, recall: 0.9508196721311475, precision: 0.9508196721311475, f_beta: 0.9508196721311475\n",
      "train: step: 735, loss: 0.15252390503883362, acc: 0.921875, recall: 0.9516129032258065, precision: 0.8939393939393939, f_beta: 0.921875\n",
      "train: step: 736, loss: 0.15791311860084534, acc: 0.921875, recall: 0.9666666666666667, precision: 0.8787878787878788, f_beta: 0.9206349206349207\n",
      "train: step: 737, loss: 0.10236898064613342, acc: 0.96875, recall: 0.9636363636363636, precision: 0.9636363636363636, f_beta: 0.9636363636363636\n",
      "train: step: 738, loss: 0.1239800676703453, acc: 0.9609375, recall: 0.9726027397260274, precision: 0.9594594594594594, f_beta: 0.9659863945578231\n",
      "train: step: 739, loss: 0.13446977734565735, acc: 0.953125, recall: 0.9508196721311475, precision: 0.9508196721311475, f_beta: 0.9508196721311475\n",
      "train: step: 740, loss: 0.13610363006591797, acc: 0.9609375, recall: 0.9285714285714286, precision: 1.0, f_beta: 0.962962962962963\n",
      "train: step: 741, loss: 0.09554530680179596, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 742, loss: 0.18194785714149475, acc: 0.9296875, recall: 0.9393939393939394, precision: 0.9253731343283582, f_beta: 0.9323308270676692\n",
      "train: step: 743, loss: 0.10904677957296371, acc: 0.96875, recall: 0.9444444444444444, precision: 1.0, f_beta: 0.9714285714285714\n",
      "train: step: 744, loss: 0.15947222709655762, acc: 0.9453125, recall: 0.9354838709677419, precision: 0.9508196721311475, f_beta: 0.943089430894309\n",
      "train: step: 745, loss: 0.12987428903579712, acc: 0.953125, recall: 0.96875, precision: 0.9393939393939394, f_beta: 0.9538461538461539\n",
      "train: step: 746, loss: 0.14974157512187958, acc: 0.9296875, recall: 0.9315068493150684, precision: 0.9444444444444444, f_beta: 0.9379310344827586\n",
      "train: step: 747, loss: 0.2103775143623352, acc: 0.9140625, recall: 0.9473684210526315, precision: 0.9113924050632911, f_beta: 0.9290322580645162\n",
      "train: step: 748, loss: 0.20715859532356262, acc: 0.9453125, recall: 0.9523809523809523, precision: 0.9375, f_beta: 0.9448818897637795\n",
      "train: step: 749, loss: 0.13497930765151978, acc: 0.96875, recall: 0.9814814814814815, precision: 0.9464285714285714, f_beta: 0.9636363636363636\n",
      "train: step: 750, loss: 0.12558439373970032, acc: 0.953125, recall: 0.9846153846153847, precision: 0.927536231884058, f_beta: 0.9552238805970149\n",
      "train: step: 751, loss: 0.1648305058479309, acc: 0.921875, recall: 0.95, precision: 0.890625, f_beta: 0.9193548387096774\n",
      "train: step: 752, loss: 0.16891039907932281, acc: 0.9453125, recall: 0.9117647058823529, precision: 0.9841269841269841, f_beta: 0.9465648854961831\n",
      "train: step: 753, loss: 0.11565550416707993, acc: 0.984375, recall: 0.9710144927536232, precision: 1.0, f_beta: 0.9852941176470589\n",
      "train: step: 754, loss: 0.11288627237081528, acc: 0.953125, recall: 0.9516129032258065, precision: 0.9516129032258065, f_beta: 0.9516129032258065\n",
      "train: step: 755, loss: 0.0997663140296936, acc: 0.984375, recall: 1.0, precision: 0.9666666666666667, f_beta: 0.983050847457627\n",
      "train: step: 756, loss: 0.16352218389511108, acc: 0.9375, recall: 0.9821428571428571, precision: 0.8870967741935484, f_beta: 0.9322033898305085\n",
      "train: step: 757, loss: 0.1679915189743042, acc: 0.9609375, recall: 0.9577464788732394, precision: 0.9714285714285714, f_beta: 0.9645390070921985\n",
      "train: step: 758, loss: 0.15845945477485657, acc: 0.9453125, recall: 0.967741935483871, precision: 0.9230769230769231, f_beta: 0.9448818897637796\n",
      "train: step: 759, loss: 0.13892608880996704, acc: 0.953125, recall: 0.9666666666666667, precision: 0.9354838709677419, f_beta: 0.9508196721311476\n",
      "train: step: 760, loss: 0.15564817190170288, acc: 0.953125, recall: 0.9491525423728814, precision: 0.9491525423728814, f_beta: 0.9491525423728814\n",
      "train: step: 761, loss: 0.1102023497223854, acc: 0.96875, recall: 0.967741935483871, precision: 0.967741935483871, f_beta: 0.967741935483871\n",
      "train: step: 762, loss: 0.14421163499355316, acc: 0.9453125, recall: 0.9324324324324325, precision: 0.971830985915493, f_beta: 0.9517241379310345\n",
      "train: step: 763, loss: 0.09256286174058914, acc: 0.984375, recall: 0.9821428571428571, precision: 0.9821428571428571, f_beta: 0.9821428571428571\n",
      "train: step: 764, loss: 0.13514026999473572, acc: 0.9765625, recall: 0.95, precision: 1.0, f_beta: 0.9743589743589743\n",
      "train: step: 765, loss: 0.1140371710062027, acc: 0.9765625, recall: 0.9672131147540983, precision: 0.9833333333333333, f_beta: 0.9752066115702478\n",
      "train: step: 766, loss: 0.1681915670633316, acc: 0.96875, recall: 0.9841269841269841, precision: 0.9538461538461539, f_beta: 0.96875\n",
      "train: step: 767, loss: 0.1435738056898117, acc: 0.9453125, recall: 0.9710144927536232, precision: 0.9305555555555556, f_beta: 0.9503546099290779\n",
      "train: step: 768, loss: 0.16323232650756836, acc: 0.9453125, recall: 1.0, precision: 0.9054054054054054, f_beta: 0.950354609929078\n",
      "train: step: 769, loss: 0.09427701681852341, acc: 0.9921875, recall: 0.9838709677419355, precision: 1.0, f_beta: 0.991869918699187\n",
      "train: step: 770, loss: 0.14719834923744202, acc: 0.9609375, recall: 0.9516129032258065, precision: 0.9672131147540983, f_beta: 0.959349593495935\n",
      "train: step: 771, loss: 0.18735793232917786, acc: 0.921875, recall: 0.9078947368421053, precision: 0.9583333333333334, f_beta: 0.9324324324324325\n",
      "train: step: 772, loss: 0.15680217742919922, acc: 0.953125, recall: 0.9464285714285714, precision: 0.9464285714285714, f_beta: 0.9464285714285714\n",
      "train: step: 773, loss: 0.1391288936138153, acc: 0.953125, recall: 0.9473684210526315, precision: 0.9473684210526315, f_beta: 0.9473684210526315\n",
      "train: step: 774, loss: 0.16826649010181427, acc: 0.9609375, recall: 0.984375, precision: 0.9402985074626866, f_beta: 0.9618320610687023\n",
      "train: step: 775, loss: 0.1125020831823349, acc: 0.96875, recall: 0.9692307692307692, precision: 0.9692307692307692, f_beta: 0.9692307692307692\n",
      "train: step: 776, loss: 0.14276117086410522, acc: 0.9609375, recall: 0.9701492537313433, precision: 0.9558823529411765, f_beta: 0.962962962962963\n",
      "train: step: 777, loss: 0.16626980900764465, acc: 0.9453125, recall: 0.9428571428571428, precision: 0.9565217391304348, f_beta: 0.9496402877697843\n",
      "train: step: 778, loss: 0.0979384034872055, acc: 0.9765625, recall: 0.9852941176470589, precision: 0.9710144927536232, f_beta: 0.9781021897810219\n",
      "train: step: 779, loss: 0.13053393363952637, acc: 0.96875, recall: 0.9538461538461539, precision: 0.9841269841269841, f_beta: 0.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0924 12:06:11.657923  9780 deprecation.py:506] From <ipython-input-50-354640a71ea9>:120: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 780, loss: 0.14485587179660797, acc: 0.9609375, recall: 0.967741935483871, precision: 0.9523809523809523, f_beta: 0.96\n"
     ]
    }
   ],
   "source": [
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "labelList = data.labelList\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(config, wordEmbedding)\n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"writing to {} \\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        savedModelPath = \"\\\\\".join(directory_path.split(\"\\\\\")[:-1]) + \"\\\\model\\\\textCNN\\\\savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "            os.rmdir(savedModelPath)\n",
    "        \n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        def trainStep(batchX, batchY):\n",
    "            feed_dict = {\n",
    "                cnn.inputX: batchX, \n",
    "                cnn.inputY: batchY, \n",
    "                cnn.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run([trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions], feed_dict)\n",
    "            \n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "            return loss, acc, prec, recall, f_beta\n",
    "        \n",
    "        \n",
    "        def devStep(batchX, batchY):\n",
    "            feed_dict = {\n",
    "                cnn.inputX: batchX, \n",
    "                cnn.inputY: batchY, \n",
    "                cnn.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run([summaryOp, globalStep, cnn.loss, cnn.predictions], feed_dict)\n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, precision, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, precision, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            return loss, acc, precision, recall, f_beta\n",
    "        \n",
    "        \n",
    "        for index in range(config.training.epochs):\n",
    "            print(\"start to train models...\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, precision, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "                currentStep = tf.train.global_step(sess, globalStep)\n",
    "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                    currentStep, loss, acc, recall, precision, f_beta))\n",
    "                \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\n evaluate model...\")\n",
    "                    losses, accs, f_betas, precisions, recalls = [], [], [], [], []\n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        f_betas.append(f_beta)\n",
    "                    \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, precision: {}, recall: {}, f_beta: {}\".format(\n",
    "                        time_str, currentStep, mean(losses), mean(accs), mean(precisions), mean(recalls), mean(f_betas)))\n",
    "            \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    path = saver.save(sess, \"\\\\\".join(savedModelPath.split(\"\\\\\")[:-1]) + \"\\\\my_model\", global_step=currentStep)\n",
    "                    print(\"saved mdoel checkpoint tp {}\\n\".format(path))\n",
    "        \n",
    "        \n",
    "        inputs = {\n",
    "            \"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX), \n",
    "            \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)\n",
    "        }\n",
    "        outputs = {\n",
    "            \"precisions\": tf.saved_model.utils.build_tensor_info(cnn.predictions)\n",
    "        }\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, \n",
    "                                                                                      outputs=outputs, \n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        \n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING], \n",
    "                                            signature_def_map={\"predict\": prediction_signature}, \n",
    "                                            legacy_init_op=legacy_init_op)\n",
    "        \n",
    "        builder.save()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.training.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
