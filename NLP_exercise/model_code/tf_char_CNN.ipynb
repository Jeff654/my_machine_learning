{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import csv\n",
    "from math import sqrt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:-1]) + \"\\\\data\"\n",
    "os.path.exists(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\123\\\\Documents\\\\python_experence\\\\nlp_model\\\\data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 配置 model 的一些参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epochs = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "\n",
    "\n",
    "class ModelConfig(object):\n",
    "    \n",
    "    # 卷积核数量、卷积核尺寸、池化尺寸\n",
    "    convLayers = [[256, 7, 4], \n",
    "                  [256, 7, 4], \n",
    "                  [256, 3, 4]]\n",
    "    \n",
    "    # 全连接层参数\n",
    "    fcLayers = [512]\n",
    "    dropoutKeepProb = 0.5\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    # BN 中计算滑动平均参数\n",
    "    decay = 0.999\n",
    "\n",
    "    \n",
    "class Config(object):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    sequenceLength = 1014\n",
    "    batchSize = 128\n",
    "    \n",
    "    # 训练集-测试集比例\n",
    "    rate = 0.8\n",
    "    dataSource = directory_path + \"\\\\preProcess\\\\labeledCharTrain.csv\"\n",
    "    \n",
    "    assert os.path.exists(dataSource)\n",
    "    training = TrainingConfig()\n",
    "    model = ModelConfig()\n",
    "    \n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 预处理数据类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._sequenceLength = config.sequenceLength\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self._alphabet = config.alphabet\n",
    "        self.charEmbedding = None\n",
    "        self._charToIndex = dict()\n",
    "        self._indexToChar = dict()\n",
    "        \n",
    "    \n",
    "    def _readData(self, filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [[char for char in line if char != \" \"] for line in review]\n",
    "        \n",
    "        return reviews, labels\n",
    "    \n",
    "    \n",
    "    def _reviewProcess(self, review, sequenceLength, charToIndex):\n",
    "        \"\"\"\n",
    "            将数据集内的每条评论文本使用 index 来表示\n",
    "            wordToIndex 中 \"pad\" 对应为 index 中的 0\n",
    "        \"\"\"\n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前序列是否小于所定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "        \n",
    "        for index in range(sequenceLen):\n",
    "            if review[index] in charToIndex:\n",
    "                reviewVec[index] = charToIndex[review[index]]\n",
    "            else:\n",
    "                reviewVec[index] = charToIndex[\"UNK\"]\n",
    "        \n",
    "        return reviewVec\n",
    "    \n",
    "    \n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "            生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 将 word 转成 index 表示\n",
    "        for index in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[index], self._sequenceLength, self._charToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            labels.append([y[index]])\n",
    "        \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.asarray(labels[:trainIndex], dtype=\"float32\")\n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.asarray(labels[trainIndex:], dtype=\"float32\")\n",
    "        \n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "    \n",
    "    \n",
    "    def _getCharEmbedding(self, chars):\n",
    "        \"\"\"\n",
    "            按照 one-hot 的形式将字符映射成为向量\n",
    "        \"\"\"\n",
    "        alphabet = [\"UNK\"] + [char for char in self._alphabet]\n",
    "        vocab = [\"pad\"] + alphabet\n",
    "        charEmbedding = []\n",
    "        charEmbedding.append(np.zeros(len(alphabet), dtype=\"float32\"))\n",
    "        \n",
    "        for index, alpha in enumerate(alphabet):\n",
    "            one_hot = np.zeros(len(alphabet), dtype=\"float32\")\n",
    "            \n",
    "            # 生成每个字符对应的 1 位置向量\n",
    "            one_hot[index] = 1\n",
    "            charEmbedding.append(one_hot)\n",
    "        \n",
    "        return vocab, np.array(charEmbedding)\n",
    "    \n",
    "    \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "            生成字符向量以及 字符-索引 映射字典\n",
    "        \"\"\"\n",
    "        chars = [char for char in self._alphabet]\n",
    "        vocab, charEmbedding = self._getCharEmbedding(chars)\n",
    "        self.charEmbedding = charEmbedding\n",
    "        self._charToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToChar = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        \n",
    "        # 将 word-index 映射表保存为 json 数据(持久化)\n",
    "        with open(directory_path + \"\\\\charJson\\\\charToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._charToIndex, f)\n",
    "        \n",
    "        with open(directory_path + \"\\\\charJson\\\\indexToChar.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToChar, f)\n",
    "    \n",
    "    \n",
    "    def dataGen(self):\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        self._genVocabulary(reviews)\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "\n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training data shape:  (20000, 1014)\n",
      "the training label shape:  (20000, 1)\n",
      "the evaluate data shape:  (5000, 1014)\n",
      "the evaluate label shape:  (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"the training data shape: \", data.trainReviews.shape)\n",
    "print(\"the training label shape: \", data.trainLabels.shape)\n",
    "print(\"the evaluate data shape: \", data.evalReviews.shape)\n",
    "print(\"the evaluate label shape: \", data.evalLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 70)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.charEmbedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 生成 batch 数据集\n",
    "def nextBatch(x, y, batchSize):\n",
    "    \"\"\"\n",
    "        利用生成器来生成 batch 数据集\n",
    "    \"\"\"\n",
    "    perm = np.arange(len(x))\n",
    "    np.random.shuffle(perm)\n",
    "    x = x[perm]\n",
    "    y = y[perm]\n",
    "    \n",
    "    numBatches = len(x) // batchSize\n",
    "    for index in range(numBatches):\n",
    "        start = index * batchSize\n",
    "        end = start + batchSize\n",
    "        batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "        batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "        \n",
    "        yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义 char-CNN 分类器 model\n",
    "\n",
    "class CharCNN(object):\n",
    "    def __init__(self, config, charEmbedding):\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        self.isTraining = tf.placeholder(tf.bool, name=\"isTraining\")\n",
    "        self.epsilon = config.model.epsilon\n",
    "        self.decay = config.model.decay\n",
    "        \n",
    "        \n",
    "        # char embedding\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            # 利用 one-hot 编码的字符向量作为初始化的 embedding matrix\n",
    "            self.W = tf.Variable(tf.cast(charEmbedding, dtype=tf.float32, name=\"charEmbedding\"), name=\"W\")\n",
    "            \n",
    "            # 获取 char embedding\n",
    "            self.embededChars = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "            # 新增一个通道维度\n",
    "            self.embededCharsExpand = tf.expand_dims(self.embededChars, -1)\n",
    "        \n",
    "        \n",
    "        for index, current_layer in enumerate(config.model.convLayers):\n",
    "            print(\"the {0} conv layer to process\".format(index + 1))\n",
    "            with tf.name_scope(\"convLayer-%s\"%(index + 1)):\n",
    "                # 获取到字符的向量长度\n",
    "                filterWidth= self.embededCharsExpand.get_shape()[2].value\n",
    "                \n",
    "                # filterShape = [height, width, in_channels, out_channels]\n",
    "                filterShape = [current_layer[1], filterWidth, 1, current_layer[0]]\n",
    "                stdv = 1 / sqrt(current_layer[0] * current_layer[1])\n",
    "                \n",
    "                # 初始化 W 与 b 的值\n",
    "                wConv = tf.Variable(tf.random_uniform(filterShape, minval=-stdv, maxval=stdv), dtype=\"float32\", name=\"w\")\n",
    "                bConv = tf.Variable(tf.random_uniform(shape=[current_layer[0]], minval=-stdv, maxval=stdv), name=\"b\")\n",
    "                \n",
    "                # 构造卷积层\n",
    "                conv = tf.nn.conv2d(self.embededCharsExpand, wConv, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "                hConv = tf.nn.bias_add(conv, bConv)\n",
    "                hConv = tf.nn.relu(hConv)\n",
    "                \n",
    "                if current_layer[-1] is not None:\n",
    "                    ksizeShape = [1, current_layer[-1], 1, 1]\n",
    "                    hPool = tf.nn.max_pool(hConv, ksize=ksizeShape, strides=ksizeShape, padding=\"VALID\", name=\"pool\")\n",
    "                else:\n",
    "                    hPool = hConv\n",
    "                \n",
    "                print(\"the final hPool shape is: \", hPool.shape)\n",
    "                \n",
    "                \n",
    "                # 针对维度进行变换，转换成卷积层的输入维度\n",
    "                self.embededCharsExpand = tf.transpose(hPool, [0, 1, 3, 2], name=\"transpose\")\n",
    "        \n",
    "        \n",
    "        print(\"the current expand embedding is: \", self.embededCharsExpand, self.embededCharsExpand.shape)\n",
    "        print(\"the shape of embedding is: \", self.embededChars.shape)\n",
    "        \n",
    "        with tf.name_scope(\"reshape\"):\n",
    "            fcDim = self.embededCharsExpand.get_shape()[1].value * self.embededCharsExpand.get_shape()[2].value\n",
    "            self.inputReshape = tf.reshape(self.embededCharsExpand, [-1, fcDim])\n",
    "            print(\"the reshape of input: \", self.inputReshape.shape)\n",
    "        \n",
    "        \n",
    "        # 保存的是 unit number [34 * 256, 1024, 1024]\n",
    "        weights = [fcDim] + config.model.fcLayers\n",
    "        print(\"the weights are: \", weights)\n",
    "        \n",
    "        for index, fc_layer in enumerate(config.model.fcLayers):\n",
    "            with tf.name_scope(\"fcLayer-%s\"%(index + 1)):\n",
    "                print(\"begin process {}-th layer\".format(index + 1))\n",
    "                stdv = 1 / sqrt(weights[index])\n",
    "                \n",
    "                wFc = tf.Variable(tf.random_uniform([weights[index], fc_layer], minval=-stdv, maxval=stdv), dtype=\"float32\", name=\"w\")\n",
    "                bFc = tf.Variable(tf.random_uniform(shape=[fc_layer], minval=-stdv, maxval=stdv), dtype=\"float32\", name=\"b\")\n",
    "                \n",
    "                self.fcInput = tf.nn.relu(tf.matmul(self.inputReshape, wFc) + bFc)\n",
    "                \n",
    "                with tf.name_scope(\"dropOut\"):\n",
    "                    self.fcInputDrop = tf.nn.dropout(self.fcInput, self.dropoutKeepProb)\n",
    "                \n",
    "            self.inputReshape = self.fcInputDrop\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"outputLayer\"):\n",
    "            stdv = 1 / sqrt(weights[-1])\n",
    "            \n",
    "            wOut = tf.Variable(tf.random_uniform([config.model.fcLayers[-1], 1], minval=-stdv, maxval=stdv), dtype=\"float32\", name=\"w\")\n",
    "            bOut = tf.Variable(tf.random_uniform(shape=[1], minval=-stdv, maxval=stdv), name=\"b\")\n",
    "            \n",
    "            self.predictions = tf.nn.xw_plus_b(self.inputReshape, wOut, bOut,name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.0), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss =tf.reduce_sum(losses)\n",
    "        \n",
    "    \n",
    "    def _batchNorm(self, x):\n",
    "        print(\"the shape of x: \", x.get_shape())\n",
    "        gamma = tf.Variable(tf.ones([x.get_shape()[3].value]))\n",
    "        beta = tf.Variable(tf.zeros([x.get_shape()[3].value]))\n",
    "        self.popMean = tf.Variable(tf.zeros([x.get_shape()[3].value]), trainable=False, name=\"popMean\")\n",
    "        self.popVariance = tf.Variable(tf.ones([x.get_shape()[3].value]), trainable=False, name=\"popVariance\")\n",
    "        \n",
    "        def batchNormTraining():\n",
    "            batchMean, batchVariance = tf.nn.moments(x, [0, 1, 2], keep_dims=False)\n",
    "            decay = 0.99\n",
    "            trainMean = tf.assign(self.popMean, self.popMean * self.decay + batchMean * (1 - self.decay))\n",
    "            trainVariance = tf.assign(self.popVariance, self.popVariance * self.decay + batchVariance * (1 - self.decay))\n",
    "            \n",
    "            with tf.control_dependencies([trainMean, trainVariance]):\n",
    "                return tf.nn.batch_normalization(x, batchMean, batchVariance, beta, gamma, self.epsilon)\n",
    "        \n",
    "        def batchNormInference():\n",
    "            return tf.nn.batch_normalization(x, self.popMean, self.popVariance, beta, gamma, self.epsilon)\n",
    "        \n",
    "        batchNormalizedOutput = tf.cond(self.isTraining, batchNormTraining, batchNormInference)\n",
    "        \n",
    "        return tf.nn.relu(batchNormalizedOutput)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "def getMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "        生成 acc 和 auc 值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY, average=\"macro\")\n",
    "    recall = recall_score(trueY, binaryPredY, average=\"macro\")\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1 conv layer to process\n",
      "the final hPool shape is:  (?, 252, 1, 256)\n",
      "the 2 conv layer to process\n",
      "the final hPool shape is:  (?, 61, 1, 256)\n",
      "the 3 conv layer to process\n",
      "the final hPool shape is:  (?, 14, 1, 256)\n",
      "the current expand embedding is:  Tensor(\"convLayer-3/transpose:0\", shape=(?, 14, 256, 1), dtype=float32) (?, 14, 256, 1)\n",
      "the shape of embedding is:  (?, 1014, 70)\n",
      "the reshape of input:  (?, 3584)\n",
      "the weights are:  [3584, 512]\n",
      "begin process 1-th layer\n",
      "write to C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model_code\\summarys \n",
      "\n",
      "start to train models...\n",
      "2019-09-23T09:54:50.397980, step: 1, loss: 88.50857543945312, acc: 0.5625, auc: 0.5972, precision: 0.7795, recall: 0.5088\n",
      "2019-09-23T09:54:52.275045, step: 2, loss: 88.8498306274414, acc: 0.4844, auc: 0.4321, precision: 0.2422, recall: 0.5\n",
      "2019-09-23T09:54:54.071244, step: 3, loss: 88.65156555175781, acc: 0.5234, auc: 0.5243, precision: 0.2617, recall: 0.5\n",
      "2019-09-23T09:54:55.871427, step: 4, loss: 88.3875732421875, acc: 0.6094, auc: 0.5067, precision: 0.3047, recall: 0.5\n",
      "2019-09-23T09:54:57.634709, step: 5, loss: 88.91400909423828, acc: 0.4688, auc: 0.4708, precision: 0.2344, recall: 0.5\n",
      "2019-09-23T09:54:59.398015, step: 6, loss: 88.73311614990234, acc: 0.5156, auc: 0.4247, precision: 0.2578, recall: 0.5\n",
      "2019-09-23T09:55:01.174241, step: 7, loss: 89.07720947265625, acc: 0.4219, auc: 0.4945, precision: 0.2109, recall: 0.5\n",
      "2019-09-23T09:55:02.938522, step: 8, loss: 88.69154357910156, acc: 0.5, auc: 0.5457, precision: 0.25, recall: 0.5\n",
      "2019-09-23T09:55:04.709785, step: 9, loss: 88.67552185058594, acc: 0.5, auc: 0.5913, precision: 0.25, recall: 0.5\n",
      "2019-09-23T09:55:06.491020, step: 10, loss: 88.69625854492188, acc: 0.5078, auc: 0.5106, precision: 0.2539, recall: 0.5\n",
      "2019-09-23T09:55:08.241347, step: 11, loss: 88.68025970458984, acc: 0.4922, auc: 0.5756, precision: 0.2461, recall: 0.5\n",
      "2019-09-23T09:55:10.024148, step: 12, loss: 88.7879638671875, acc: 0.4531, auc: 0.5785, precision: 0.7244, recall: 0.507\n",
      "2019-09-23T09:55:11.780451, step: 13, loss: 88.65003967285156, acc: 0.5234, auc: 0.5841, precision: 0.5301, recall: 0.511\n",
      "2019-09-23T09:55:13.551713, step: 14, loss: 88.67472076416016, acc: 0.5547, auc: 0.4815, precision: 0.5551, recall: 0.5145\n",
      "2019-09-23T09:55:15.307017, step: 15, loss: 88.85711669921875, acc: 0.4375, auc: 0.5095, precision: 0.2205, recall: 0.4912\n",
      "2019-09-23T09:55:17.088252, step: 16, loss: 88.69718933105469, acc: 0.4766, auc: 0.5133, precision: 0.4167, recall: 0.4596\n",
      "2019-09-23T09:55:18.888438, step: 17, loss: 88.67250061035156, acc: 0.5078, auc: 0.5397, precision: 0.5041, recall: 0.5007\n",
      "2019-09-23T09:55:20.660697, step: 18, loss: 88.72178649902344, acc: 0.5234, auc: 0.4446, precision: 0.5121, recall: 0.5015\n",
      "2019-09-23T09:55:22.496785, step: 19, loss: 88.8072509765625, acc: 0.4766, auc: 0.4527, precision: 0.2383, recall: 0.5\n",
      "2019-09-23T09:55:24.390719, step: 20, loss: 88.76342010498047, acc: 0.5156, auc: 0.4758, precision: 0.75, recall: 0.5303\n",
      "2019-09-23T09:55:26.213843, step: 21, loss: 88.63824462890625, acc: 0.5312, auc: 0.5468, precision: 0.5056, recall: 0.5026\n",
      "2019-09-23T09:55:27.981115, step: 22, loss: 88.69097900390625, acc: 0.5547, auc: 0.4272, precision: 0.2773, recall: 0.5\n",
      "2019-09-23T09:55:29.722457, step: 23, loss: 88.76919555664062, acc: 0.5078, auc: 0.4237, precision: 0.2539, recall: 0.5\n",
      "2019-09-23T09:55:31.494728, step: 24, loss: 88.71662139892578, acc: 0.5078, auc: 0.4864, precision: 0.2539, recall: 0.5\n",
      "2019-09-23T09:55:33.283930, step: 25, loss: 88.69198608398438, acc: 0.5, auc: 0.5576, precision: 0.25, recall: 0.5\n",
      "2019-09-23T09:55:35.067161, step: 26, loss: 88.86505126953125, acc: 0.4688, auc: 0.475, precision: 0.2344, recall: 0.5\n",
      "2019-09-23T09:55:36.861361, step: 27, loss: 88.75541687011719, acc: 0.4922, auc: 0.4991, precision: 0.7421, recall: 0.5149\n",
      "2019-09-23T09:55:38.646586, step: 28, loss: 88.78138732910156, acc: 0.4922, auc: 0.4214, precision: 0.4845, recall: 0.4922\n",
      "2019-09-23T09:55:40.427822, step: 29, loss: 88.79254150390625, acc: 0.4297, auc: 0.4819, precision: 0.4247, recall: 0.4538\n",
      "2019-09-23T09:55:42.184123, step: 30, loss: 88.68194580078125, acc: 0.5391, auc: 0.5318, precision: 0.5389, recall: 0.533\n",
      "2019-09-23T09:55:43.952394, step: 31, loss: 88.65222930908203, acc: 0.5234, auc: 0.5035, precision: 0.4616, recall: 0.4752\n",
      "2019-09-23T09:55:45.712687, step: 32, loss: 88.74552154541016, acc: 0.5156, auc: 0.4421, precision: 0.2578, recall: 0.5\n",
      "2019-09-23T09:55:47.486939, step: 33, loss: 88.82154846191406, acc: 0.4844, auc: 0.4861, precision: 0.2422, recall: 0.5\n",
      "2019-09-23T09:55:49.256206, step: 34, loss: 88.74898529052734, acc: 0.4766, auc: 0.5576, precision: 0.2383, recall: 0.5\n",
      "2019-09-23T09:55:51.021485, step: 35, loss: 88.83546447753906, acc: 0.4297, auc: 0.5107, precision: 0.4154, recall: 0.4871\n",
      "2019-09-23T09:55:52.951323, step: 36, loss: 88.72785949707031, acc: 0.5078, auc: 0.5251, precision: 0.579, recall: 0.5269\n",
      "2019-09-23T09:55:54.788408, step: 37, loss: 88.79230499267578, acc: 0.4219, auc: 0.4326, precision: 0.4038, recall: 0.414\n",
      "2019-09-23T09:55:56.595575, step: 38, loss: 88.77586364746094, acc: 0.4922, auc: 0.4803, precision: 0.533, recall: 0.5209\n",
      "2019-09-23T09:55:58.369829, step: 39, loss: 88.69454956054688, acc: 0.5312, auc: 0.5784, precision: 0.6293, recall: 0.544\n",
      "2019-09-23T09:56:00.554984, step: 40, loss: 88.61408996582031, acc: 0.5859, auc: 0.6027, precision: 0.5729, recall: 0.5697\n",
      "2019-09-23T09:56:03.364469, step: 41, loss: 88.60317993164062, acc: 0.5234, auc: 0.5757, precision: 0.2617, recall: 0.5\n",
      "2019-09-23T09:56:05.430941, step: 42, loss: 88.61054229736328, acc: 0.5078, auc: 0.6022, precision: 0.2539, recall: 0.5\n",
      "2019-09-23T09:56:07.239105, step: 43, loss: 88.6556396484375, acc: 0.5234, auc: 0.4869, precision: 0.2617, recall: 0.5\n",
      "2019-09-23T09:56:09.074195, step: 44, loss: 88.64214324951172, acc: 0.5234, auc: 0.4862, precision: 0.2617, recall: 0.5\n",
      "2019-09-23T09:56:10.887346, step: 45, loss: 89.3780746459961, acc: 0.375, auc: 0.5292, precision: 0.1875, recall: 0.5\n",
      "2019-09-23T09:56:12.691520, step: 46, loss: 88.76858520507812, acc: 0.5312, auc: 0.4701, precision: 0.5507, recall: 0.5355\n",
      "2019-09-23T09:56:14.656264, step: 47, loss: 88.64877319335938, acc: 0.5234, auc: 0.5573, precision: 0.5149, recall: 0.5149\n",
      "2019-09-23T09:56:16.545211, step: 48, loss: 88.97328186035156, acc: 0.4219, auc: 0.5205, precision: 0.2109, recall: 0.5\n",
      "2019-09-23T09:56:18.342403, step: 49, loss: 88.76239013671875, acc: 0.5234, auc: 0.5195, precision: 0.5954, recall: 0.5506\n",
      "2019-09-23T09:56:20.087735, step: 50, loss: 88.73939514160156, acc: 0.4844, auc: 0.4929, precision: 0.4926, recall: 0.4931\n",
      "2019-09-23T09:56:21.844037, step: 51, loss: 88.63053894042969, acc: 0.5156, auc: 0.44, precision: 0.4014, recall: 0.441\n",
      "2019-09-23T09:56:23.643225, step: 52, loss: 88.6246566772461, acc: 0.5312, auc: 0.461, precision: 0.2656, recall: 0.5\n",
      "2019-09-23T09:56:25.432439, step: 53, loss: 88.355224609375, acc: 0.5547, auc: 0.4888, precision: 0.2773, recall: 0.5\n",
      "2019-09-23T09:56:27.221653, step: 54, loss: 88.50052642822266, acc: 0.5312, auc: 0.5096, precision: 0.2656, recall: 0.5\n",
      "2019-09-23T09:56:28.962008, step: 55, loss: 89.32752990722656, acc: 0.4609, auc: 0.4419, precision: 0.2305, recall: 0.5\n",
      "2019-09-23T09:56:30.724298, step: 56, loss: 88.71742248535156, acc: 0.5078, auc: 0.4899, precision: 0.2539, recall: 0.5\n",
      "2019-09-23T09:56:32.503525, step: 57, loss: 88.62356567382812, acc: 0.5078, auc: 0.5866, precision: 0.2539, recall: 0.5\n",
      "2019-09-23T09:56:34.258830, step: 58, loss: 88.9373550415039, acc: 0.4688, auc: 0.4794, precision: 0.2344, recall: 0.5\n",
      "2019-09-23T09:56:36.056025, step: 59, loss: 88.45722961425781, acc: 0.6016, auc: 0.4418, precision: 0.3008, recall: 0.5\n",
      "2019-09-23T09:56:37.802350, step: 60, loss: 89.37113952636719, acc: 0.4375, auc: 0.4633, precision: 0.2188, recall: 0.5\n",
      "2019-09-23T09:56:39.593559, step: 61, loss: 88.43155670166016, acc: 0.5625, auc: 0.5533, precision: 0.2812, recall: 0.5\n",
      "2019-09-23T09:56:41.352854, step: 62, loss: 89.46455383300781, acc: 0.4141, auc: 0.3867, precision: 0.207, recall: 0.5\n",
      "2019-09-23T09:56:43.085219, step: 63, loss: 88.80261993408203, acc: 0.5156, auc: 0.4686, precision: 0.6458, recall: 0.5422\n",
      "2019-09-23T09:56:44.842519, step: 64, loss: 88.65431213378906, acc: 0.5469, auc: 0.5522, precision: 0.5365, recall: 0.5281\n",
      "2019-09-23T09:56:46.591839, step: 65, loss: 88.676513671875, acc: 0.5234, auc: 0.4884, precision: 0.2617, recall: 0.5\n",
      "2019-09-23T09:56:48.320216, step: 66, loss: 88.97610473632812, acc: 0.4375, auc: 0.4514, precision: 0.2188, recall: 0.5\n",
      "2019-09-23T09:56:50.101452, step: 67, loss: 88.750732421875, acc: 0.4688, auc: 0.4664, precision: 0.4688, recall: 0.4686\n",
      "2019-09-23T09:56:51.852767, step: 68, loss: 88.81047821044922, acc: 0.4453, auc: 0.3857, precision: 0.4064, recall: 0.4266\n",
      "2019-09-23T09:56:53.618045, step: 69, loss: 88.65359497070312, acc: 0.5312, auc: 0.5106, precision: 0.5973, recall: 0.5089\n",
      "2019-09-23T09:56:55.352406, step: 70, loss: 88.66036987304688, acc: 0.5234, auc: 0.5027, precision: 0.2638, recall: 0.4926\n",
      "2019-09-23T09:56:57.082777, step: 71, loss: 88.65486145019531, acc: 0.5078, auc: 0.5306, precision: 0.2539, recall: 0.5\n",
      "2019-09-23T09:56:58.839079, step: 72, loss: 88.4373779296875, acc: 0.5547, auc: 0.5004, precision: 0.2773, recall: 0.5\n",
      "2019-09-23T09:57:00.574437, step: 73, loss: 88.42764282226562, acc: 0.5391, auc: 0.5075, precision: 0.2695, recall: 0.5\n",
      "2019-09-23T09:57:02.339716, step: 74, loss: 89.96839141845703, acc: 0.4297, auc: 0.4618, precision: 0.2148, recall: 0.5\n",
      "2019-09-23T09:57:04.083052, step: 75, loss: 88.73697662353516, acc: 0.4766, auc: 0.4935, precision: 0.474, recall: 0.4805\n",
      "2019-09-23T09:57:05.823397, step: 76, loss: 88.69895935058594, acc: 0.4609, auc: 0.5049, precision: 0.4624, recall: 0.4651\n",
      "2019-09-23T09:57:07.592664, step: 77, loss: 88.70813751220703, acc: 0.4844, auc: 0.4991, precision: 0.484, recall: 0.484\n",
      "2019-09-23T09:57:09.328023, step: 78, loss: 88.74322509765625, acc: 0.4844, auc: 0.5071, precision: 0.4914, recall: 0.4971\n",
      "2019-09-23T09:57:11.087316, step: 79, loss: 88.71871948242188, acc: 0.5312, auc: 0.517, precision: 0.5507, recall: 0.5355\n",
      "2019-09-23T09:57:12.816691, step: 80, loss: 88.73529052734375, acc: 0.5156, auc: 0.4809, precision: 0.5118, recall: 0.5119\n",
      "2019-09-23T09:57:14.570998, step: 81, loss: 88.69847106933594, acc: 0.5156, auc: 0.4643, precision: 0.2578, recall: 0.5\n",
      "2019-09-23T09:57:16.343258, step: 82, loss: 88.53256225585938, acc: 0.5391, auc: 0.5222, precision: 0.2695, recall: 0.5\n",
      "2019-09-23T09:57:18.070636, step: 83, loss: 89.06280517578125, acc: 0.4688, auc: 0.4583, precision: 0.2344, recall: 0.5\n",
      "2019-09-23T09:57:19.824945, step: 84, loss: 88.56700897216797, acc: 0.5469, auc: 0.5785, precision: 0.5584, recall: 0.5122\n",
      "2019-09-23T09:57:21.558308, step: 85, loss: 88.87696838378906, acc: 0.4453, auc: 0.5256, precision: 0.2244, recall: 0.4914\n",
      "2019-09-23T09:57:23.298652, step: 86, loss: 88.69929504394531, acc: 0.5312, auc: 0.5208, precision: 0.5292, recall: 0.5279\n",
      "2019-09-23T09:57:25.043983, step: 87, loss: 88.55158996582031, acc: 0.5156, auc: 0.6132, precision: 0.5258, recall: 0.5094\n",
      "2019-09-23T09:57:26.778347, step: 88, loss: 88.78054809570312, acc: 0.4688, auc: 0.4389, precision: 0.436, recall: 0.4478\n",
      "2019-09-23T09:57:28.514700, step: 89, loss: 88.85647583007812, acc: 0.4766, auc: 0.4869, precision: 0.2383, recall: 0.5\n",
      "2019-09-23T09:57:30.258037, step: 90, loss: 88.9344253540039, acc: 0.4453, auc: 0.4544, precision: 0.4449, recall: 0.4855\n",
      "2019-09-23T09:57:32.007358, step: 91, loss: 88.61911010742188, acc: 0.5234, auc: 0.5458, precision: 0.499, recall: 0.4993\n",
      "2019-09-23T09:57:33.780614, step: 92, loss: 88.2450942993164, acc: 0.5625, auc: 0.5541, precision: 0.2812, recall: 0.5\n",
      "2019-09-23T09:57:35.520960, step: 93, loss: 87.54481506347656, acc: 0.5859, auc: 0.4928, precision: 0.293, recall: 0.5\n",
      "2019-09-23T09:57:37.259309, step: 94, loss: 91.50292205810547, acc: 0.5, auc: 0.509, precision: 0.25, recall: 0.5\n",
      "2019-09-23T09:57:39.031568, step: 95, loss: 88.59527587890625, acc: 0.5391, auc: 0.5711, precision: 0.5388, recall: 0.5252\n",
      "2019-09-23T09:57:40.755956, step: 96, loss: 88.54519653320312, acc: 0.5391, auc: 0.5218, precision: 0.5543, recall: 0.5113\n",
      "2019-09-23T09:57:42.511261, step: 97, loss: 88.91179656982422, acc: 0.5078, auc: 0.4823, precision: 0.6626, recall: 0.5291\n",
      "2019-09-23T09:57:44.260581, step: 98, loss: 88.85926818847656, acc: 0.4844, auc: 0.4196, precision: 0.4524, recall: 0.47\n",
      "2019-09-23T09:57:46.000930, step: 99, loss: 88.53466796875, acc: 0.5625, auc: 0.3807, precision: 0.2927, recall: 0.4675\n",
      "2019-09-23T09:57:47.789153, step: 100, loss: 88.00531768798828, acc: 0.5703, auc: 0.5061, precision: 0.2852, recall: 0.5\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T09:58:59.941078, step: 100, loss: 88.90494400415665, acc: 0.5058102564102563, auc: 0.4998153846153846, precision: 0.25290256410256406, recall: 0.5\n",
      "2019-09-23T09:59:01.761978, step: 101, loss: 89.86859893798828, acc: 0.4453, auc: 0.528, precision: 0.2227, recall: 0.5\n",
      "2019-09-23T09:59:03.579505, step: 102, loss: 89.0427017211914, acc: 0.4609, auc: 0.5382, precision: 0.2305, recall: 0.5\n",
      "2019-09-23T09:59:05.355400, step: 103, loss: 88.66455841064453, acc: 0.5156, auc: 0.5828, precision: 0.748, recall: 0.5373\n",
      "2019-09-23T09:59:07.100731, step: 104, loss: 88.95712280273438, acc: 0.4297, auc: 0.4911, precision: 0.4438, recall: 0.4792\n",
      "2019-09-23T09:59:08.898359, step: 105, loss: 88.65824127197266, acc: 0.5391, auc: 0.5192, precision: 0.5507, recall: 0.5347\n",
      "2019-09-23T09:59:10.660640, step: 106, loss: 88.757080078125, acc: 0.5156, auc: 0.5355, precision: 0.6161, recall: 0.5512\n",
      "2019-09-23T09:59:12.428790, step: 107, loss: 88.65119934082031, acc: 0.5312, auc: 0.5999, precision: 0.6048, recall: 0.5578\n",
      "2019-09-23T09:59:14.188319, step: 108, loss: 88.73383331298828, acc: 0.4766, auc: 0.4885, precision: 0.4786, recall: 0.4792\n",
      "2019-09-23T09:59:15.949138, step: 109, loss: 88.65567016601562, acc: 0.4922, auc: 0.5139, precision: 0.4922, recall: 0.4921\n",
      "2019-09-23T09:59:17.735380, step: 110, loss: 88.92520141601562, acc: 0.4453, auc: 0.4144, precision: 0.4323, recall: 0.4492\n",
      "2019-09-23T09:59:19.472341, step: 111, loss: 88.80451202392578, acc: 0.4297, auc: 0.4325, precision: 0.4304, recall: 0.4298\n",
      "2019-09-23T09:59:21.213893, step: 112, loss: 88.78569030761719, acc: 0.5, auc: 0.4662, precision: 0.748, recall: 0.5077\n",
      "2019-09-23T09:59:22.980327, step: 113, loss: 88.41910552978516, acc: 0.6172, auc: 0.5068, precision: 0.5636, recall: 0.5194\n",
      "2019-09-23T09:59:24.952150, step: 114, loss: 88.45067596435547, acc: 0.5156, auc: 0.5995, precision: 0.2578, recall: 0.5\n",
      "2019-09-23T09:59:26.788078, step: 115, loss: 88.86652374267578, acc: 0.4922, auc: 0.5087, precision: 0.2461, recall: 0.5\n",
      "2019-09-23T09:59:28.533689, step: 116, loss: 88.64784240722656, acc: 0.5156, auc: 0.5178, precision: 0.2578, recall: 0.5\n",
      "2019-09-23T09:59:30.318416, step: 117, loss: 89.1942138671875, acc: 0.4141, auc: 0.5072, precision: 0.207, recall: 0.5\n",
      "2019-09-23T09:59:32.059043, step: 118, loss: 88.6848373413086, acc: 0.5078, auc: 0.4998, precision: 0.499, recall: 0.4993\n",
      "2019-09-23T09:59:33.816530, step: 119, loss: 88.7140884399414, acc: 0.5234, auc: 0.4941, precision: 0.5212, recall: 0.5149\n",
      "2019-09-23T09:59:35.581253, step: 120, loss: 88.71221923828125, acc: 0.5156, auc: 0.4693, precision: 0.2598, recall: 0.4925\n",
      "2019-09-23T09:59:37.361584, step: 121, loss: 88.47539520263672, acc: 0.5469, auc: 0.4682, precision: 0.2734, recall: 0.5\n",
      "2019-09-23T09:59:39.122690, step: 122, loss: 87.00825500488281, acc: 0.5938, auc: 0.5757, precision: 0.2969, recall: 0.5\n",
      "2019-09-23T09:59:40.886831, step: 123, loss: 97.66177368164062, acc: 0.4922, auc: 0.5243, precision: 0.2461, recall: 0.5\n",
      "2019-09-23T09:59:42.734082, step: 124, loss: 88.6202163696289, acc: 0.5234, auc: 0.4999, precision: 0.5119, recall: 0.5007\n",
      "2019-09-23T09:59:44.493239, step: 125, loss: 88.464599609375, acc: 0.5312, auc: 0.5201, precision: 0.2656, recall: 0.5\n",
      "2019-09-23T09:59:46.250755, step: 126, loss: 89.26422882080078, acc: 0.4688, auc: 0.4096, precision: 0.2344, recall: 0.5\n",
      "2019-09-23T09:59:48.025793, step: 127, loss: 88.75349426269531, acc: 0.4844, auc: 0.5015, precision: 0.454, recall: 0.4844\n",
      "2019-09-23T09:59:49.774394, step: 128, loss: 88.42387390136719, acc: 0.5625, auc: 0.5709, precision: 0.5636, recall: 0.5377\n",
      "2019-09-23T09:59:51.544455, step: 129, loss: 88.8046646118164, acc: 0.5, auc: 0.4718, precision: 0.4664, recall: 0.493\n",
      "2019-09-23T09:59:53.349621, step: 130, loss: 89.33935546875, acc: 0.3984, auc: 0.4865, precision: 0.3033, recall: 0.4697\n",
      "2019-09-23T09:59:55.284654, step: 131, loss: 88.6778335571289, acc: 0.5859, auc: 0.5389, precision: 0.5869, recall: 0.5883\n",
      "2019-09-23T09:59:57.186953, step: 132, loss: 89.06095123291016, acc: 0.4453, auc: 0.5293, precision: 0.5195, recall: 0.503\n",
      "2019-09-23T09:59:58.920935, step: 133, loss: 88.68515014648438, acc: 0.5391, auc: 0.5203, precision: 0.5399, recall: 0.5391\n",
      "2019-09-23T10:00:00.691675, step: 134, loss: 88.59229278564453, acc: 0.5312, auc: 0.5593, precision: 0.5417, recall: 0.5392\n",
      "2019-09-23T10:00:02.471860, step: 135, loss: 88.72164916992188, acc: 0.5078, auc: 0.5154, precision: 0.4982, recall: 0.4988\n",
      "2019-09-23T10:00:04.239836, step: 136, loss: 88.50335693359375, acc: 0.5469, auc: 0.4819, precision: 0.4885, recall: 0.496\n",
      "2019-09-23T10:00:05.967989, step: 137, loss: 91.61502075195312, acc: 0.4062, auc: 0.42, precision: 0.2031, recall: 0.5\n",
      "2019-09-23T10:00:07.722381, step: 138, loss: 88.77122497558594, acc: 0.4766, auc: 0.487, precision: 0.479, recall: 0.4846\n",
      "2019-09-23T10:00:09.450576, step: 139, loss: 88.68219757080078, acc: 0.5, auc: 0.5263, precision: 0.4986, recall: 0.4987\n",
      "2019-09-23T10:00:11.231267, step: 140, loss: 88.80044555664062, acc: 0.4531, auc: 0.471, precision: 0.4556, recall: 0.4552\n",
      "2019-09-23T10:00:12.957010, step: 141, loss: 89.121337890625, acc: 0.4219, auc: 0.5468, precision: 0.5806, recall: 0.5101\n",
      "2019-09-23T10:00:14.718069, step: 142, loss: 88.69766235351562, acc: 0.5, auc: 0.541, precision: 0.512, recall: 0.5099\n",
      "2019-09-23T10:00:16.473435, step: 143, loss: 88.69080352783203, acc: 0.4844, auc: 0.511, precision: 0.4701, recall: 0.4735\n",
      "2019-09-23T10:00:18.208780, step: 144, loss: 88.85633087158203, acc: 0.4688, auc: 0.4391, precision: 0.4526, recall: 0.474\n",
      "2019-09-23T10:00:19.970671, step: 145, loss: 88.89186096191406, acc: 0.4453, auc: 0.4196, precision: 0.4412, recall: 0.4652\n",
      "2019-09-23T10:00:21.706831, step: 146, loss: 88.67958068847656, acc: 0.5234, auc: 0.5358, precision: 0.5261, recall: 0.5269\n",
      "2019-09-23T10:00:23.486864, step: 147, loss: 88.51910400390625, acc: 0.5703, auc: 0.5016, precision: 0.6161, recall: 0.521\n",
      "2019-09-23T10:00:25.240308, step: 148, loss: 88.45250701904297, acc: 0.5391, auc: 0.5264, precision: 0.2695, recall: 0.5\n",
      "2019-09-23T10:00:27.013711, step: 149, loss: 88.5991439819336, acc: 0.5156, auc: 0.5342, precision: 0.2578, recall: 0.5\n",
      "2019-09-23T10:00:28.763724, step: 150, loss: 87.9747085571289, acc: 0.5781, auc: 0.5268, precision: 0.2891, recall: 0.5\n",
      "2019-09-23T10:00:30.469206, step: 151, loss: 89.64443969726562, acc: 0.5, auc: 0.501, precision: 0.25, recall: 0.5\n",
      "2019-09-23T10:00:32.222745, step: 152, loss: 88.88348388671875, acc: 0.4844, auc: 0.5166, precision: 0.2422, recall: 0.5\n",
      "2019-09-23T10:00:33.966380, step: 153, loss: 88.4862060546875, acc: 0.5469, auc: 0.5361, precision: 0.6683, recall: 0.5253\n",
      "2019-09-23T10:00:35.768456, step: 154, loss: 88.54059600830078, acc: 0.5625, auc: 0.4485, precision: 0.7795, recall: 0.5088\n",
      "2019-09-23T10:00:37.540870, step: 155, loss: 88.53085327148438, acc: 0.5234, auc: 0.5197, precision: 0.2617, recall: 0.5\n",
      "2019-09-23T10:00:39.353341, step: 156, loss: 90.11358642578125, acc: 0.4062, auc: 0.479, precision: 0.2031, recall: 0.5\n",
      "start to train models...\n",
      "2019-09-23T10:00:41.303943, step: 157, loss: 88.82620239257812, acc: 0.4688, auc: 0.4495, precision: 0.4198, recall: 0.4688\n",
      "2019-09-23T10:00:43.042371, step: 158, loss: 88.77117919921875, acc: 0.4844, auc: 0.4814, precision: 0.4824, recall: 0.4821\n",
      "2019-09-23T10:00:44.793249, step: 159, loss: 88.99012756347656, acc: 0.4531, auc: 0.4727, precision: 0.2266, recall: 0.5\n",
      "2019-09-23T10:00:46.532385, step: 160, loss: 88.69526672363281, acc: 0.5, auc: 0.5665, precision: 0.5125, recall: 0.5057\n",
      "2019-09-23T10:00:48.285745, step: 161, loss: 88.802490234375, acc: 0.4453, auc: 0.4644, precision: 0.4427, recall: 0.4569\n",
      "2019-09-23T10:00:50.007699, step: 162, loss: 88.81962585449219, acc: 0.4688, auc: 0.4571, precision: 0.4695, recall: 0.4859\n",
      "2019-09-23T10:00:51.771242, step: 163, loss: 88.79811096191406, acc: 0.5, auc: 0.4322, precision: 0.548, recall: 0.5072\n",
      "2019-09-23T10:00:53.534816, step: 164, loss: 88.7998046875, acc: 0.4609, auc: 0.4549, precision: 0.4907, recall: 0.4931\n",
      "2019-09-23T10:00:55.317586, step: 165, loss: 88.56489562988281, acc: 0.5547, auc: 0.5137, precision: 0.2773, recall: 0.5\n",
      "2019-09-23T10:00:57.103984, step: 166, loss: 89.34778594970703, acc: 0.4141, auc: 0.5326, precision: 0.207, recall: 0.5\n",
      "2019-09-23T10:00:58.939356, step: 167, loss: 88.75627136230469, acc: 0.4531, auc: 0.4483, precision: 0.4541, recall: 0.4541\n",
      "2019-09-23T10:01:00.707468, step: 168, loss: 88.71153259277344, acc: 0.5, auc: 0.5182, precision: 0.4664, recall: 0.493\n",
      "2019-09-23T10:01:02.468674, step: 169, loss: 88.6990966796875, acc: 0.5156, auc: 0.561, precision: 0.5451, recall: 0.5348\n",
      "2019-09-23T10:01:04.264662, step: 170, loss: 88.5993423461914, acc: 0.5078, auc: 0.6012, precision: 0.2539, recall: 0.5\n",
      "2019-09-23T10:01:06.094856, step: 171, loss: 89.154052734375, acc: 0.4609, auc: 0.3992, precision: 0.2305, recall: 0.5\n",
      "2019-09-23T10:01:07.889109, step: 172, loss: 88.68191528320312, acc: 0.5312, auc: 0.5182, precision: 0.4981, recall: 0.499\n",
      "2019-09-23T10:01:09.669413, step: 173, loss: 88.74129486083984, acc: 0.4688, auc: 0.5953, precision: 0.2344, recall: 0.5\n",
      "2019-09-23T10:01:11.439354, step: 174, loss: 88.5822982788086, acc: 0.5938, auc: 0.6319, precision: 0.578, recall: 0.5654\n",
      "2019-09-23T10:01:13.259999, step: 175, loss: 88.69674682617188, acc: 0.4922, auc: 0.5712, precision: 0.2461, recall: 0.5\n",
      "2019-09-23T10:01:15.216924, step: 176, loss: 88.53533935546875, acc: 0.5078, auc: 0.6265, precision: 0.752, recall: 0.5078\n",
      "2019-09-23T10:01:17.258911, step: 177, loss: 88.9016342163086, acc: 0.4609, auc: 0.5623, precision: 0.2305, recall: 0.5\n",
      "2019-09-23T10:01:19.308227, step: 178, loss: 88.71209716796875, acc: 0.4844, auc: 0.5532, precision: 0.5084, recall: 0.5031\n",
      "2019-09-23T10:01:21.258995, step: 179, loss: 88.5124282836914, acc: 0.5625, auc: 0.5361, precision: 0.5193, recall: 0.5132\n",
      "2019-09-23T10:01:23.083774, step: 180, loss: 87.72755432128906, acc: 0.5625, auc: 0.5665, precision: 0.2812, recall: 0.5\n",
      "2019-09-23T10:01:24.856289, step: 181, loss: 95.83511352539062, acc: 0.4375, auc: 0.4675, precision: 0.2188, recall: 0.5\n",
      "2019-09-23T10:01:26.610333, step: 182, loss: 88.67881774902344, acc: 0.5, auc: 0.5496, precision: 0.5268, recall: 0.5117\n",
      "2019-09-23T10:01:28.389065, step: 183, loss: 88.81420135498047, acc: 0.4531, auc: 0.5926, precision: 0.2266, recall: 0.5\n",
      "2019-09-23T10:01:30.131667, step: 184, loss: 88.51167297363281, acc: 0.5391, auc: 0.5753, precision: 0.6642, recall: 0.5247\n",
      "2019-09-23T10:01:31.937133, step: 185, loss: 88.68406677246094, acc: 0.5078, auc: 0.5197, precision: 0.5042, recall: 0.501\n",
      "2019-09-23T10:01:33.942375, step: 186, loss: 88.59488677978516, acc: 0.4844, auc: 0.5869, precision: 0.4333, recall: 0.4844\n",
      "2019-09-23T10:01:35.817538, step: 187, loss: 88.44581604003906, acc: 0.5781, auc: 0.6166, precision: 0.586, recall: 0.5718\n",
      "2019-09-23T10:01:37.676356, step: 188, loss: 88.24810791015625, acc: 0.5234, auc: 0.6127, precision: 0.4642, recall: 0.4946\n",
      "2019-09-23T10:01:39.462950, step: 189, loss: 87.85551452636719, acc: 0.5625, auc: 0.5835, precision: 0.6532, recall: 0.5187\n",
      "2019-09-23T10:01:41.358437, step: 190, loss: 87.47544860839844, acc: 0.5547, auc: 0.5275, precision: 0.2817, recall: 0.4863\n",
      "2019-09-23T10:01:43.293937, step: 191, loss: 90.51500701904297, acc: 0.4453, auc: 0.721, precision: 0.2227, recall: 0.5\n",
      "2019-09-23T10:01:45.327803, step: 192, loss: 89.1012191772461, acc: 0.5078, auc: 0.465, precision: 0.2539, recall: 0.5\n",
      "2019-09-23T10:01:47.329739, step: 193, loss: 88.90352630615234, acc: 0.4609, auc: 0.548, precision: 0.2305, recall: 0.5\n",
      "2019-09-23T10:01:49.128937, step: 194, loss: 88.6112060546875, acc: 0.5391, auc: 0.5887, precision: 0.5679, recall: 0.5477\n",
      "2019-09-23T10:01:50.929365, step: 195, loss: 88.61650848388672, acc: 0.5234, auc: 0.559, precision: 0.5343, recall: 0.5181\n",
      "2019-09-23T10:01:52.754670, step: 196, loss: 88.3917236328125, acc: 0.5938, auc: 0.5691, precision: 0.5957, recall: 0.5765\n",
      "2019-09-23T10:01:54.839533, step: 197, loss: 88.18719482421875, acc: 0.5156, auc: 0.5907, precision: 0.2578, recall: 0.5\n",
      "2019-09-23T10:01:56.787236, step: 198, loss: 86.2467041015625, acc: 0.5469, auc: 0.6958, precision: 0.2734, recall: 0.5\n",
      "2019-09-23T10:01:58.973214, step: 199, loss: 94.03079223632812, acc: 0.4766, auc: 0.6736, precision: 0.2383, recall: 0.5\n",
      "2019-09-23T10:02:00.831391, step: 200, loss: 87.50788116455078, acc: 0.5625, auc: 0.561, precision: 0.2812, recall: 0.5\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:03:08.953570, step: 200, loss: 89.94314633882962, acc: 0.5056051282051283, auc: 0.580497435897436, precision: 0.2656384615384616, recall: 0.5000666666666667\n",
      "2019-09-23T10:03:10.697887, step: 201, loss: 91.14998626708984, acc: 0.4766, auc: 0.5268, precision: 0.2383, recall: 0.5\n",
      "2019-09-23T10:03:12.422625, step: 202, loss: 89.03620147705078, acc: 0.4453, auc: 0.5179, precision: 0.4708, recall: 0.4931\n",
      "2019-09-23T10:03:14.171751, step: 203, loss: 88.62063598632812, acc: 0.5234, auc: 0.5374, precision: 0.532, recall: 0.5286\n",
      "2019-09-23T10:03:15.882062, step: 204, loss: 88.57398986816406, acc: 0.4844, auc: 0.5198, precision: 0.4825, recall: 0.4824\n",
      "2019-09-23T10:03:17.623217, step: 205, loss: 88.13665771484375, acc: 0.5, auc: 0.5995, precision: 0.4187, recall: 0.4926\n",
      "2019-09-23T10:03:19.366149, step: 206, loss: 88.56761169433594, acc: 0.4922, auc: 0.6259, precision: 0.6125, recall: 0.5265\n",
      "2019-09-23T10:03:21.099798, step: 207, loss: 87.61026000976562, acc: 0.5469, auc: 0.6, precision: 0.5554, recall: 0.5525\n",
      "2019-09-23T10:03:22.848986, step: 208, loss: 85.86076354980469, acc: 0.5781, auc: 0.749, precision: 0.647, recall: 0.5926\n",
      "2019-09-23T10:03:24.566655, step: 209, loss: 87.73418426513672, acc: 0.5391, auc: 0.5895, precision: 0.5406, recall: 0.5366\n",
      "2019-09-23T10:03:26.321287, step: 210, loss: 88.0643310546875, acc: 0.5703, auc: 0.6175, precision: 0.566, recall: 0.5658\n",
      "2019-09-23T10:03:28.076771, step: 211, loss: 87.35487365722656, acc: 0.5312, auc: 0.652, precision: 0.5627, recall: 0.5181\n",
      "2019-09-23T10:03:29.841361, step: 212, loss: 86.83653259277344, acc: 0.5938, auc: 0.6249, precision: 0.5968, recall: 0.5949\n",
      "2019-09-23T10:03:31.575340, step: 213, loss: 87.76055908203125, acc: 0.5312, auc: 0.6144, precision: 0.5359, recall: 0.5275\n",
      "2019-09-23T10:03:33.341368, step: 214, loss: 85.10291290283203, acc: 0.5703, auc: 0.668, precision: 0.5976, recall: 0.5956\n",
      "2019-09-23T10:03:35.078434, step: 215, loss: 123.78961944580078, acc: 0.5078, auc: 0.6005, precision: 0.2539, recall: 0.5\n",
      "2019-09-23T10:03:36.810635, step: 216, loss: 88.20884704589844, acc: 0.5, auc: 0.6072, precision: 0.5682, recall: 0.5437\n",
      "2019-09-23T10:03:38.559804, step: 217, loss: 87.34278106689453, acc: 0.6406, auc: 0.6873, precision: 0.6558, recall: 0.6406\n",
      "2019-09-23T10:03:40.328112, step: 218, loss: 87.37898254394531, acc: 0.5859, auc: 0.6508, precision: 0.5922, recall: 0.5804\n",
      "2019-09-23T10:03:42.051196, step: 219, loss: 86.010498046875, acc: 0.6016, auc: 0.6884, precision: 0.6266, recall: 0.6165\n",
      "2019-09-23T10:03:43.804527, step: 220, loss: 85.35165405273438, acc: 0.6328, auc: 0.6858, precision: 0.6355, recall: 0.6328\n",
      "2019-09-23T10:03:45.552923, step: 221, loss: 83.015869140625, acc: 0.6094, auc: 0.735, precision: 0.6169, recall: 0.6137\n",
      "2019-09-23T10:03:47.471829, step: 222, loss: 89.36963653564453, acc: 0.5781, auc: 0.6179, precision: 0.5633, recall: 0.5625\n",
      "2019-09-23T10:03:49.321162, step: 223, loss: 86.57642364501953, acc: 0.5469, auc: 0.6828, precision: 0.6184, recall: 0.5676\n",
      "2019-09-23T10:03:51.059698, step: 224, loss: 83.40119171142578, acc: 0.6562, auc: 0.7197, precision: 0.6772, recall: 0.6562\n",
      "2019-09-23T10:03:52.796283, step: 225, loss: 85.13243103027344, acc: 0.6016, auc: 0.6431, precision: 0.5988, recall: 0.5985\n",
      "2019-09-23T10:03:54.575261, step: 226, loss: 80.65672302246094, acc: 0.6562, auc: 0.7272, precision: 0.6826, recall: 0.6591\n",
      "2019-09-23T10:03:56.331727, step: 227, loss: 82.28240966796875, acc: 0.6094, auc: 0.6519, precision: 0.6098, recall: 0.6094\n",
      "2019-09-23T10:03:58.093276, step: 228, loss: 85.82373046875, acc: 0.6094, auc: 0.6435, precision: 0.651, recall: 0.6203\n",
      "2019-09-23T10:03:59.878113, step: 229, loss: 85.25392150878906, acc: 0.5547, auc: 0.6345, precision: 0.5473, recall: 0.5341\n",
      "2019-09-23T10:04:01.641796, step: 230, loss: 85.65174865722656, acc: 0.6484, auc: 0.6777, precision: 0.6488, recall: 0.6484\n",
      "2019-09-23T10:04:03.397919, step: 231, loss: 85.96310424804688, acc: 0.6172, auc: 0.6696, precision: 0.6234, recall: 0.6127\n",
      "2019-09-23T10:04:05.133582, step: 232, loss: 82.69573974609375, acc: 0.6484, auc: 0.7045, precision: 0.7031, recall: 0.6524\n",
      "2019-09-23T10:04:06.911096, step: 233, loss: 85.9564208984375, acc: 0.5938, auc: 0.6736, precision: 0.6153, recall: 0.5456\n",
      "2019-09-23T10:04:08.713190, step: 234, loss: 87.8559799194336, acc: 0.5469, auc: 0.6481, precision: 0.5942, recall: 0.5806\n",
      "2019-09-23T10:04:10.471509, step: 235, loss: 84.00142669677734, acc: 0.5625, auc: 0.6862, precision: 0.6061, recall: 0.5513\n",
      "2019-09-23T10:04:12.252509, step: 236, loss: 80.53648376464844, acc: 0.7031, auc: 0.7539, precision: 0.701, recall: 0.703\n",
      "2019-09-23T10:04:14.170585, step: 237, loss: 83.32888793945312, acc: 0.6328, auc: 0.6791, precision: 0.6743, recall: 0.6037\n",
      "2019-09-23T10:04:16.067424, step: 238, loss: 84.27771759033203, acc: 0.6328, auc: 0.6603, precision: 0.6328, recall: 0.6328\n",
      "2019-09-23T10:04:17.803179, step: 239, loss: 82.80805969238281, acc: 0.5938, auc: 0.6634, precision: 0.5841, recall: 0.5813\n",
      "2019-09-23T10:04:19.554665, step: 240, loss: 91.0606689453125, acc: 0.5312, auc: 0.5963, precision: 0.5573, recall: 0.536\n",
      "2019-09-23T10:04:21.319592, step: 241, loss: 98.41462707519531, acc: 0.4844, auc: 0.6574, precision: 0.7381, recall: 0.5147\n",
      "2019-09-23T10:04:23.057198, step: 242, loss: 80.96644592285156, acc: 0.6797, auc: 0.7442, precision: 0.7287, recall: 0.6243\n",
      "2019-09-23T10:04:24.806733, step: 243, loss: 83.01788330078125, acc: 0.6172, auc: 0.7598, precision: 0.6864, recall: 0.6172\n",
      "2019-09-23T10:04:26.609339, step: 244, loss: 84.26069641113281, acc: 0.6094, auc: 0.6709, precision: 0.6149, recall: 0.6129\n",
      "2019-09-23T10:04:28.394445, step: 245, loss: 84.46317291259766, acc: 0.6094, auc: 0.7, precision: 0.619, recall: 0.6144\n",
      "2019-09-23T10:04:30.157315, step: 246, loss: 83.86039733886719, acc: 0.625, auc: 0.7035, precision: 0.6929, recall: 0.5981\n",
      "2019-09-23T10:04:32.021616, step: 247, loss: 84.28019714355469, acc: 0.5938, auc: 0.663, precision: 0.5916, recall: 0.5912\n",
      "2019-09-23T10:04:33.893426, step: 248, loss: 82.95844268798828, acc: 0.6016, auc: 0.6996, precision: 0.6004, recall: 0.601\n",
      "2019-09-23T10:04:35.666789, step: 249, loss: 89.62498474121094, acc: 0.5469, auc: 0.672, precision: 0.5988, recall: 0.5409\n",
      "2019-09-23T10:04:37.453037, step: 250, loss: 81.46101379394531, acc: 0.6094, auc: 0.6708, precision: 0.6167, recall: 0.5283\n",
      "2019-09-23T10:04:39.227983, step: 251, loss: 90.28704833984375, acc: 0.5078, auc: 0.6634, precision: 0.5981, recall: 0.5593\n",
      "2019-09-23T10:04:41.076807, step: 252, loss: 91.38211059570312, acc: 0.4766, auc: 0.5908, precision: 0.5065, recall: 0.5034\n",
      "2019-09-23T10:04:42.853492, step: 253, loss: 85.97235107421875, acc: 0.5938, auc: 0.6601, precision: 0.599, recall: 0.5973\n",
      "2019-09-23T10:04:44.615435, step: 254, loss: 81.46742248535156, acc: 0.6406, auc: 0.7468, precision: 0.6518, recall: 0.6325\n",
      "2019-09-23T10:04:46.371508, step: 255, loss: 79.92414855957031, acc: 0.7266, auc: 0.7687, precision: 0.7312, recall: 0.7276\n",
      "2019-09-23T10:04:48.178761, step: 256, loss: 82.50486755371094, acc: 0.7031, auc: 0.7287, precision: 0.7001, recall: 0.7013\n",
      "2019-09-23T10:04:50.631924, step: 257, loss: 86.44076538085938, acc: 0.5703, auc: 0.6762, precision: 0.6192, recall: 0.5794\n",
      "2019-09-23T10:04:52.537248, step: 258, loss: 80.80632019042969, acc: 0.6328, auc: 0.7247, precision: 0.6806, recall: 0.5964\n",
      "2019-09-23T10:04:54.332140, step: 259, loss: 83.4823989868164, acc: 0.6562, auc: 0.6978, precision: 0.6554, recall: 0.6573\n",
      "2019-09-23T10:04:56.138079, step: 260, loss: 85.3390884399414, acc: 0.5859, auc: 0.6318, precision: 0.5798, recall: 0.5682\n",
      "2019-09-23T10:04:57.944026, step: 261, loss: 79.86555480957031, acc: 0.6641, auc: 0.7685, precision: 0.6812, recall: 0.6703\n",
      "2019-09-23T10:04:59.742554, step: 262, loss: 84.69354248046875, acc: 0.5625, auc: 0.677, precision: 0.6106, recall: 0.5291\n",
      "2019-09-23T10:05:02.195089, step: 263, loss: 85.25241088867188, acc: 0.6094, auc: 0.6653, precision: 0.6513, recall: 0.5953\n",
      "2019-09-23T10:05:04.022079, step: 264, loss: 82.04952239990234, acc: 0.6328, auc: 0.6988, precision: 0.6621, recall: 0.6115\n",
      "2019-09-23T10:05:05.823967, step: 265, loss: 82.59609985351562, acc: 0.6484, auc: 0.6741, precision: 0.6548, recall: 0.6484\n",
      "2019-09-23T10:05:07.619105, step: 266, loss: 74.54017639160156, acc: 0.7344, auc: 0.7975, precision: 0.7267, recall: 0.7208\n",
      "2019-09-23T10:05:09.436823, step: 267, loss: 81.88924407958984, acc: 0.6172, auc: 0.8016, precision: 0.7277, recall: 0.5998\n",
      "2019-09-23T10:05:11.269660, step: 268, loss: 112.36753845214844, acc: 0.4922, auc: 0.6623, precision: 0.496, recall: 0.4998\n",
      "2019-09-23T10:05:13.129745, step: 269, loss: 80.4202880859375, acc: 0.6562, auc: 0.7279, precision: 0.6803, recall: 0.6279\n",
      "2019-09-23T10:05:14.959242, step: 270, loss: 83.635986328125, acc: 0.6406, auc: 0.7331, precision: 0.6777, recall: 0.6605\n",
      "2019-09-23T10:05:16.827696, step: 271, loss: 85.62327575683594, acc: 0.6406, auc: 0.662, precision: 0.6376, recall: 0.6401\n",
      "2019-09-23T10:05:18.646585, step: 272, loss: 82.87022399902344, acc: 0.6016, auc: 0.6983, precision: 0.633, recall: 0.6113\n",
      "2019-09-23T10:05:20.491149, step: 273, loss: 80.06950378417969, acc: 0.6719, auc: 0.7688, precision: 0.6821, recall: 0.6916\n",
      "2019-09-23T10:05:22.295292, step: 274, loss: 80.21066284179688, acc: 0.6016, auc: 0.7658, precision: 0.6354, recall: 0.5878\n",
      "2019-09-23T10:05:24.097831, step: 275, loss: 82.39340209960938, acc: 0.6641, auc: 0.6863, precision: 0.6629, recall: 0.6603\n",
      "2019-09-23T10:05:25.922264, step: 276, loss: 79.65564727783203, acc: 0.7031, auc: 0.7474, precision: 0.7021, recall: 0.7061\n",
      "2019-09-23T10:05:27.746137, step: 277, loss: 85.62945556640625, acc: 0.5547, auc: 0.7155, precision: 0.6546, recall: 0.582\n",
      "2019-09-23T10:05:29.680140, step: 278, loss: 78.23202514648438, acc: 0.6562, auc: 0.7729, precision: 0.6736, recall: 0.6179\n",
      "2019-09-23T10:05:31.481137, step: 279, loss: 81.00845336914062, acc: 0.6562, auc: 0.6767, precision: 0.6448, recall: 0.643\n",
      "2019-09-23T10:05:33.330743, step: 280, loss: 81.9467544555664, acc: 0.6328, auc: 0.6697, precision: 0.633, recall: 0.5974\n",
      "2019-09-23T10:05:35.203414, step: 281, loss: 78.77983093261719, acc: 0.7109, auc: 0.759, precision: 0.7487, recall: 0.7168\n",
      "2019-09-23T10:05:37.011334, step: 282, loss: 71.96743774414062, acc: 0.7266, auc: 0.8283, precision: 0.776, recall: 0.7083\n",
      "2019-09-23T10:05:38.810778, step: 283, loss: 83.40284729003906, acc: 0.6484, auc: 0.7255, precision: 0.7517, recall: 0.6433\n",
      "2019-09-23T10:05:40.587796, step: 284, loss: 95.20761108398438, acc: 0.5625, auc: 0.7151, precision: 0.7, recall: 0.612\n",
      "2019-09-23T10:05:42.408083, step: 285, loss: 88.63143920898438, acc: 0.5391, auc: 0.7235, precision: 0.6411, recall: 0.5171\n",
      "2019-09-23T10:05:44.218028, step: 286, loss: 82.79708862304688, acc: 0.7031, auc: 0.7432, precision: 0.7303, recall: 0.7031\n",
      "2019-09-23T10:05:46.063764, step: 287, loss: 76.8173828125, acc: 0.7266, auc: 0.8122, precision: 0.741, recall: 0.738\n",
      "2019-09-23T10:05:47.922185, step: 288, loss: 76.97346496582031, acc: 0.6797, auc: 0.7832, precision: 0.6905, recall: 0.6814\n",
      "2019-09-23T10:05:50.016886, step: 289, loss: 76.0220947265625, acc: 0.6797, auc: 0.7708, precision: 0.6863, recall: 0.6762\n",
      "2019-09-23T10:05:52.203929, step: 290, loss: 75.32218933105469, acc: 0.7031, auc: 0.7485, precision: 0.6967, recall: 0.6857\n",
      "2019-09-23T10:05:54.142454, step: 291, loss: 75.51683044433594, acc: 0.7031, auc: 0.7693, precision: 0.7082, recall: 0.7031\n",
      "2019-09-23T10:05:56.239672, step: 292, loss: 75.36544799804688, acc: 0.6484, auc: 0.7538, precision: 0.6323, recall: 0.625\n",
      "2019-09-23T10:05:58.151010, step: 293, loss: 83.75448608398438, acc: 0.625, auc: 0.6953, precision: 0.6331, recall: 0.6304\n",
      "2019-09-23T10:06:00.684647, step: 294, loss: 88.65505981445312, acc: 0.5234, auc: 0.7845, precision: 0.7598, recall: 0.5081\n",
      "2019-09-23T10:06:02.624009, step: 295, loss: 93.49901580810547, acc: 0.5078, auc: 0.7761, precision: 0.6297, recall: 0.5511\n",
      "2019-09-23T10:06:04.471544, step: 296, loss: 78.86819458007812, acc: 0.6172, auc: 0.7993, precision: 0.7514, recall: 0.586\n",
      "2019-09-23T10:06:06.326208, step: 297, loss: 84.7228012084961, acc: 0.5859, auc: 0.6531, precision: 0.6014, recall: 0.5859\n",
      "2019-09-23T10:06:08.187137, step: 298, loss: 82.64079284667969, acc: 0.6484, auc: 0.6989, precision: 0.6534, recall: 0.6497\n",
      "2019-09-23T10:06:10.041324, step: 299, loss: 76.46175384521484, acc: 0.6953, auc: 0.7706, precision: 0.6902, recall: 0.6875\n",
      "2019-09-23T10:06:11.894744, step: 300, loss: 79.80870819091797, acc: 0.6797, auc: 0.7548, precision: 0.6922, recall: 0.6969\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:07:21.568424, step: 300, loss: 84.44594045785757, acc: 0.5819333333333334, auc: 0.7497128205128206, precision: 0.6881051282051281, recall: 0.5854333333333331\n",
      "2019-09-23T10:07:23.322136, step: 301, loss: 82.43244934082031, acc: 0.6328, auc: 0.7761, precision: 0.7844, recall: 0.6439\n",
      "2019-09-23T10:07:25.110171, step: 302, loss: 81.15319061279297, acc: 0.6406, auc: 0.7206, precision: 0.6751, recall: 0.6288\n",
      "2019-09-23T10:07:26.844383, step: 303, loss: 76.43310546875, acc: 0.6953, auc: 0.7706, precision: 0.7102, recall: 0.6818\n",
      "2019-09-23T10:07:28.583847, step: 304, loss: 74.63139343261719, acc: 0.6797, auc: 0.7775, precision: 0.6811, recall: 0.682\n",
      "2019-09-23T10:07:30.399741, step: 305, loss: 75.68020629882812, acc: 0.6641, auc: 0.758, precision: 0.6688, recall: 0.6586\n",
      "2019-09-23T10:07:32.137354, step: 306, loss: 84.03373718261719, acc: 0.5391, auc: 0.7751, precision: 0.6203, recall: 0.5967\n",
      "2019-09-23T10:07:33.886999, step: 307, loss: 115.52017211914062, acc: 0.5312, auc: 0.6221, precision: 0.2656, recall: 0.5\n",
      "2019-09-23T10:07:35.626292, step: 308, loss: 77.2934799194336, acc: 0.7109, auc: 0.7636, precision: 0.715, recall: 0.7136\n",
      "2019-09-23T10:07:37.432581, step: 309, loss: 81.08885192871094, acc: 0.7031, auc: 0.7643, precision: 0.705, recall: 0.7038\n",
      "2019-09-23T10:07:39.177226, step: 310, loss: 75.86463928222656, acc: 0.7109, auc: 0.7697, precision: 0.7123, recall: 0.7115\n",
      "2019-09-23T10:07:41.053924, step: 311, loss: 71.60124206542969, acc: 0.75, auc: 0.8256, precision: 0.7509, recall: 0.7495\n",
      "2019-09-23T10:07:42.874488, step: 312, loss: 75.55453491210938, acc: 0.7188, auc: 0.7563, precision: 0.7189, recall: 0.7189\n",
      "start to train models...\n",
      "2019-09-23T10:07:44.931828, step: 313, loss: 71.30635833740234, acc: 0.7109, auc: 0.8077, precision: 0.7146, recall: 0.7085\n",
      "2019-09-23T10:07:46.860953, step: 314, loss: 68.60932922363281, acc: 0.7266, auc: 0.8315, precision: 0.7256, recall: 0.7267\n",
      "2019-09-23T10:07:48.746540, step: 315, loss: 74.63346862792969, acc: 0.6953, auc: 0.7664, precision: 0.6888, recall: 0.6858\n",
      "2019-09-23T10:07:50.632698, step: 316, loss: 71.31489562988281, acc: 0.6797, auc: 0.8008, precision: 0.692, recall: 0.6752\n",
      "2019-09-23T10:07:52.735452, step: 317, loss: 84.89828491210938, acc: 0.625, auc: 0.7931, precision: 0.7557, recall: 0.5877\n",
      "2019-09-23T10:07:54.822011, step: 318, loss: 103.93211364746094, acc: 0.5469, auc: 0.6835, precision: 0.6114, recall: 0.5407\n",
      "2019-09-23T10:07:56.806423, step: 319, loss: 88.33305358886719, acc: 0.5781, auc: 0.7493, precision: 0.7318, recall: 0.5904\n",
      "2019-09-23T10:07:58.546629, step: 320, loss: 78.93404388427734, acc: 0.6484, auc: 0.7447, precision: 0.6579, recall: 0.6613\n",
      "2019-09-23T10:08:00.287168, step: 321, loss: 73.89761352539062, acc: 0.75, auc: 0.7959, precision: 0.7591, recall: 0.75\n",
      "2019-09-23T10:08:02.031753, step: 322, loss: 76.54004669189453, acc: 0.6719, auc: 0.7198, precision: 0.6644, recall: 0.6585\n",
      "2019-09-23T10:08:03.776986, step: 323, loss: 69.02458190917969, acc: 0.7812, auc: 0.8461, precision: 0.7918, recall: 0.7764\n",
      "2019-09-23T10:08:05.533112, step: 324, loss: 72.35357666015625, acc: 0.7578, auc: 0.7977, precision: 0.7629, recall: 0.7554\n",
      "2019-09-23T10:08:07.265215, step: 325, loss: 73.71240997314453, acc: 0.7031, auc: 0.786, precision: 0.7069, recall: 0.7079\n",
      "2019-09-23T10:08:09.035308, step: 326, loss: 80.98268127441406, acc: 0.625, auc: 0.7551, precision: 0.6591, recall: 0.6373\n",
      "2019-09-23T10:08:10.857326, step: 327, loss: 83.52195739746094, acc: 0.6172, auc: 0.7544, precision: 0.7254, recall: 0.5936\n",
      "2019-09-23T10:08:12.631416, step: 328, loss: 72.10466003417969, acc: 0.7109, auc: 0.804, precision: 0.7167, recall: 0.708\n",
      "2019-09-23T10:08:14.373980, step: 329, loss: 70.28913879394531, acc: 0.75, auc: 0.8243, precision: 0.7504, recall: 0.7509\n",
      "2019-09-23T10:08:16.101937, step: 330, loss: 71.72666931152344, acc: 0.7031, auc: 0.7897, precision: 0.699, recall: 0.7004\n",
      "2019-09-23T10:08:17.831974, step: 331, loss: 72.54521179199219, acc: 0.7109, auc: 0.8669, precision: 0.7745, recall: 0.745\n",
      "2019-09-23T10:08:19.601211, step: 332, loss: 126.11244201660156, acc: 0.4609, auc: 0.7267, precision: 0.556, recall: 0.5149\n",
      "2019-09-23T10:08:21.364709, step: 333, loss: 86.6085205078125, acc: 0.5938, auc: 0.7265, precision: 0.6963, recall: 0.5993\n",
      "2019-09-23T10:08:23.130279, step: 334, loss: 75.77667999267578, acc: 0.6953, auc: 0.7888, precision: 0.6997, recall: 0.6906\n",
      "2019-09-23T10:08:24.864587, step: 335, loss: 70.4923095703125, acc: 0.7344, auc: 0.8495, precision: 0.7398, recall: 0.7375\n",
      "2019-09-23T10:08:26.615728, step: 336, loss: 76.30290222167969, acc: 0.6641, auc: 0.7498, precision: 0.6695, recall: 0.6681\n",
      "2019-09-23T10:08:28.379523, step: 337, loss: 74.09503173828125, acc: 0.7422, auc: 0.7896, precision: 0.7556, recall: 0.7471\n",
      "2019-09-23T10:08:30.125491, step: 338, loss: 73.90693664550781, acc: 0.7031, auc: 0.7792, precision: 0.7033, recall: 0.7033\n",
      "2019-09-23T10:08:31.887995, step: 339, loss: 69.52688598632812, acc: 0.7812, auc: 0.836, precision: 0.7837, recall: 0.7796\n",
      "2019-09-23T10:08:33.640389, step: 340, loss: 60.41482162475586, acc: 0.8359, auc: 0.9022, precision: 0.857, recall: 0.8321\n",
      "2019-09-23T10:08:35.411956, step: 341, loss: 67.78158569335938, acc: 0.75, auc: 0.8263, precision: 0.7587, recall: 0.745\n",
      "2019-09-23T10:08:37.268103, step: 342, loss: 68.82901763916016, acc: 0.7422, auc: 0.826, precision: 0.7561, recall: 0.7383\n",
      "2019-09-23T10:08:39.044143, step: 343, loss: 85.28199768066406, acc: 0.6562, auc: 0.7597, precision: 0.7067, recall: 0.6901\n",
      "2019-09-23T10:08:40.769278, step: 344, loss: 118.89309692382812, acc: 0.5234, auc: 0.747, precision: 0.5121, recall: 0.5015\n",
      "2019-09-23T10:08:42.565417, step: 345, loss: 71.6298828125, acc: 0.7109, auc: 0.816, precision: 0.7336, recall: 0.6963\n",
      "2019-09-23T10:08:44.444231, step: 346, loss: 76.24059295654297, acc: 0.7109, auc: 0.7406, precision: 0.7136, recall: 0.7002\n",
      "2019-09-23T10:08:46.273601, step: 347, loss: 73.81645965576172, acc: 0.6953, auc: 0.7912, precision: 0.6987, recall: 0.7014\n",
      "2019-09-23T10:08:48.082057, step: 348, loss: 76.26412963867188, acc: 0.6875, auc: 0.7573, precision: 0.6889, recall: 0.6867\n",
      "2019-09-23T10:08:50.020734, step: 349, loss: 74.07164764404297, acc: 0.6953, auc: 0.7778, precision: 0.6993, recall: 0.6979\n",
      "2019-09-23T10:08:52.233403, step: 350, loss: 76.38526916503906, acc: 0.6562, auc: 0.7432, precision: 0.6562, recall: 0.6549\n",
      "2019-09-23T10:08:54.542885, step: 351, loss: 64.46028137207031, acc: 0.7891, auc: 0.856, precision: 0.7979, recall: 0.7891\n",
      "2019-09-23T10:08:56.405721, step: 352, loss: 69.45033264160156, acc: 0.7109, auc: 0.8163, precision: 0.7136, recall: 0.7002\n",
      "2019-09-23T10:08:58.334762, step: 353, loss: 65.37185668945312, acc: 0.7891, auc: 0.8468, precision: 0.7865, recall: 0.7874\n",
      "2019-09-23T10:09:00.058363, step: 354, loss: 61.87554931640625, acc: 0.7734, auc: 0.869, precision: 0.7802, recall: 0.7612\n",
      "2019-09-23T10:09:01.824405, step: 355, loss: 79.04965209960938, acc: 0.6797, auc: 0.8005, precision: 0.719, recall: 0.6943\n",
      "2019-09-23T10:09:03.600427, step: 356, loss: 103.1543197631836, acc: 0.5312, auc: 0.8745, precision: 0.7581, recall: 0.5312\n",
      "2019-09-23T10:09:05.337273, step: 357, loss: 79.43902587890625, acc: 0.6797, auc: 0.7647, precision: 0.7338, recall: 0.6718\n",
      "2019-09-23T10:09:07.066347, step: 358, loss: 68.4154281616211, acc: 0.7578, auc: 0.8326, precision: 0.7592, recall: 0.7564\n",
      "2019-09-23T10:09:08.843383, step: 359, loss: 66.24407958984375, acc: 0.7656, auc: 0.8306, precision: 0.7654, recall: 0.7654\n",
      "2019-09-23T10:09:10.661429, step: 360, loss: 65.44639587402344, acc: 0.7812, auc: 0.8467, precision: 0.7807, recall: 0.7824\n",
      "2019-09-23T10:09:12.420190, step: 361, loss: 73.1143798828125, acc: 0.7344, auc: 0.7753, precision: 0.7282, recall: 0.7302\n",
      "2019-09-23T10:09:14.166795, step: 362, loss: 68.72525024414062, acc: 0.7188, auc: 0.8353, precision: 0.7478, recall: 0.7239\n",
      "2019-09-23T10:09:15.899879, step: 363, loss: 62.65869903564453, acc: 0.7891, auc: 0.8443, precision: 0.8194, recall: 0.7635\n",
      "2019-09-23T10:09:17.631042, step: 364, loss: 69.54525756835938, acc: 0.7266, auc: 0.7888, precision: 0.7212, recall: 0.7222\n",
      "2019-09-23T10:09:19.429608, step: 365, loss: 68.47583770751953, acc: 0.7344, auc: 0.8355, precision: 0.7593, recall: 0.739\n",
      "2019-09-23T10:09:21.241056, step: 366, loss: 84.98329162597656, acc: 0.6484, auc: 0.72, precision: 0.6554, recall: 0.5976\n",
      "2019-09-23T10:09:23.068959, step: 367, loss: 70.86878204345703, acc: 0.7266, auc: 0.8175, precision: 0.7466, recall: 0.7132\n",
      "2019-09-23T10:09:24.807459, step: 368, loss: 70.97168731689453, acc: 0.7344, auc: 0.7974, precision: 0.7399, recall: 0.7317\n",
      "2019-09-23T10:09:26.573197, step: 369, loss: 73.87782287597656, acc: 0.7109, auc: 0.7736, precision: 0.7114, recall: 0.7112\n",
      "2019-09-23T10:09:28.310991, step: 370, loss: 66.62205505371094, acc: 0.7656, auc: 0.8407, precision: 0.765, recall: 0.7637\n",
      "2019-09-23T10:09:30.045596, step: 371, loss: 69.8886489868164, acc: 0.7031, auc: 0.8281, precision: 0.7251, recall: 0.7031\n",
      "2019-09-23T10:09:31.814634, step: 372, loss: 86.03273010253906, acc: 0.6016, auc: 0.7813, precision: 0.7226, recall: 0.6127\n",
      "2019-09-23T10:09:33.723825, step: 373, loss: 94.740234375, acc: 0.6172, auc: 0.7809, precision: 0.7906, recall: 0.5917\n",
      "2019-09-23T10:09:35.524393, step: 374, loss: 69.18363952636719, acc: 0.7109, auc: 0.8427, precision: 0.742, recall: 0.7081\n",
      "2019-09-23T10:09:37.259991, step: 375, loss: 64.26600646972656, acc: 0.75, auc: 0.864, precision: 0.7497, recall: 0.7502\n",
      "2019-09-23T10:09:39.074417, step: 376, loss: 74.84306335449219, acc: 0.7031, auc: 0.7702, precision: 0.7083, recall: 0.7043\n",
      "2019-09-23T10:09:40.974303, step: 377, loss: 77.53330993652344, acc: 0.6797, auc: 0.754, precision: 0.6838, recall: 0.6879\n",
      "2019-09-23T10:09:42.817560, step: 378, loss: 77.809814453125, acc: 0.6641, auc: 0.7708, precision: 0.7015, recall: 0.6485\n",
      "2019-09-23T10:09:44.589127, step: 379, loss: 76.19340515136719, acc: 0.7734, auc: 0.8, precision: 0.7739, recall: 0.7737\n",
      "2019-09-23T10:09:46.365594, step: 380, loss: 72.34580993652344, acc: 0.7266, auc: 0.8208, precision: 0.7473, recall: 0.7162\n",
      "2019-09-23T10:09:48.142335, step: 381, loss: 71.22234344482422, acc: 0.7266, auc: 0.8079, precision: 0.7351, recall: 0.7329\n",
      "2019-09-23T10:09:49.944612, step: 382, loss: 77.23191833496094, acc: 0.6875, auc: 0.7912, precision: 0.7398, recall: 0.701\n",
      "2019-09-23T10:09:51.754616, step: 383, loss: 78.78518676757812, acc: 0.6406, auc: 0.768, precision: 0.701, recall: 0.6266\n",
      "2019-09-23T10:09:53.694930, step: 384, loss: 70.12499237060547, acc: 0.7266, auc: 0.8517, precision: 0.7496, recall: 0.7431\n",
      "2019-09-23T10:09:55.752817, step: 385, loss: 83.16553497314453, acc: 0.625, auc: 0.736, precision: 0.6715, recall: 0.6392\n",
      "2019-09-23T10:09:57.989859, step: 386, loss: 66.33366394042969, acc: 0.75, auc: 0.8647, precision: 0.8019, recall: 0.7325\n",
      "2019-09-23T10:10:00.377464, step: 387, loss: 63.143699645996094, acc: 0.7656, auc: 0.8713, precision: 0.7642, recall: 0.7642\n",
      "2019-09-23T10:10:02.310229, step: 388, loss: 61.95703887939453, acc: 0.7969, auc: 0.8664, precision: 0.7993, recall: 0.7931\n",
      "2019-09-23T10:10:04.148631, step: 389, loss: 63.69054412841797, acc: 0.7656, auc: 0.8425, precision: 0.7679, recall: 0.7663\n",
      "2019-09-23T10:10:06.124000, step: 390, loss: 64.74861145019531, acc: 0.7422, auc: 0.8451, precision: 0.7465, recall: 0.7383\n",
      "2019-09-23T10:10:08.093043, step: 391, loss: 90.50843048095703, acc: 0.6328, auc: 0.7779, precision: 0.7385, recall: 0.6103\n",
      "2019-09-23T10:10:09.873442, step: 392, loss: 98.84578704833984, acc: 0.6094, auc: 0.8212, precision: 0.7899, recall: 0.5763\n",
      "2019-09-23T10:10:11.663971, step: 393, loss: 69.16705322265625, acc: 0.7344, auc: 0.8586, precision: 0.7724, recall: 0.7372\n",
      "2019-09-23T10:10:13.531747, step: 394, loss: 63.904056549072266, acc: 0.7656, auc: 0.8613, precision: 0.7656, recall: 0.7667\n",
      "2019-09-23T10:10:15.601036, step: 395, loss: 69.44745635986328, acc: 0.75, auc: 0.8033, precision: 0.7463, recall: 0.748\n",
      "2019-09-23T10:10:17.374527, step: 396, loss: 62.435791015625, acc: 0.75, auc: 0.8591, precision: 0.7535, recall: 0.7465\n",
      "2019-09-23T10:10:19.156131, step: 397, loss: 58.06233215332031, acc: 0.8203, auc: 0.8824, precision: 0.8184, recall: 0.8245\n",
      "2019-09-23T10:10:20.888032, step: 398, loss: 76.70050811767578, acc: 0.6484, auc: 0.759, precision: 0.6557, recall: 0.6346\n",
      "2019-09-23T10:10:22.648112, step: 399, loss: 60.12737274169922, acc: 0.7734, auc: 0.8813, precision: 0.7819, recall: 0.772\n",
      "2019-09-23T10:10:24.427663, step: 400, loss: 65.47842407226562, acc: 0.7578, auc: 0.8297, precision: 0.7575, recall: 0.7554\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:11:31.611871, step: 400, loss: 65.83605497311324, acc: 0.7492025641025641, auc: 0.8300076923076921, precision: 0.7508871794871796, recall: 0.7490102564102562\n",
      "2019-09-23T10:11:33.475885, step: 401, loss: 63.616004943847656, acc: 0.75, auc: 0.8479, precision: 0.754, recall: 0.75\n",
      "2019-09-23T10:11:35.418067, step: 402, loss: 67.85823059082031, acc: 0.7422, auc: 0.8608, precision: 0.7946, recall: 0.7422\n",
      "2019-09-23T10:11:37.285110, step: 403, loss: 90.19793701171875, acc: 0.6719, auc: 0.8539, precision: 0.7659, recall: 0.652\n",
      "2019-09-23T10:11:39.147613, step: 404, loss: 76.54972839355469, acc: 0.6875, auc: 0.7727, precision: 0.6946, recall: 0.6912\n",
      "2019-09-23T10:11:40.990532, step: 405, loss: 69.85671997070312, acc: 0.7109, auc: 0.8186, precision: 0.725, recall: 0.7221\n",
      "2019-09-23T10:11:42.877687, step: 406, loss: 73.88082885742188, acc: 0.6719, auc: 0.7741, precision: 0.6827, recall: 0.6698\n",
      "2019-09-23T10:11:44.738604, step: 407, loss: 64.49394226074219, acc: 0.7656, auc: 0.8527, precision: 0.7886, recall: 0.7502\n",
      "2019-09-23T10:11:46.494189, step: 408, loss: 64.29379272460938, acc: 0.7734, auc: 0.8395, precision: 0.7748, recall: 0.7711\n",
      "2019-09-23T10:11:48.237238, step: 409, loss: 59.8656120300293, acc: 0.7891, auc: 0.8829, precision: 0.8097, recall: 0.793\n",
      "2019-09-23T10:11:49.973742, step: 410, loss: 78.38923645019531, acc: 0.6641, auc: 0.8662, precision: 0.7783, recall: 0.6641\n",
      "2019-09-23T10:11:51.723313, step: 411, loss: 95.21910095214844, acc: 0.6172, auc: 0.8197, precision: 0.7941, recall: 0.5776\n",
      "2019-09-23T10:11:53.484826, step: 412, loss: 65.23384857177734, acc: 0.7422, auc: 0.844, precision: 0.7495, recall: 0.7435\n",
      "2019-09-23T10:11:55.273124, step: 413, loss: 69.23558044433594, acc: 0.6719, auc: 0.8066, precision: 0.6872, recall: 0.674\n",
      "2019-09-23T10:11:57.047166, step: 414, loss: 59.253814697265625, acc: 0.8125, auc: 0.887, precision: 0.8132, recall: 0.815\n",
      "2019-09-23T10:11:58.828529, step: 415, loss: 69.97134399414062, acc: 0.6953, auc: 0.8206, precision: 0.7177, recall: 0.7098\n",
      "2019-09-23T10:12:00.794086, step: 416, loss: 67.67127990722656, acc: 0.7891, auc: 0.8485, precision: 0.8081, recall: 0.7966\n",
      "2019-09-23T10:12:02.559739, step: 417, loss: 72.60443115234375, acc: 0.6797, auc: 0.8379, precision: 0.7257, recall: 0.6983\n",
      "2019-09-23T10:12:04.319263, step: 418, loss: 57.553768157958984, acc: 0.8203, auc: 0.9111, precision: 0.8417, recall: 0.8076\n",
      "2019-09-23T10:12:06.068344, step: 419, loss: 54.759132385253906, acc: 0.8125, auc: 0.8963, precision: 0.8138, recall: 0.8036\n",
      "2019-09-23T10:12:07.819416, step: 420, loss: 56.552406311035156, acc: 0.7891, auc: 0.8962, precision: 0.8062, recall: 0.7872\n",
      "2019-09-23T10:12:09.665901, step: 421, loss: 81.22262573242188, acc: 0.6797, auc: 0.7803, precision: 0.7049, recall: 0.699\n",
      "2019-09-23T10:12:11.430835, step: 422, loss: 99.5434799194336, acc: 0.5625, auc: 0.8831, precision: 0.7565, recall: 0.5942\n",
      "2019-09-23T10:12:13.234793, step: 423, loss: 75.20368957519531, acc: 0.7344, auc: 0.8008, precision: 0.7527, recall: 0.7438\n",
      "2019-09-23T10:12:14.954712, step: 424, loss: 62.554176330566406, acc: 0.8047, auc: 0.8872, precision: 0.8271, recall: 0.8067\n",
      "2019-09-23T10:12:16.705902, step: 425, loss: 61.5532112121582, acc: 0.8047, auc: 0.8637, precision: 0.8047, recall: 0.8066\n",
      "2019-09-23T10:12:18.464272, step: 426, loss: 64.60075378417969, acc: 0.7422, auc: 0.8284, precision: 0.731, recall: 0.7294\n",
      "2019-09-23T10:12:20.333875, step: 427, loss: 75.1463623046875, acc: 0.7422, auc: 0.7885, precision: 0.7451, recall: 0.743\n",
      "2019-09-23T10:12:22.091909, step: 428, loss: 67.9248046875, acc: 0.7266, auc: 0.8354, precision: 0.7602, recall: 0.7266\n",
      "2019-09-23T10:12:23.867437, step: 429, loss: 56.715118408203125, acc: 0.8125, auc: 0.8994, precision: 0.8203, recall: 0.8125\n",
      "2019-09-23T10:12:25.612742, step: 430, loss: 62.054054260253906, acc: 0.7891, auc: 0.8484, precision: 0.7979, recall: 0.7616\n",
      "2019-09-23T10:12:27.355391, step: 431, loss: 66.97825622558594, acc: 0.7734, auc: 0.8429, precision: 0.7726, recall: 0.773\n",
      "2019-09-23T10:12:29.109936, step: 432, loss: 64.2840576171875, acc: 0.7266, auc: 0.8327, precision: 0.7351, recall: 0.7329\n",
      "2019-09-23T10:12:30.854542, step: 433, loss: 61.19601821899414, acc: 0.7734, auc: 0.85, precision: 0.77, recall: 0.7688\n",
      "2019-09-23T10:12:32.726330, step: 434, loss: 67.5674057006836, acc: 0.6953, auc: 0.8213, precision: 0.704, recall: 0.6868\n",
      "2019-09-23T10:12:34.529320, step: 435, loss: 74.92157745361328, acc: 0.6953, auc: 0.8936, precision: 0.7934, recall: 0.6953\n",
      "2019-09-23T10:12:36.495395, step: 436, loss: 77.4010238647461, acc: 0.625, auc: 0.8765, precision: 0.7344, recall: 0.5499\n",
      "2019-09-23T10:12:38.269931, step: 437, loss: 65.16690826416016, acc: 0.7656, auc: 0.8916, precision: 0.8454, recall: 0.7541\n",
      "2019-09-23T10:12:40.032999, step: 438, loss: 65.480224609375, acc: 0.7891, auc: 0.8385, precision: 0.789, recall: 0.7886\n",
      "2019-09-23T10:12:41.823808, step: 439, loss: 52.071861267089844, acc: 0.8438, auc: 0.9166, precision: 0.8435, recall: 0.8441\n",
      "2019-09-23T10:12:43.571935, step: 440, loss: 54.981815338134766, acc: 0.8125, auc: 0.8958, precision: 0.8118, recall: 0.8127\n",
      "2019-09-23T10:12:45.331452, step: 441, loss: 67.18768310546875, acc: 0.7422, auc: 0.8215, precision: 0.7563, recall: 0.7422\n",
      "2019-09-23T10:12:47.196734, step: 442, loss: 59.823917388916016, acc: 0.8125, auc: 0.8604, precision: 0.8171, recall: 0.8033\n",
      "2019-09-23T10:12:49.027335, step: 443, loss: 64.78993225097656, acc: 0.7656, auc: 0.8441, precision: 0.782, recall: 0.771\n",
      "2019-09-23T10:12:50.992324, step: 444, loss: 62.60694122314453, acc: 0.8047, auc: 0.8595, precision: 0.8145, recall: 0.8018\n",
      "2019-09-23T10:12:52.882936, step: 445, loss: 64.76255798339844, acc: 0.7266, auc: 0.906, precision: 0.7929, recall: 0.7373\n",
      "2019-09-23T10:12:54.718364, step: 446, loss: 72.2025146484375, acc: 0.7266, auc: 0.8921, precision: 0.7857, recall: 0.723\n",
      "2019-09-23T10:12:56.532462, step: 447, loss: 78.28346252441406, acc: 0.6719, auc: 0.8043, precision: 0.7643, recall: 0.6808\n",
      "2019-09-23T10:12:58.337090, step: 448, loss: 68.94097900390625, acc: 0.7656, auc: 0.8253, precision: 0.7783, recall: 0.7688\n",
      "2019-09-23T10:13:00.193063, step: 449, loss: 68.44801330566406, acc: 0.7266, auc: 0.8079, precision: 0.7278, recall: 0.7275\n",
      "2019-09-23T10:13:01.938139, step: 450, loss: 59.757713317871094, acc: 0.7812, auc: 0.8633, precision: 0.775, recall: 0.7851\n",
      "2019-09-23T10:13:03.717108, step: 451, loss: 57.205169677734375, acc: 0.7812, auc: 0.8859, precision: 0.8, recall: 0.7619\n",
      "2019-09-23T10:13:05.483131, step: 452, loss: 65.32112121582031, acc: 0.7734, auc: 0.8644, precision: 0.8111, recall: 0.7814\n",
      "2019-09-23T10:13:07.322422, step: 453, loss: 74.72908782958984, acc: 0.7031, auc: 0.8301, precision: 0.7397, recall: 0.7242\n",
      "2019-09-23T10:13:09.073703, step: 454, loss: 80.02668762207031, acc: 0.6406, auc: 0.8687, precision: 0.7443, recall: 0.6685\n",
      "2019-09-23T10:13:10.851338, step: 455, loss: 78.52630615234375, acc: 0.625, auc: 0.8071, precision: 0.68, recall: 0.6468\n",
      "2019-09-23T10:13:12.673575, step: 456, loss: 63.48261260986328, acc: 0.7891, auc: 0.8734, precision: 0.8048, recall: 0.7925\n",
      "2019-09-23T10:13:14.507869, step: 457, loss: 60.97480773925781, acc: 0.7891, auc: 0.8814, precision: 0.7953, recall: 0.7945\n",
      "2019-09-23T10:13:16.255139, step: 458, loss: 53.2545166015625, acc: 0.8594, auc: 0.9157, precision: 0.8587, recall: 0.8598\n",
      "2019-09-23T10:13:18.007213, step: 459, loss: 54.3512077331543, acc: 0.7812, auc: 0.8946, precision: 0.7808, recall: 0.7794\n",
      "2019-09-23T10:13:19.780729, step: 460, loss: 70.57369232177734, acc: 0.7031, auc: 0.8306, precision: 0.7364, recall: 0.7031\n",
      "2019-09-23T10:13:21.525868, step: 461, loss: 72.51683044433594, acc: 0.6797, auc: 0.901, precision: 0.7507, recall: 0.7093\n",
      "2019-09-23T10:13:23.292399, step: 462, loss: 65.64334869384766, acc: 0.7422, auc: 0.8831, precision: 0.8267, recall: 0.6857\n",
      "2019-09-23T10:13:25.075125, step: 463, loss: 57.87604904174805, acc: 0.7734, auc: 0.8902, precision: 0.8011, recall: 0.7604\n",
      "2019-09-23T10:13:26.831272, step: 464, loss: 65.38616180419922, acc: 0.7578, auc: 0.833, precision: 0.763, recall: 0.7578\n",
      "2019-09-23T10:13:28.581322, step: 465, loss: 61.12126922607422, acc: 0.7969, auc: 0.8645, precision: 0.7971, recall: 0.7971\n",
      "2019-09-23T10:13:30.354320, step: 466, loss: 51.04288864135742, acc: 0.8359, auc: 0.9231, precision: 0.8323, recall: 0.8436\n",
      "2019-09-23T10:13:32.138865, step: 467, loss: 76.84596252441406, acc: 0.7031, auc: 0.8882, precision: 0.7885, recall: 0.7271\n",
      "2019-09-23T10:13:33.973669, step: 468, loss: 109.06474304199219, acc: 0.6406, auc: 0.8357, precision: 0.8, recall: 0.6102\n",
      "start to train models...\n",
      "2019-09-23T10:13:35.988604, step: 469, loss: 54.07344055175781, acc: 0.8516, auc: 0.9238, precision: 0.8541, recall: 0.8502\n",
      "2019-09-23T10:13:37.980942, step: 470, loss: 60.806156158447266, acc: 0.7578, auc: 0.8933, precision: 0.7688, recall: 0.7679\n",
      "2019-09-23T10:13:39.844421, step: 471, loss: 53.65071105957031, acc: 0.8281, auc: 0.9116, precision: 0.8265, recall: 0.8281\n",
      "2019-09-23T10:13:41.681796, step: 472, loss: 57.26282501220703, acc: 0.7969, auc: 0.876, precision: 0.7944, recall: 0.7944\n",
      "2019-09-23T10:13:43.564186, step: 473, loss: 56.76244354248047, acc: 0.8125, auc: 0.8815, precision: 0.825, recall: 0.802\n",
      "2019-09-23T10:13:45.407580, step: 474, loss: 53.8023681640625, acc: 0.7578, auc: 0.8901, precision: 0.7579, recall: 0.7578\n",
      "2019-09-23T10:13:47.268435, step: 475, loss: 60.01791763305664, acc: 0.7734, auc: 0.8647, precision: 0.7751, recall: 0.7728\n",
      "2019-09-23T10:13:49.224112, step: 476, loss: 49.29649353027344, acc: 0.8203, auc: 0.9189, precision: 0.8204, recall: 0.8201\n",
      "2019-09-23T10:13:51.088902, step: 477, loss: 52.20710754394531, acc: 0.8281, auc: 0.9061, precision: 0.8281, recall: 0.8401\n",
      "2019-09-23T10:13:52.957390, step: 478, loss: 73.50740051269531, acc: 0.7266, auc: 0.8821, precision: 0.7763, recall: 0.7161\n",
      "2019-09-23T10:13:54.803320, step: 479, loss: 99.893310546875, acc: 0.5938, auc: 0.8248, precision: 0.7172, recall: 0.6051\n",
      "2019-09-23T10:13:56.648328, step: 480, loss: 53.75830078125, acc: 0.8281, auc: 0.9176, precision: 0.8284, recall: 0.8275\n",
      "2019-09-23T10:13:58.488819, step: 481, loss: 56.00340270996094, acc: 0.8203, auc: 0.8879, precision: 0.8204, recall: 0.8203\n",
      "2019-09-23T10:14:00.398076, step: 482, loss: 49.76207733154297, acc: 0.8281, auc: 0.9196, precision: 0.8314, recall: 0.8314\n",
      "2019-09-23T10:14:02.305486, step: 483, loss: 54.01356506347656, acc: 0.8125, auc: 0.9015, precision: 0.8182, recall: 0.8167\n",
      "2019-09-23T10:14:04.224247, step: 484, loss: 47.45768737792969, acc: 0.8594, auc: 0.9217, precision: 0.8613, recall: 0.8613\n",
      "2019-09-23T10:14:05.997243, step: 485, loss: 58.2236328125, acc: 0.7344, auc: 0.8659, precision: 0.739, recall: 0.7284\n",
      "2019-09-23T10:14:07.805171, step: 486, loss: 51.88949203491211, acc: 0.7969, auc: 0.904, precision: 0.799, recall: 0.7919\n",
      "2019-09-23T10:14:09.631274, step: 487, loss: 56.50872802734375, acc: 0.8047, auc: 0.9106, precision: 0.8165, recall: 0.8153\n",
      "2019-09-23T10:14:11.529521, step: 488, loss: 110.45510864257812, acc: 0.6094, auc: 0.8396, precision: 0.7472, recall: 0.6365\n",
      "2019-09-23T10:14:13.445496, step: 489, loss: 86.23014831542969, acc: 0.625, auc: 0.8738, precision: 0.7778, recall: 0.6471\n",
      "2019-09-23T10:14:15.289505, step: 490, loss: 61.590599060058594, acc: 0.7656, auc: 0.8611, precision: 0.7571, recall: 0.7571\n",
      "2019-09-23T10:14:17.142307, step: 491, loss: 66.41710662841797, acc: 0.75, auc: 0.8154, precision: 0.75, recall: 0.75\n",
      "2019-09-23T10:14:19.072540, step: 492, loss: 51.786407470703125, acc: 0.8203, auc: 0.9189, precision: 0.8218, recall: 0.8214\n",
      "2019-09-23T10:14:20.962228, step: 493, loss: 57.37836456298828, acc: 0.7734, auc: 0.8804, precision: 0.7721, recall: 0.7727\n",
      "2019-09-23T10:14:22.748689, step: 494, loss: 50.46299743652344, acc: 0.8125, auc: 0.9121, precision: 0.8174, recall: 0.8049\n",
      "2019-09-23T10:14:24.538661, step: 495, loss: 55.693878173828125, acc: 0.7734, auc: 0.8825, precision: 0.7767, recall: 0.7715\n",
      "2019-09-23T10:14:26.351722, step: 496, loss: 45.06903839111328, acc: 0.8672, auc: 0.9406, precision: 0.8682, recall: 0.8663\n",
      "2019-09-23T10:14:28.143546, step: 497, loss: 51.804664611816406, acc: 0.8438, auc: 0.8998, precision: 0.8431, recall: 0.8431\n",
      "2019-09-23T10:14:29.962959, step: 498, loss: 46.83805465698242, acc: 0.8203, auc: 0.9153, precision: 0.8209, recall: 0.8188\n",
      "2019-09-23T10:14:31.803020, step: 499, loss: 53.426513671875, acc: 0.7734, auc: 0.893, precision: 0.7812, recall: 0.7759\n",
      "2019-09-23T10:14:33.636402, step: 500, loss: 50.648406982421875, acc: 0.8047, auc: 0.9285, precision: 0.8234, recall: 0.8139\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:15:50.661603, step: 500, loss: 115.5500007042518, acc: 0.6614564102564101, auc: 0.8722025641025641, precision: 0.7788256410256409, recall: 0.6574230769230769\n",
      "2019-09-23T10:15:52.864885, step: 501, loss: 108.58827209472656, acc: 0.6562, auc: 0.8926, precision: 0.7764, recall: 0.6401\n",
      "2019-09-23T10:15:54.982009, step: 502, loss: 93.84429931640625, acc: 0.5938, auc: 0.8999, precision: 0.7406, recall: 0.5938\n",
      "2019-09-23T10:15:57.117098, step: 503, loss: 56.71147537231445, acc: 0.8281, auc: 0.898, precision: 0.834, recall: 0.8324\n",
      "2019-09-23T10:15:59.230960, step: 504, loss: 50.981590270996094, acc: 0.8594, auc: 0.9263, precision: 0.861, recall: 0.8589\n",
      "2019-09-23T10:16:01.200521, step: 505, loss: 56.00407028198242, acc: 0.8203, auc: 0.8809, precision: 0.8268, recall: 0.8203\n",
      "2019-09-23T10:16:03.200030, step: 506, loss: 57.99108123779297, acc: 0.7891, auc: 0.8684, precision: 0.7897, recall: 0.7886\n",
      "2019-09-23T10:16:05.183677, step: 507, loss: 56.0947265625, acc: 0.7969, auc: 0.8794, precision: 0.8045, recall: 0.7956\n",
      "2019-09-23T10:16:07.187219, step: 508, loss: 58.16480255126953, acc: 0.8047, auc: 0.8622, precision: 0.8061, recall: 0.8004\n",
      "2019-09-23T10:16:09.123634, step: 509, loss: 43.605873107910156, acc: 0.8672, auc: 0.9511, precision: 0.8707, recall: 0.8688\n",
      "2019-09-23T10:16:10.869429, step: 510, loss: 47.11642074584961, acc: 0.8594, auc: 0.9214, precision: 0.862, recall: 0.8634\n",
      "2019-09-23T10:16:12.629732, step: 511, loss: 58.193050384521484, acc: 0.8047, auc: 0.887, precision: 0.8096, recall: 0.8076\n",
      "2019-09-23T10:16:14.449852, step: 512, loss: 62.44014358520508, acc: 0.7891, auc: 0.9106, precision: 0.8323, recall: 0.8057\n",
      "2019-09-23T10:16:16.619073, step: 513, loss: 74.67861938476562, acc: 0.7109, auc: 0.8999, precision: 0.7873, recall: 0.7109\n",
      "2019-09-23T10:16:18.838114, step: 514, loss: 80.2395248413086, acc: 0.6641, auc: 0.8353, precision: 0.7344, recall: 0.6762\n",
      "2019-09-23T10:16:20.876660, step: 515, loss: 54.79933166503906, acc: 0.8047, auc: 0.8922, precision: 0.8034, recall: 0.8041\n",
      "2019-09-23T10:16:22.733694, step: 516, loss: 53.60179138183594, acc: 0.8047, auc: 0.9045, precision: 0.8064, recall: 0.8053\n",
      "2019-09-23T10:16:24.611670, step: 517, loss: 52.749568939208984, acc: 0.7969, auc: 0.9044, precision: 0.7951, recall: 0.7951\n",
      "2019-09-23T10:16:26.715043, step: 518, loss: 45.539451599121094, acc: 0.8594, auc: 0.9222, precision: 0.8595, recall: 0.8509\n",
      "2019-09-23T10:16:28.587036, step: 519, loss: 50.840057373046875, acc: 0.7891, auc: 0.9137, precision: 0.8034, recall: 0.7956\n",
      "2019-09-23T10:16:30.416143, step: 520, loss: 55.776710510253906, acc: 0.7578, auc: 0.9227, precision: 0.7812, recall: 0.7643\n",
      "2019-09-23T10:16:32.173443, step: 521, loss: 70.36245727539062, acc: 0.7109, auc: 0.9001, precision: 0.7654, recall: 0.7109\n",
      "2019-09-23T10:16:33.931740, step: 522, loss: 50.27688217163086, acc: 0.7969, auc: 0.916, precision: 0.8128, recall: 0.7847\n",
      "2019-09-23T10:16:35.779797, step: 523, loss: 51.35283660888672, acc: 0.7969, auc: 0.8955, precision: 0.7944, recall: 0.7944\n",
      "2019-09-23T10:16:37.608904, step: 524, loss: 56.241111755371094, acc: 0.7812, auc: 0.8794, precision: 0.7838, recall: 0.7805\n",
      "2019-09-23T10:16:39.361216, step: 525, loss: 52.28478240966797, acc: 0.8203, auc: 0.893, precision: 0.8185, recall: 0.8173\n",
      "2019-09-23T10:16:41.092586, step: 526, loss: 54.811424255371094, acc: 0.7969, auc: 0.898, precision: 0.8075, recall: 0.8054\n",
      "2019-09-23T10:16:42.883807, step: 527, loss: 61.12945556640625, acc: 0.7891, auc: 0.8877, precision: 0.819, recall: 0.7789\n",
      "2019-09-23T10:16:44.667024, step: 528, loss: 62.603302001953125, acc: 0.7109, auc: 0.9081, precision: 0.7756, recall: 0.6806\n",
      "2019-09-23T10:16:46.509097, step: 529, loss: 59.93709945678711, acc: 0.7812, auc: 0.9031, precision: 0.8161, recall: 0.7888\n",
      "2019-09-23T10:16:48.257419, step: 530, loss: 68.89686584472656, acc: 0.7031, auc: 0.8817, precision: 0.7812, recall: 0.7111\n",
      "2019-09-23T10:16:50.071567, step: 531, loss: 54.0518913269043, acc: 0.8047, auc: 0.9252, precision: 0.8244, recall: 0.8123\n",
      "2019-09-23T10:16:51.824877, step: 532, loss: 55.410377502441406, acc: 0.8047, auc: 0.8922, precision: 0.8146, recall: 0.7985\n",
      "2019-09-23T10:16:53.657978, step: 533, loss: 56.89165115356445, acc: 0.7969, auc: 0.876, precision: 0.7935, recall: 0.8018\n",
      "2019-09-23T10:16:55.524980, step: 534, loss: 61.17820739746094, acc: 0.75, auc: 0.8655, precision: 0.7626, recall: 0.75\n",
      "2019-09-23T10:16:57.288267, step: 535, loss: 63.9472770690918, acc: 0.75, auc: 0.8703, precision: 0.7773, recall: 0.7475\n",
      "2019-09-23T10:16:59.179205, step: 536, loss: 55.356048583984375, acc: 0.8047, auc: 0.9088, precision: 0.8234, recall: 0.8008\n",
      "2019-09-23T10:17:01.069150, step: 537, loss: 66.21350860595703, acc: 0.7578, auc: 0.8865, precision: 0.817, recall: 0.7741\n",
      "2019-09-23T10:17:02.846399, step: 538, loss: 61.51890563964844, acc: 0.7891, auc: 0.9234, precision: 0.8306, recall: 0.8081\n",
      "2019-09-23T10:17:05.148239, step: 539, loss: 61.905540466308594, acc: 0.7578, auc: 0.8938, precision: 0.805, recall: 0.7547\n",
      "2019-09-23T10:17:07.046162, step: 540, loss: 52.60749053955078, acc: 0.8203, auc: 0.9059, precision: 0.8203, recall: 0.8268\n",
      "2019-09-23T10:17:08.942090, step: 541, loss: 58.456275939941406, acc: 0.7812, auc: 0.9026, precision: 0.8093, recall: 0.7881\n",
      "2019-09-23T10:17:10.858963, step: 542, loss: 55.16630554199219, acc: 0.7812, auc: 0.9189, precision: 0.7976, recall: 0.7882\n",
      "2019-09-23T10:17:12.813734, step: 543, loss: 67.03030395507812, acc: 0.75, auc: 0.8496, precision: 0.7591, recall: 0.75\n",
      "2019-09-23T10:17:14.821367, step: 544, loss: 48.638938903808594, acc: 0.8594, auc: 0.9225, precision: 0.8618, recall: 0.8607\n",
      "2019-09-23T10:17:16.889831, step: 545, loss: 60.470001220703125, acc: 0.8047, auc: 0.8755, precision: 0.8165, recall: 0.8153\n",
      "2019-09-23T10:17:18.964281, step: 546, loss: 60.63304138183594, acc: 0.7578, auc: 0.8784, precision: 0.7748, recall: 0.7668\n",
      "2019-09-23T10:17:21.043720, step: 547, loss: 56.467857360839844, acc: 0.7891, auc: 0.8992, precision: 0.8122, recall: 0.7824\n",
      "2019-09-23T10:17:23.069302, step: 548, loss: 44.955970764160156, acc: 0.8438, auc: 0.93, precision: 0.8473, recall: 0.8391\n",
      "2019-09-23T10:17:25.176676, step: 549, loss: 52.05918502807617, acc: 0.7969, auc: 0.9089, precision: 0.8113, recall: 0.7985\n",
      "2019-09-23T10:17:27.263086, step: 550, loss: 68.22330474853516, acc: 0.7578, auc: 0.8906, precision: 0.8015, recall: 0.7665\n",
      "2019-09-23T10:17:29.451231, step: 551, loss: 80.65513610839844, acc: 0.6484, auc: 0.9381, precision: 0.7837, recall: 0.6739\n",
      "2019-09-23T10:17:31.600482, step: 552, loss: 62.78626251220703, acc: 0.7891, auc: 0.8767, precision: 0.7911, recall: 0.7943\n",
      "2019-09-23T10:17:33.808589, step: 553, loss: 51.15235900878906, acc: 0.8203, auc: 0.9199, precision: 0.8218, recall: 0.8235\n",
      "2019-09-23T10:17:35.986749, step: 554, loss: 52.72438049316406, acc: 0.8047, auc: 0.9159, precision: 0.809, recall: 0.8083\n",
      "2019-09-23T10:17:38.042251, step: 555, loss: 63.91682052612305, acc: 0.7344, auc: 0.8736, precision: 0.7641, recall: 0.7578\n",
      "2019-09-23T10:17:40.543560, step: 556, loss: 67.34516906738281, acc: 0.7656, auc: 0.8822, precision: 0.7933, recall: 0.7837\n",
      "2019-09-23T10:17:42.745679, step: 557, loss: 51.618812561035156, acc: 0.8047, auc: 0.9245, precision: 0.881, recall: 0.7396\n",
      "2019-09-23T10:17:44.803173, step: 558, loss: 49.53641128540039, acc: 0.8516, auc: 0.915, precision: 0.8575, recall: 0.8536\n",
      "2019-09-23T10:17:47.026220, step: 559, loss: 46.40229034423828, acc: 0.8047, auc: 0.915, precision: 0.7974, recall: 0.7991\n",
      "2019-09-23T10:17:49.272213, step: 560, loss: 55.56492614746094, acc: 0.7734, auc: 0.887, precision: 0.776, recall: 0.7755\n",
      "2019-09-23T10:17:51.846328, step: 561, loss: 51.245967864990234, acc: 0.8281, auc: 0.9097, precision: 0.8446, recall: 0.8281\n",
      "2019-09-23T10:17:54.464327, step: 562, loss: 56.223358154296875, acc: 0.8281, auc: 0.9251, precision: 0.8569, recall: 0.8394\n",
      "2019-09-23T10:17:56.610583, step: 563, loss: 63.42694091796875, acc: 0.7734, auc: 0.9187, precision: 0.8395, recall: 0.7515\n",
      "2019-09-23T10:17:58.643147, step: 564, loss: 60.4207763671875, acc: 0.75, auc: 0.8794, precision: 0.7767, recall: 0.7524\n",
      "2019-09-23T10:18:00.844259, step: 565, loss: 55.7834358215332, acc: 0.7969, auc: 0.8807, precision: 0.801, recall: 0.7986\n",
      "2019-09-23T10:18:02.994508, step: 566, loss: 51.69622802734375, acc: 0.7812, auc: 0.8978, precision: 0.7812, recall: 0.7815\n",
      "2019-09-23T10:18:05.173677, step: 567, loss: 54.14585876464844, acc: 0.8359, auc: 0.8811, precision: 0.8352, recall: 0.8358\n",
      "2019-09-23T10:18:07.591211, step: 568, loss: 46.19239807128906, acc: 0.7734, auc: 0.9362, precision: 0.7857, recall: 0.7825\n",
      "2019-09-23T10:18:09.803294, step: 569, loss: 55.66022491455078, acc: 0.7734, auc: 0.8961, precision: 0.7885, recall: 0.7769\n",
      "2019-09-23T10:18:11.853808, step: 570, loss: 69.92689514160156, acc: 0.7656, auc: 0.9029, precision: 0.8142, recall: 0.7923\n",
      "2019-09-23T10:18:13.971144, step: 571, loss: 81.54118347167969, acc: 0.6094, auc: 0.9011, precision: 0.711, recall: 0.6247\n",
      "2019-09-23T10:18:16.074518, step: 572, loss: 60.159889221191406, acc: 0.7812, auc: 0.8852, precision: 0.7875, recall: 0.7719\n",
      "2019-09-23T10:18:18.098106, step: 573, loss: 60.127723693847656, acc: 0.7734, auc: 0.8667, precision: 0.7893, recall: 0.7734\n",
      "2019-09-23T10:18:20.124684, step: 574, loss: 47.168251037597656, acc: 0.8516, auc: 0.9448, precision: 0.8524, recall: 0.8554\n",
      "2019-09-23T10:18:22.128327, step: 575, loss: 48.82830810546875, acc: 0.8516, auc: 0.9135, precision: 0.8541, recall: 0.8502\n",
      "2019-09-23T10:18:24.123987, step: 576, loss: 55.944637298583984, acc: 0.8047, auc: 0.8872, precision: 0.8084, recall: 0.8047\n",
      "2019-09-23T10:18:26.157548, step: 577, loss: 46.28276824951172, acc: 0.8281, auc: 0.9255, precision: 0.8359, recall: 0.8293\n",
      "2019-09-23T10:18:28.162186, step: 578, loss: 48.66963195800781, acc: 0.8281, auc: 0.9262, precision: 0.8275, recall: 0.8284\n",
      "2019-09-23T10:18:30.201730, step: 579, loss: 39.846927642822266, acc: 0.8594, auc: 0.9419, precision: 0.8634, recall: 0.8537\n",
      "2019-09-23T10:18:32.329040, step: 580, loss: 51.22709655761719, acc: 0.7969, auc: 0.9047, precision: 0.7977, recall: 0.7977\n",
      "2019-09-23T10:18:34.640855, step: 581, loss: 62.963783264160156, acc: 0.7578, auc: 0.8928, precision: 0.778, recall: 0.7795\n",
      "2019-09-23T10:18:36.647489, step: 582, loss: 179.27780151367188, acc: 0.4844, auc: 0.8456, precision: 0.7317, recall: 0.5352\n",
      "2019-09-23T10:18:38.720941, step: 583, loss: 58.67441940307617, acc: 0.7734, auc: 0.908, precision: 0.8036, recall: 0.7196\n",
      "2019-09-23T10:18:40.889151, step: 584, loss: 67.8154296875, acc: 0.7656, auc: 0.8586, precision: 0.8209, recall: 0.7556\n",
      "2019-09-23T10:18:43.021439, step: 585, loss: 68.37762451171875, acc: 0.7188, auc: 0.9024, precision: 0.7679, recall: 0.7517\n",
      "2019-09-23T10:18:45.218561, step: 586, loss: 51.458702087402344, acc: 0.875, auc: 0.9267, precision: 0.8767, recall: 0.8745\n",
      "2019-09-23T10:18:47.272069, step: 587, loss: 49.481468200683594, acc: 0.8359, auc: 0.9194, precision: 0.8372, recall: 0.8284\n",
      "2019-09-23T10:18:49.365469, step: 588, loss: 57.2808837890625, acc: 0.7734, auc: 0.8708, precision: 0.7734, recall: 0.7735\n",
      "2019-09-23T10:18:51.607471, step: 589, loss: 47.34950256347656, acc: 0.8594, auc: 0.9317, precision: 0.8583, recall: 0.8597\n",
      "2019-09-23T10:18:53.831522, step: 590, loss: 49.12334060668945, acc: 0.8672, auc: 0.9189, precision: 0.8716, recall: 0.8672\n",
      "2019-09-23T10:18:55.989752, step: 591, loss: 48.71071243286133, acc: 0.8359, auc: 0.9103, precision: 0.836, recall: 0.8323\n",
      "2019-09-23T10:18:58.042259, step: 592, loss: 52.071266174316406, acc: 0.8359, auc: 0.8942, precision: 0.8359, recall: 0.8355\n",
      "2019-09-23T10:19:00.039915, step: 593, loss: 49.294593811035156, acc: 0.8438, auc: 0.917, precision: 0.8452, recall: 0.8432\n",
      "2019-09-23T10:19:02.088436, step: 594, loss: 47.73501205444336, acc: 0.8438, auc: 0.9201, precision: 0.8574, recall: 0.8407\n",
      "2019-09-23T10:19:04.287554, step: 595, loss: 45.792720794677734, acc: 0.8438, auc: 0.9197, precision: 0.8434, recall: 0.8434\n",
      "2019-09-23T10:19:06.452771, step: 596, loss: 52.261131286621094, acc: 0.7969, auc: 0.8979, precision: 0.8017, recall: 0.7958\n",
      "2019-09-23T10:19:08.482332, step: 597, loss: 49.699737548828125, acc: 0.8203, auc: 0.9266, precision: 0.8469, recall: 0.8137\n",
      "2019-09-23T10:19:10.532849, step: 598, loss: 45.73536682128906, acc: 0.8203, auc: 0.9614, precision: 0.8523, recall: 0.8179\n",
      "2019-09-23T10:19:12.621261, step: 599, loss: 76.59391021728516, acc: 0.7344, auc: 0.8473, precision: 0.7864, recall: 0.7471\n",
      "2019-09-23T10:19:14.667787, step: 600, loss: 63.245216369628906, acc: 0.75, auc: 0.9131, precision: 0.8101, recall: 0.7465\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:20:40.623895, step: 600, loss: 55.532690586187904, acc: 0.8094948717948718, auc: 0.8901333333333334, precision: 0.8093051282051282, recall: 0.8092435897435898\n",
      "2019-09-23T10:20:42.934683, step: 601, loss: 46.6016845703125, acc: 0.8906, auc: 0.9385, precision: 0.893, recall: 0.8988\n",
      "2019-09-23T10:20:45.236526, step: 602, loss: 55.775413513183594, acc: 0.7891, auc: 0.8908, precision: 0.7896, recall: 0.7894\n",
      "2019-09-23T10:20:47.459579, step: 603, loss: 50.74610900878906, acc: 0.8125, auc: 0.9158, precision: 0.8222, recall: 0.8165\n",
      "2019-09-23T10:20:49.570931, step: 604, loss: 51.84389877319336, acc: 0.8125, auc: 0.8979, precision: 0.8132, recall: 0.802\n",
      "2019-09-23T10:20:51.784012, step: 605, loss: 61.39472961425781, acc: 0.7578, auc: 0.8719, precision: 0.7696, recall: 0.773\n",
      "2019-09-23T10:20:54.100816, step: 606, loss: 61.20152282714844, acc: 0.7578, auc: 0.9376, precision: 0.8315, recall: 0.7687\n",
      "2019-09-23T10:20:56.217153, step: 607, loss: 46.6156005859375, acc: 0.8438, auc: 0.9436, precision: 0.8532, recall: 0.849\n",
      "2019-09-23T10:20:58.342468, step: 608, loss: 52.65130615234375, acc: 0.8047, auc: 0.8959, precision: 0.8053, recall: 0.8038\n",
      "2019-09-23T10:21:00.649297, step: 609, loss: 41.04377746582031, acc: 0.8672, auc: 0.9344, precision: 0.8672, recall: 0.8665\n",
      "2019-09-23T10:21:03.359056, step: 610, loss: 44.022132873535156, acc: 0.8516, auc: 0.9242, precision: 0.8514, recall: 0.8517\n",
      "2019-09-23T10:21:05.456447, step: 611, loss: 49.10314178466797, acc: 0.8359, auc: 0.9064, precision: 0.8319, recall: 0.8381\n",
      "2019-09-23T10:21:07.322447, step: 612, loss: 77.0552749633789, acc: 0.7656, auc: 0.8999, precision: 0.8515, recall: 0.7368\n",
      "2019-09-23T10:21:09.293176, step: 613, loss: 93.34074401855469, acc: 0.6328, auc: 0.8534, precision: 0.7422, recall: 0.643\n",
      "2019-09-23T10:21:11.149212, step: 614, loss: 67.6566162109375, acc: 0.7344, auc: 0.8372, precision: 0.756, recall: 0.7485\n",
      "2019-09-23T10:21:13.052122, step: 615, loss: 63.80366897583008, acc: 0.7422, auc: 0.8269, precision: 0.7402, recall: 0.7448\n",
      "2019-09-23T10:21:15.046786, step: 616, loss: 49.51642990112305, acc: 0.8672, auc: 0.9379, precision: 0.8707, recall: 0.8688\n",
      "2019-09-23T10:21:16.885866, step: 617, loss: 59.75409698486328, acc: 0.7969, auc: 0.8654, precision: 0.8017, recall: 0.7958\n",
      "2019-09-23T10:21:18.757860, step: 618, loss: 61.318092346191406, acc: 0.7656, auc: 0.8429, precision: 0.7657, recall: 0.7649\n",
      "2019-09-23T10:21:20.562033, step: 619, loss: 54.245758056640625, acc: 0.8438, auc: 0.9107, precision: 0.8435, recall: 0.8441\n",
      "2019-09-23T10:21:22.473918, step: 620, loss: 47.09534454345703, acc: 0.8359, auc: 0.9197, precision: 0.8411, recall: 0.8389\n",
      "2019-09-23T10:21:24.427692, step: 621, loss: 48.448089599609375, acc: 0.8516, auc: 0.9312, precision: 0.8641, recall: 0.856\n",
      "2019-09-23T10:21:26.338582, step: 622, loss: 67.83431243896484, acc: 0.7734, auc: 0.9178, precision: 0.8051, recall: 0.8031\n",
      "2019-09-23T10:21:28.196611, step: 623, loss: 56.992431640625, acc: 0.7734, auc: 0.9342, precision: 0.8395, recall: 0.7515\n",
      "2019-09-23T10:21:29.992806, step: 624, loss: 59.637046813964844, acc: 0.8359, auc: 0.866, precision: 0.8379, recall: 0.8308\n",
      "start to train models...\n",
      "2019-09-23T10:21:40.299174, step: 625, loss: 40.2463264465332, acc: 0.9297, auc: 0.9677, precision: 0.9294, recall: 0.9299\n",
      "2019-09-23T10:21:42.222031, step: 626, loss: 55.029197692871094, acc: 0.7891, auc: 0.8789, precision: 0.7909, recall: 0.7884\n",
      "2019-09-23T10:21:44.039170, step: 627, loss: 45.770423889160156, acc: 0.875, auc: 0.9264, precision: 0.873, recall: 0.8752\n",
      "2019-09-23T10:21:45.860299, step: 628, loss: 40.46311569213867, acc: 0.8672, auc: 0.9558, precision: 0.8768, recall: 0.8697\n",
      "2019-09-23T10:21:47.816067, step: 629, loss: 50.627471923828125, acc: 0.8516, auc: 0.9097, precision: 0.871, recall: 0.8363\n",
      "2019-09-23T10:21:49.650161, step: 630, loss: 44.31117248535156, acc: 0.8516, auc: 0.9249, precision: 0.8525, recall: 0.8501\n",
      "2019-09-23T10:21:51.456330, step: 631, loss: 48.53707504272461, acc: 0.8672, auc: 0.9164, precision: 0.8724, recall: 0.8653\n",
      "2019-09-23T10:21:53.262499, step: 632, loss: 53.611061096191406, acc: 0.8203, auc: 0.9033, precision: 0.8305, recall: 0.8189\n",
      "2019-09-23T10:21:55.052710, step: 633, loss: 42.77335739135742, acc: 0.8516, auc: 0.9456, precision: 0.8593, recall: 0.8564\n",
      "2019-09-23T10:21:56.820981, step: 634, loss: 47.130577087402344, acc: 0.8438, auc: 0.9152, precision: 0.8424, recall: 0.8335\n",
      "2019-09-23T10:21:58.612189, step: 635, loss: 37.602542877197266, acc: 0.8828, auc: 0.9541, precision: 0.8879, recall: 0.8819\n",
      "2019-09-23T10:22:00.398411, step: 636, loss: 55.40773391723633, acc: 0.7734, auc: 0.8889, precision: 0.7898, recall: 0.7674\n",
      "2019-09-23T10:22:02.211561, step: 637, loss: 36.875186920166016, acc: 0.8828, auc: 0.9632, precision: 0.8973, recall: 0.877\n",
      "2019-09-23T10:22:04.043661, step: 638, loss: 54.006370544433594, acc: 0.7734, auc: 0.9192, precision: 0.801, recall: 0.7869\n",
      "2019-09-23T10:22:05.846837, step: 639, loss: 102.22607421875, acc: 0.6719, auc: 0.852, precision: 0.8037, recall: 0.6667\n",
      "2019-09-23T10:22:07.648020, step: 640, loss: 68.14938354492188, acc: 0.7656, auc: 0.89, precision: 0.8234, recall: 0.7403\n",
      "2019-09-23T10:22:09.470145, step: 641, loss: 65.50245666503906, acc: 0.7344, auc: 0.8238, precision: 0.7377, recall: 0.7322\n",
      "2019-09-23T10:22:11.292290, step: 642, loss: 46.94647216796875, acc: 0.8516, auc: 0.9409, precision: 0.8502, recall: 0.8541\n",
      "2019-09-23T10:22:13.085474, step: 643, loss: 42.57545852661133, acc: 0.875, auc: 0.9479, precision: 0.8792, recall: 0.8644\n",
      "2019-09-23T10:22:14.854742, step: 644, loss: 45.726654052734375, acc: 0.8281, auc: 0.9233, precision: 0.8257, recall: 0.8224\n",
      "2019-09-23T10:22:16.621017, step: 645, loss: 47.411163330078125, acc: 0.8438, auc: 0.9196, precision: 0.8438, recall: 0.8451\n",
      "2019-09-23T10:22:18.380311, step: 646, loss: 44.923343658447266, acc: 0.875, auc: 0.9275, precision: 0.8744, recall: 0.8755\n",
      "2019-09-23T10:22:20.144592, step: 647, loss: 46.71487045288086, acc: 0.8359, auc: 0.9309, precision: 0.8435, recall: 0.8407\n",
      "2019-09-23T10:22:21.945774, step: 648, loss: 55.24187469482422, acc: 0.8125, auc: 0.901, precision: 0.8178, recall: 0.8104\n",
      "2019-09-23T10:22:23.751942, step: 649, loss: 52.51435852050781, acc: 0.8281, auc: 0.9097, precision: 0.8446, recall: 0.8281\n",
      "2019-09-23T10:22:25.539162, step: 650, loss: 49.9434928894043, acc: 0.8125, auc: 0.9228, precision: 0.8209, recall: 0.8099\n",
      "2019-09-23T10:22:27.318403, step: 651, loss: 54.542015075683594, acc: 0.8203, auc: 0.9043, precision: 0.831, recall: 0.8159\n",
      "2019-09-23T10:22:29.089665, step: 652, loss: 51.40724182128906, acc: 0.8047, auc: 0.9119, precision: 0.8303, recall: 0.7752\n",
      "2019-09-23T10:22:30.857935, step: 653, loss: 45.38008117675781, acc: 0.8438, auc: 0.9223, precision: 0.8449, recall: 0.8442\n",
      "2019-09-23T10:22:32.652136, step: 654, loss: 43.672149658203125, acc: 0.8281, auc: 0.9304, precision: 0.8294, recall: 0.8255\n",
      "2019-09-23T10:22:34.470272, step: 655, loss: 38.24087905883789, acc: 0.8672, auc: 0.9453, precision: 0.8699, recall: 0.8658\n",
      "2019-09-23T10:22:36.272452, step: 656, loss: 44.92769241333008, acc: 0.8359, auc: 0.9221, precision: 0.8365, recall: 0.8363\n",
      "2019-09-23T10:22:38.059670, step: 657, loss: 33.88276290893555, acc: 0.9062, auc: 0.9597, precision: 0.9155, recall: 0.9008\n",
      "2019-09-23T10:22:39.818966, step: 658, loss: 37.217132568359375, acc: 0.8828, auc: 0.9565, precision: 0.8864, recall: 0.8844\n",
      "2019-09-23T10:22:41.590226, step: 659, loss: 90.28392028808594, acc: 0.7109, auc: 0.8859, precision: 0.774, recall: 0.6704\n",
      "2019-09-23T10:22:43.362486, step: 660, loss: 59.97469711303711, acc: 0.7266, auc: 0.9446, precision: 0.8333, recall: 0.6983\n",
      "2019-09-23T10:22:45.134746, step: 661, loss: 56.243621826171875, acc: 0.8203, auc: 0.901, precision: 0.8242, recall: 0.8135\n",
      "2019-09-23T10:22:46.914985, step: 662, loss: 39.554588317871094, acc: 0.9062, auc: 0.9501, precision: 0.9069, recall: 0.9057\n",
      "2019-09-23T10:22:48.699212, step: 663, loss: 49.875465393066406, acc: 0.8281, auc: 0.9038, precision: 0.8308, recall: 0.8288\n",
      "2019-09-23T10:22:50.502387, step: 664, loss: 36.3030891418457, acc: 0.8906, auc: 0.9646, precision: 0.8941, recall: 0.8906\n",
      "2019-09-23T10:22:52.278636, step: 665, loss: 50.552818298339844, acc: 0.8203, auc: 0.9146, precision: 0.8389, recall: 0.8203\n",
      "2019-09-23T10:22:54.036934, step: 666, loss: 49.73569869995117, acc: 0.8672, auc: 0.9292, precision: 0.879, recall: 0.8658\n",
      "2019-09-23T10:22:55.800217, step: 667, loss: 47.64082336425781, acc: 0.7969, auc: 0.937, precision: 0.8042, recall: 0.8085\n",
      "2019-09-23T10:22:57.582450, step: 668, loss: 54.13496398925781, acc: 0.8203, auc: 0.9103, precision: 0.836, recall: 0.8069\n",
      "2019-09-23T10:22:59.340746, step: 669, loss: 45.132896423339844, acc: 0.8438, auc: 0.9497, precision: 0.8646, recall: 0.8475\n",
      "2019-09-23T10:23:01.126968, step: 670, loss: 39.99345779418945, acc: 0.8672, auc: 0.9366, precision: 0.8626, recall: 0.8645\n",
      "2019-09-23T10:23:02.911196, step: 671, loss: 52.98959732055664, acc: 0.8359, auc: 0.8944, precision: 0.8383, recall: 0.8346\n",
      "2019-09-23T10:23:04.680477, step: 672, loss: 54.760475158691406, acc: 0.8125, auc: 0.9051, precision: 0.8355, recall: 0.8039\n",
      "2019-09-23T10:23:06.465688, step: 673, loss: 50.39635467529297, acc: 0.8125, auc: 0.9387, precision: 0.8439, recall: 0.7917\n",
      "2019-09-23T10:23:08.240939, step: 674, loss: 38.44495391845703, acc: 0.8672, auc: 0.9656, precision: 0.8885, recall: 0.8672\n",
      "2019-09-23T10:23:10.031150, step: 675, loss: 47.82420349121094, acc: 0.8203, auc: 0.9135, precision: 0.8218, recall: 0.8214\n",
      "2019-09-23T10:23:11.792439, step: 676, loss: 30.875648498535156, acc: 0.9141, auc: 0.9705, precision: 0.9182, recall: 0.9105\n",
      "2019-09-23T10:23:13.568696, step: 677, loss: 45.73896408081055, acc: 0.8438, auc: 0.919, precision: 0.8422, recall: 0.8438\n",
      "2019-09-23T10:23:15.363899, step: 678, loss: 57.34770965576172, acc: 0.8516, auc: 0.8845, precision: 0.8566, recall: 0.8487\n",
      "2019-09-23T10:23:17.128166, step: 679, loss: 42.62631607055664, acc: 0.875, auc: 0.9353, precision: 0.8934, recall: 0.8631\n",
      "2019-09-23T10:23:18.958271, step: 680, loss: 47.115726470947266, acc: 0.8516, auc: 0.9242, precision: 0.8601, recall: 0.8366\n",
      "2019-09-23T10:23:20.772418, step: 681, loss: 47.20240020751953, acc: 0.8125, auc: 0.9158, precision: 0.8128, recall: 0.8125\n",
      "2019-09-23T10:23:22.653388, step: 682, loss: 48.207393646240234, acc: 0.8203, auc: 0.9132, precision: 0.8225, recall: 0.8189\n",
      "2019-09-23T10:23:24.502387, step: 683, loss: 52.98936462402344, acc: 0.8125, auc: 0.8974, precision: 0.8167, recall: 0.8143\n",
      "2019-09-23T10:23:26.397317, step: 684, loss: 52.17708969116211, acc: 0.7812, auc: 0.9602, precision: 0.8478, recall: 0.7812\n",
      "2019-09-23T10:23:28.228419, step: 685, loss: 87.596435546875, acc: 0.6562, auc: 0.9426, precision: 0.7732, recall: 0.7067\n",
      "2019-09-23T10:23:30.040573, step: 686, loss: 55.19523620605469, acc: 0.75, auc: 0.916, precision: 0.7742, recall: 0.7607\n",
      "2019-09-23T10:23:31.920543, step: 687, loss: 47.99060821533203, acc: 0.8594, auc: 0.9302, precision: 0.861, recall: 0.8561\n",
      "2019-09-23T10:23:33.693801, step: 688, loss: 42.79004669189453, acc: 0.8672, auc: 0.9429, precision: 0.8676, recall: 0.863\n",
      "2019-09-23T10:23:35.455089, step: 689, loss: 50.19212341308594, acc: 0.8125, auc: 0.901, precision: 0.8049, recall: 0.8174\n",
      "2019-09-23T10:23:37.240314, step: 690, loss: 60.905277252197266, acc: 0.7578, auc: 0.9269, precision: 0.8056, recall: 0.7515\n",
      "2019-09-23T10:23:38.991640, step: 691, loss: 50.897422790527344, acc: 0.8281, auc: 0.938, precision: 0.8711, recall: 0.8087\n",
      "2019-09-23T10:23:40.776853, step: 692, loss: 36.32484436035156, acc: 0.9141, auc: 0.9651, precision: 0.9157, recall: 0.9178\n",
      "2019-09-23T10:23:42.540138, step: 693, loss: 59.339317321777344, acc: 0.7422, auc: 0.8606, precision: 0.7471, recall: 0.7422\n",
      "2019-09-23T10:23:44.308407, step: 694, loss: 39.9755973815918, acc: 0.8906, auc: 0.943, precision: 0.8912, recall: 0.8897\n",
      "2019-09-23T10:23:46.107595, step: 695, loss: 33.88373565673828, acc: 0.9219, auc: 0.9632, precision: 0.9214, recall: 0.9214\n",
      "2019-09-23T10:23:47.879858, step: 696, loss: 42.44084548950195, acc: 0.8594, auc: 0.9597, precision: 0.8732, recall: 0.8579\n",
      "2019-09-23T10:23:49.674055, step: 697, loss: 61.09764099121094, acc: 0.7734, auc: 0.9251, precision: 0.8286, recall: 0.745\n",
      "2019-09-23T10:23:51.444319, step: 698, loss: 51.28972625732422, acc: 0.7969, auc: 0.9135, precision: 0.8099, recall: 0.7682\n",
      "2019-09-23T10:23:53.230541, step: 699, loss: 42.0777587890625, acc: 0.8594, auc: 0.9383, precision: 0.8534, recall: 0.8573\n",
      "2019-09-23T10:23:55.020753, step: 700, loss: 44.48072814941406, acc: 0.8438, auc: 0.9394, precision: 0.8465, recall: 0.8549\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:25:02.109575, step: 700, loss: 58.15536782680414, acc: 0.7948717948717948, auc: 0.9022307692307695, precision: 0.8091358974358976, recall: 0.7958948717948718\n",
      "2019-09-23T10:25:03.926715, step: 701, loss: 41.10436248779297, acc: 0.8672, auc: 0.9528, precision: 0.8801, recall: 0.8717\n",
      "2019-09-23T10:25:05.673043, step: 702, loss: 41.93193435668945, acc: 0.8672, auc: 0.9582, precision: 0.8838, recall: 0.8656\n",
      "2019-09-23T10:25:07.468241, step: 703, loss: 45.06266403198242, acc: 0.8359, auc: 0.9313, precision: 0.835, recall: 0.8403\n",
      "2019-09-23T10:25:09.238507, step: 704, loss: 45.611915588378906, acc: 0.8594, auc: 0.9216, precision: 0.8659, recall: 0.8573\n",
      "2019-09-23T10:25:11.025725, step: 705, loss: 44.06444549560547, acc: 0.8438, auc: 0.9387, precision: 0.8412, recall: 0.8473\n",
      "2019-09-23T10:25:12.805964, step: 706, loss: 45.908653259277344, acc: 0.8672, auc: 0.9421, precision: 0.8784, recall: 0.8672\n",
      "2019-09-23T10:25:14.588196, step: 707, loss: 64.87483978271484, acc: 0.7422, auc: 0.8836, precision: 0.7775, recall: 0.7192\n",
      "2019-09-23T10:25:16.368434, step: 708, loss: 52.126800537109375, acc: 0.8047, auc: 0.9389, precision: 0.8486, recall: 0.8074\n",
      "2019-09-23T10:25:18.179600, step: 709, loss: 43.75303649902344, acc: 0.8516, auc: 0.9413, precision: 0.8519, recall: 0.8522\n",
      "2019-09-23T10:25:20.016676, step: 710, loss: 48.12896728515625, acc: 0.8516, auc: 0.9364, precision: 0.8546, recall: 0.865\n",
      "2019-09-23T10:25:21.835809, step: 711, loss: 46.238365173339844, acc: 0.8359, auc: 0.9681, precision: 0.8688, recall: 0.8478\n",
      "2019-09-23T10:25:23.648960, step: 712, loss: 52.386844635009766, acc: 0.7969, auc: 0.9384, precision: 0.8344, recall: 0.8021\n",
      "2019-09-23T10:25:25.459118, step: 713, loss: 48.66755294799805, acc: 0.8203, auc: 0.9294, precision: 0.8389, recall: 0.8203\n",
      "2019-09-23T10:25:27.225394, step: 714, loss: 40.4321174621582, acc: 0.8594, auc: 0.9411, precision: 0.8593, recall: 0.8593\n",
      "2019-09-23T10:25:29.007625, step: 715, loss: 52.64105224609375, acc: 0.8125, auc: 0.8941, precision: 0.8125, recall: 0.8137\n",
      "2019-09-23T10:25:30.763927, step: 716, loss: 45.791419982910156, acc: 0.8516, auc: 0.9201, precision: 0.8525, recall: 0.8507\n",
      "2019-09-23T10:25:32.562118, step: 717, loss: 35.5992317199707, acc: 0.9141, auc: 0.9547, precision: 0.9141, recall: 0.915\n",
      "2019-09-23T10:25:34.383246, step: 718, loss: 50.55037307739258, acc: 0.8047, auc: 0.9043, precision: 0.8054, recall: 0.8043\n",
      "2019-09-23T10:25:36.150519, step: 719, loss: 45.481990814208984, acc: 0.875, auc: 0.9249, precision: 0.8769, recall: 0.8769\n",
      "2019-09-23T10:25:37.957904, step: 720, loss: 37.16806411743164, acc: 0.875, auc: 0.9738, precision: 0.8906, recall: 0.8799\n",
      "2019-09-23T10:25:39.741134, step: 721, loss: 54.62527084350586, acc: 0.7891, auc: 0.9429, precision: 0.8319, recall: 0.7891\n",
      "2019-09-23T10:25:41.532342, step: 722, loss: 58.4622688293457, acc: 0.7578, auc: 0.9412, precision: 0.8187, recall: 0.7315\n",
      "2019-09-23T10:25:43.321557, step: 723, loss: 46.809486389160156, acc: 0.8516, auc: 0.9226, precision: 0.8522, recall: 0.8488\n",
      "2019-09-23T10:25:45.100797, step: 724, loss: 46.69196701049805, acc: 0.8438, auc: 0.9204, precision: 0.8424, recall: 0.8424\n",
      "2019-09-23T10:25:46.909958, step: 725, loss: 56.0128173828125, acc: 0.7969, auc: 0.8823, precision: 0.7973, recall: 0.7932\n",
      "2019-09-23T10:25:48.718121, step: 726, loss: 40.59980392456055, acc: 0.8594, auc: 0.9377, precision: 0.8594, recall: 0.8594\n",
      "2019-09-23T10:25:50.498360, step: 727, loss: 39.91331100463867, acc: 0.8984, auc: 0.9328, precision: 0.9104, recall: 0.8894\n",
      "2019-09-23T10:25:52.260645, step: 728, loss: 42.32114791870117, acc: 0.8359, auc: 0.9287, precision: 0.8382, recall: 0.8328\n",
      "2019-09-23T10:25:54.103724, step: 729, loss: 37.85059356689453, acc: 0.8828, auc: 0.951, precision: 0.8788, recall: 0.8836\n",
      "2019-09-23T10:25:55.916865, step: 730, loss: 48.477272033691406, acc: 0.8438, auc: 0.948, precision: 0.8618, recall: 0.842\n",
      "2019-09-23T10:25:57.689125, step: 731, loss: 68.74789428710938, acc: 0.7656, auc: 0.9293, precision: 0.8243, recall: 0.7754\n",
      "2019-09-23T10:25:59.486317, step: 732, loss: 68.3013916015625, acc: 0.7031, auc: 0.9171, precision: 0.7832, recall: 0.7375\n",
      "2019-09-23T10:26:01.274534, step: 733, loss: 51.92618942260742, acc: 0.8125, auc: 0.9062, precision: 0.8103, recall: 0.8075\n",
      "2019-09-23T10:26:03.063747, step: 734, loss: 38.739803314208984, acc: 0.8672, auc: 0.9679, precision: 0.8719, recall: 0.8711\n",
      "2019-09-23T10:26:04.874904, step: 735, loss: 35.1357421875, acc: 0.9062, auc: 0.9536, precision: 0.9062, recall: 0.9066\n",
      "2019-09-23T10:26:06.667110, step: 736, loss: 47.629417419433594, acc: 0.8594, auc: 0.9133, precision: 0.8559, recall: 0.8588\n",
      "2019-09-23T10:26:08.497214, step: 737, loss: 40.049957275390625, acc: 0.8828, auc: 0.9409, precision: 0.8832, recall: 0.8834\n",
      "2019-09-23T10:26:10.292412, step: 738, loss: 37.92008972167969, acc: 0.8594, auc: 0.9456, precision: 0.8588, recall: 0.8588\n",
      "2019-09-23T10:26:12.090602, step: 739, loss: 45.00676727294922, acc: 0.8516, auc: 0.926, precision: 0.8537, recall: 0.8516\n",
      "2019-09-23T10:26:13.876944, step: 740, loss: 42.91270446777344, acc: 0.8438, auc: 0.936, precision: 0.8441, recall: 0.8438\n",
      "2019-09-23T10:26:15.701063, step: 741, loss: 47.268306732177734, acc: 0.8047, auc: 0.9356, precision: 0.8389, recall: 0.7725\n",
      "2019-09-23T10:26:17.476315, step: 742, loss: 44.939796447753906, acc: 0.8281, auc: 0.9218, precision: 0.8369, recall: 0.8134\n",
      "2019-09-23T10:26:19.319390, step: 743, loss: 40.73033905029297, acc: 0.8359, auc: 0.9438, precision: 0.8377, recall: 0.8365\n",
      "2019-09-23T10:26:21.089650, step: 744, loss: 51.50550842285156, acc: 0.8125, auc: 0.9143, precision: 0.8282, recall: 0.8125\n",
      "2019-09-23T10:26:22.903797, step: 745, loss: 86.85240173339844, acc: 0.7266, auc: 0.8673, precision: 0.7836, recall: 0.7334\n",
      "2019-09-23T10:26:24.726922, step: 746, loss: 85.0545883178711, acc: 0.5625, auc: 0.9254, precision: 0.7455, recall: 0.6216\n",
      "2019-09-23T10:26:26.522118, step: 747, loss: 59.556888580322266, acc: 0.8047, auc: 0.9316, precision: 0.8415, recall: 0.8047\n",
      "2019-09-23T10:26:28.447967, step: 748, loss: 43.66579818725586, acc: 0.8672, auc: 0.9473, precision: 0.8658, recall: 0.8682\n",
      "2019-09-23T10:26:30.333922, step: 749, loss: 47.49729537963867, acc: 0.8203, auc: 0.925, precision: 0.8308, recall: 0.8174\n",
      "2019-09-23T10:26:32.151061, step: 750, loss: 48.37907409667969, acc: 0.8438, auc: 0.9188, precision: 0.8435, recall: 0.8441\n",
      "2019-09-23T10:26:34.011086, step: 751, loss: 35.52936553955078, acc: 0.9141, auc: 0.9619, precision: 0.9154, recall: 0.9132\n",
      "2019-09-23T10:26:35.880087, step: 752, loss: 55.34046936035156, acc: 0.8203, auc: 0.8901, precision: 0.8224, recall: 0.8181\n",
      "2019-09-23T10:26:37.708197, step: 753, loss: 31.9251651763916, acc: 0.9062, auc: 0.9681, precision: 0.9057, recall: 0.9069\n",
      "2019-09-23T10:26:39.574205, step: 754, loss: 50.242828369140625, acc: 0.8281, auc: 0.9062, precision: 0.8289, recall: 0.8289\n",
      "2019-09-23T10:26:41.391353, step: 755, loss: 53.684749603271484, acc: 0.7578, auc: 0.8926, precision: 0.7606, recall: 0.7623\n",
      "2019-09-23T10:26:43.206489, step: 756, loss: 47.04417419433594, acc: 0.8125, auc: 0.9243, precision: 0.8175, recall: 0.8125\n",
      "2019-09-23T10:26:44.986727, step: 757, loss: 47.39625549316406, acc: 0.8438, auc: 0.937, precision: 0.8478, recall: 0.854\n",
      "2019-09-23T10:26:46.777936, step: 758, loss: 61.65265655517578, acc: 0.7656, auc: 0.9413, precision: 0.8322, recall: 0.7548\n",
      "2019-09-23T10:26:48.571140, step: 759, loss: 51.70692443847656, acc: 0.8281, auc: 0.9307, precision: 0.845, recall: 0.8369\n",
      "2019-09-23T10:26:50.369329, step: 760, loss: 42.59349822998047, acc: 0.8438, auc: 0.9456, precision: 0.8426, recall: 0.8453\n",
      "2019-09-23T10:26:52.166522, step: 761, loss: 44.59661865234375, acc: 0.8203, auc: 0.9313, precision: 0.8402, recall: 0.8165\n",
      "2019-09-23T10:26:53.935789, step: 762, loss: 48.32977294921875, acc: 0.8516, auc: 0.9087, precision: 0.8566, recall: 0.8487\n",
      "2019-09-23T10:26:55.722011, step: 763, loss: 45.0531005859375, acc: 0.8203, auc: 0.9233, precision: 0.8209, recall: 0.8206\n",
      "2019-09-23T10:26:57.482303, step: 764, loss: 44.882789611816406, acc: 0.8281, auc: 0.925, precision: 0.8325, recall: 0.8299\n",
      "2019-09-23T10:26:59.281490, step: 765, loss: 46.1932373046875, acc: 0.8438, auc: 0.921, precision: 0.8477, recall: 0.8463\n",
      "2019-09-23T10:27:01.084667, step: 766, loss: 59.86878967285156, acc: 0.7891, auc: 0.9152, precision: 0.8101, recall: 0.8117\n",
      "2019-09-23T10:27:02.912776, step: 767, loss: 70.51432800292969, acc: 0.7188, auc: 0.9541, precision: 0.8182, recall: 0.7231\n",
      "2019-09-23T10:27:04.698001, step: 768, loss: 54.01282501220703, acc: 0.8281, auc: 0.9296, precision: 0.869, recall: 0.8333\n",
      "2019-09-23T10:27:06.488213, step: 769, loss: 52.150516510009766, acc: 0.8047, auc: 0.901, precision: 0.805, recall: 0.8052\n",
      "2019-09-23T10:27:08.320312, step: 770, loss: 50.06352233886719, acc: 0.8672, auc: 0.9187, precision: 0.8673, recall: 0.867\n",
      "2019-09-23T10:27:10.101548, step: 771, loss: 46.11093521118164, acc: 0.8438, auc: 0.9335, precision: 0.849, recall: 0.8532\n",
      "2019-09-23T10:27:11.894751, step: 772, loss: 55.61585235595703, acc: 0.8281, auc: 0.9136, precision: 0.8508, recall: 0.8261\n",
      "2019-09-23T10:27:13.716878, step: 773, loss: 48.01735305786133, acc: 0.8281, auc: 0.9375, precision: 0.8395, recall: 0.8295\n",
      "2019-09-23T10:27:15.510081, step: 774, loss: 42.872344970703125, acc: 0.8672, auc: 0.9358, precision: 0.8772, recall: 0.8507\n",
      "2019-09-23T10:27:17.298297, step: 775, loss: 49.31589889526367, acc: 0.8203, auc: 0.9103, precision: 0.8214, recall: 0.8138\n",
      "2019-09-23T10:27:19.115436, step: 776, loss: 48.16834259033203, acc: 0.8438, auc: 0.9128, precision: 0.8437, recall: 0.8437\n",
      "2019-09-23T10:27:20.889690, step: 777, loss: 45.15522003173828, acc: 0.8438, auc: 0.938, precision: 0.8502, recall: 0.8522\n",
      "2019-09-23T10:27:22.861416, step: 778, loss: 51.27887725830078, acc: 0.8359, auc: 0.9297, precision: 0.8833, recall: 0.822\n",
      "2019-09-23T10:27:24.661601, step: 779, loss: 66.97325897216797, acc: 0.7891, auc: 0.9211, precision: 0.8354, recall: 0.8005\n",
      "2019-09-23T10:27:26.439844, step: 780, loss: 58.91156768798828, acc: 0.8047, auc: 0.8924, precision: 0.8303, recall: 0.793\n",
      "start to train models...\n",
      "2019-09-23T10:27:28.348738, step: 781, loss: 40.227752685546875, acc: 0.9141, auc: 0.9684, precision: 0.9185, recall: 0.9087\n",
      "2019-09-23T10:27:30.176848, step: 782, loss: 36.14323806762695, acc: 0.9062, auc: 0.9633, precision: 0.9054, recall: 0.9054\n",
      "2019-09-23T10:27:31.962073, step: 783, loss: 33.86997604370117, acc: 0.9141, auc: 0.9624, precision: 0.9157, recall: 0.9117\n",
      "2019-09-23T10:27:33.768242, step: 784, loss: 31.856782913208008, acc: 0.8984, auc: 0.9624, precision: 0.8985, recall: 0.8984\n",
      "2019-09-23T10:27:35.540513, step: 785, loss: 39.65782165527344, acc: 0.875, auc: 0.9457, precision: 0.8757, recall: 0.8779\n",
      "2019-09-23T10:27:37.354648, step: 786, loss: 52.15355682373047, acc: 0.7891, auc: 0.9348, precision: 0.8231, recall: 0.7916\n",
      "2019-09-23T10:27:39.152839, step: 787, loss: 45.03171157836914, acc: 0.8281, auc: 0.9753, precision: 0.8671, recall: 0.8204\n",
      "2019-09-23T10:27:40.932079, step: 788, loss: 36.82886505126953, acc: 0.8594, auc: 0.9623, precision: 0.8617, recall: 0.8671\n",
      "2019-09-23T10:27:42.737250, step: 789, loss: 39.463932037353516, acc: 0.8594, auc: 0.9549, precision: 0.8584, recall: 0.8655\n",
      "2019-09-23T10:27:44.579162, step: 790, loss: 32.51231002807617, acc: 0.8984, auc: 0.9751, precision: 0.9058, recall: 0.8995\n",
      "2019-09-23T10:27:46.371372, step: 791, loss: 34.67812728881836, acc: 0.9062, auc: 0.9821, precision: 0.9259, recall: 0.8983\n",
      "2019-09-23T10:27:48.152606, step: 792, loss: 78.74295043945312, acc: 0.7734, auc: 0.9181, precision: 0.8352, recall: 0.7666\n",
      "2019-09-23T10:27:49.938374, step: 793, loss: 54.21503829956055, acc: 0.7891, auc: 0.9344, precision: 0.8285, recall: 0.797\n",
      "2019-09-23T10:27:51.718613, step: 794, loss: 45.13245391845703, acc: 0.875, auc: 0.9361, precision: 0.874, recall: 0.8755\n",
      "2019-09-23T10:27:53.574647, step: 795, loss: 40.858123779296875, acc: 0.8594, auc: 0.9441, precision: 0.8774, recall: 0.8594\n",
      "2019-09-23T10:27:55.398768, step: 796, loss: 37.79181671142578, acc: 0.8672, auc: 0.9524, precision: 0.8784, recall: 0.8672\n",
      "2019-09-23T10:27:57.185987, step: 797, loss: 46.20274353027344, acc: 0.8672, auc: 0.9392, precision: 0.8784, recall: 0.8672\n",
      "2019-09-23T10:27:58.959244, step: 798, loss: 38.75112533569336, acc: 0.875, auc: 0.9627, precision: 0.8878, recall: 0.8648\n",
      "2019-09-23T10:28:00.766411, step: 799, loss: 36.725807189941406, acc: 0.8828, auc: 0.957, precision: 0.8855, recall: 0.8822\n",
      "2019-09-23T10:28:02.591532, step: 800, loss: 36.02614974975586, acc: 0.8828, auc: 0.9605, precision: 0.8877, recall: 0.8909\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:29:10.100123, step: 800, loss: 55.45488567841359, acc: 0.8225179487179486, auc: 0.9121615384615385, precision: 0.8332666666666666, recall: 0.8217076923076923\n",
      "2019-09-23T10:29:11.950175, step: 801, loss: 40.51666259765625, acc: 0.8359, auc: 0.9628, precision: 0.8494, recall: 0.8456\n",
      "2019-09-23T10:29:13.748365, step: 802, loss: 65.69429779052734, acc: 0.8359, auc: 0.9229, precision: 0.8684, recall: 0.8359\n",
      "2019-09-23T10:29:15.550544, step: 803, loss: 54.80815124511719, acc: 0.8203, auc: 0.9254, precision: 0.8335, recall: 0.8298\n",
      "2019-09-23T10:29:17.351726, step: 804, loss: 43.286869049072266, acc: 0.8594, auc: 0.9499, precision: 0.8689, recall: 0.8581\n",
      "2019-09-23T10:29:19.172855, step: 805, loss: 45.22496795654297, acc: 0.8516, auc: 0.9539, precision: 0.8565, recall: 0.8423\n",
      "2019-09-23T10:29:20.962069, step: 806, loss: 42.61756896972656, acc: 0.8359, auc: 0.95, precision: 0.8517, recall: 0.8426\n",
      "2019-09-23T10:29:22.773224, step: 807, loss: 48.48895263671875, acc: 0.8047, auc: 0.9532, precision: 0.8367, recall: 0.8142\n",
      "2019-09-23T10:29:24.558449, step: 808, loss: 65.75769805908203, acc: 0.7891, auc: 0.9282, precision: 0.8436, recall: 0.7827\n",
      "2019-09-23T10:29:26.349657, step: 809, loss: 44.99662780761719, acc: 0.8125, auc: 0.963, precision: 0.85, recall: 0.802\n",
      "2019-09-23T10:29:28.151837, step: 810, loss: 38.927913665771484, acc: 0.8828, auc: 0.9618, precision: 0.8819, recall: 0.8879\n",
      "2019-09-23T10:29:29.980944, step: 811, loss: 38.9098014831543, acc: 0.8359, auc: 0.9542, precision: 0.8381, recall: 0.8387\n",
      "2019-09-23T10:29:31.785119, step: 812, loss: 48.41252517700195, acc: 0.8203, auc: 0.9256, precision: 0.8223, recall: 0.8259\n",
      "2019-09-23T10:29:33.610236, step: 813, loss: 31.42494773864746, acc: 0.9062, auc: 0.9751, precision: 0.9159, recall: 0.8995\n",
      "2019-09-23T10:29:35.392468, step: 814, loss: 37.267276763916016, acc: 0.875, auc: 0.9487, precision: 0.8752, recall: 0.8752\n",
      "2019-09-23T10:29:37.196643, step: 815, loss: 40.98533630371094, acc: 0.875, auc: 0.9368, precision: 0.875, recall: 0.875\n",
      "2019-09-23T10:29:39.005804, step: 816, loss: 45.03278350830078, acc: 0.8359, auc: 0.9297, precision: 0.8431, recall: 0.8348\n",
      "2019-09-23T10:29:40.796015, step: 817, loss: 41.97834777832031, acc: 0.8594, auc: 0.974, precision: 0.8772, recall: 0.8683\n",
      "2019-09-23T10:29:42.618141, step: 818, loss: 58.819664001464844, acc: 0.8047, auc: 0.9242, precision: 0.8602, recall: 0.771\n",
      "2019-09-23T10:29:44.393392, step: 819, loss: 35.36839294433594, acc: 0.8828, auc: 0.976, precision: 0.8973, recall: 0.877\n",
      "2019-09-23T10:29:46.231476, step: 820, loss: 40.60173797607422, acc: 0.875, auc: 0.9487, precision: 0.8944, recall: 0.859\n",
      "2019-09-23T10:29:48.020689, step: 821, loss: 39.750484466552734, acc: 0.8594, auc: 0.9403, precision: 0.8589, recall: 0.861\n",
      "2019-09-23T10:29:49.776992, step: 822, loss: 40.40122604370117, acc: 0.8672, auc: 0.936, precision: 0.8679, recall: 0.8638\n",
      "2019-09-23T10:29:51.571193, step: 823, loss: 36.525367736816406, acc: 0.8984, auc: 0.946, precision: 0.9, recall: 0.8996\n",
      "2019-09-23T10:29:53.368386, step: 824, loss: 41.160430908203125, acc: 0.875, auc: 0.9339, precision: 0.8755, recall: 0.874\n",
      "2019-09-23T10:29:55.163583, step: 825, loss: 24.089111328125, acc: 0.9297, auc: 0.9851, precision: 0.9294, recall: 0.9276\n",
      "2019-09-23T10:29:56.941826, step: 826, loss: 47.737388610839844, acc: 0.8516, auc: 0.9358, precision: 0.8534, recall: 0.8521\n",
      "2019-09-23T10:29:58.783902, step: 827, loss: 50.004730224609375, acc: 0.8359, auc: 0.9445, precision: 0.8558, recall: 0.8454\n",
      "2019-09-23T10:30:00.586078, step: 828, loss: 50.451698303222656, acc: 0.8281, auc: 0.9253, precision: 0.8626, recall: 0.8075\n",
      "2019-09-23T10:30:02.423164, step: 829, loss: 35.8687629699707, acc: 0.8828, auc: 0.967, precision: 0.8945, recall: 0.8828\n",
      "2019-09-23T10:30:04.203403, step: 830, loss: 29.553516387939453, acc: 0.9141, auc: 0.9736, precision: 0.9142, recall: 0.9139\n",
      "2019-09-23T10:30:05.993615, step: 831, loss: 28.376863479614258, acc: 0.9141, auc: 0.9814, precision: 0.9093, recall: 0.9253\n",
      "2019-09-23T10:30:07.784823, step: 832, loss: 38.211524963378906, acc: 0.8984, auc: 0.9628, precision: 0.9188, recall: 0.878\n",
      "2019-09-23T10:30:09.576032, step: 833, loss: 51.71287536621094, acc: 0.8594, auc: 0.9412, precision: 0.8785, recall: 0.8667\n",
      "2019-09-23T10:30:11.360258, step: 834, loss: 39.22005081176758, acc: 0.8984, auc: 0.9607, precision: 0.9114, recall: 0.8971\n",
      "2019-09-23T10:30:13.195350, step: 835, loss: 28.561979293823242, acc: 0.9141, auc: 0.9801, precision: 0.9053, recall: 0.9091\n",
      "2019-09-23T10:30:14.978580, step: 836, loss: 42.26171112060547, acc: 0.8359, auc: 0.932, precision: 0.8337, recall: 0.8348\n",
      "2019-09-23T10:30:16.771784, step: 837, loss: 43.51530075073242, acc: 0.8516, auc: 0.9514, precision: 0.8609, recall: 0.8541\n",
      "2019-09-23T10:30:18.562008, step: 838, loss: 42.605552673339844, acc: 0.8359, auc: 0.9437, precision: 0.8521, recall: 0.8308\n",
      "2019-09-23T10:30:20.374148, step: 839, loss: 37.50830841064453, acc: 0.8828, auc: 0.9491, precision: 0.8836, recall: 0.8788\n",
      "2019-09-23T10:30:22.175330, step: 840, loss: 29.805038452148438, acc: 0.8906, auc: 0.9697, precision: 0.8936, recall: 0.8913\n",
      "2019-09-23T10:30:23.981498, step: 841, loss: 28.654613494873047, acc: 0.8672, auc: 0.9714, precision: 0.8642, recall: 0.8701\n",
      "2019-09-23T10:30:25.804621, step: 842, loss: 49.469390869140625, acc: 0.8672, auc: 0.936, precision: 0.8751, recall: 0.8721\n",
      "2019-09-23T10:30:27.598823, step: 843, loss: 103.82721710205078, acc: 0.625, auc: 0.947, precision: 0.7574, recall: 0.6359\n",
      "2019-09-23T10:30:29.395030, step: 844, loss: 64.28585815429688, acc: 0.7578, auc: 0.8874, precision: 0.8183, recall: 0.7357\n",
      "2019-09-23T10:30:31.224125, step: 845, loss: 39.90032196044922, acc: 0.8984, auc: 0.9721, precision: 0.9047, recall: 0.8956\n",
      "2019-09-23T10:30:33.023312, step: 846, loss: 46.46070098876953, acc: 0.8281, auc: 0.9153, precision: 0.8312, recall: 0.8234\n",
      "2019-09-23T10:30:34.822500, step: 847, loss: 41.78058624267578, acc: 0.8672, auc: 0.9364, precision: 0.8672, recall: 0.8694\n",
      "2019-09-23T10:30:36.622685, step: 848, loss: 39.24187469482422, acc: 0.8984, auc: 0.9512, precision: 0.9005, recall: 0.899\n",
      "2019-09-23T10:30:38.440821, step: 849, loss: 37.23855209350586, acc: 0.8828, auc: 0.9604, precision: 0.8879, recall: 0.8819\n",
      "2019-09-23T10:30:40.238013, step: 850, loss: 40.05621337890625, acc: 0.8672, auc: 0.9669, precision: 0.873, recall: 0.8791\n",
      "2019-09-23T10:30:42.032215, step: 851, loss: 39.67333984375, acc: 0.8672, auc: 0.953, precision: 0.8898, recall: 0.8452\n",
      "2019-09-23T10:30:43.836388, step: 852, loss: 47.84313201904297, acc: 0.8203, auc: 0.9227, precision: 0.8211, recall: 0.8239\n",
      "2019-09-23T10:30:45.640562, step: 853, loss: 39.0329704284668, acc: 0.8594, auc: 0.956, precision: 0.8586, recall: 0.8629\n",
      "2019-09-23T10:30:47.478646, step: 854, loss: 37.42445373535156, acc: 0.8672, auc: 0.9528, precision: 0.8724, recall: 0.8653\n",
      "2019-09-23T10:30:49.258884, step: 855, loss: 37.10261917114258, acc: 0.875, auc: 0.9548, precision: 0.8759, recall: 0.8804\n",
      "2019-09-23T10:30:51.079015, step: 856, loss: 42.34083557128906, acc: 0.8672, auc: 0.9338, precision: 0.8668, recall: 0.8681\n",
      "2019-09-23T10:30:52.862245, step: 857, loss: 38.013946533203125, acc: 0.9062, auc: 0.9599, precision: 0.9138, recall: 0.9097\n",
      "2019-09-23T10:30:54.674408, step: 858, loss: 54.64301300048828, acc: 0.8281, auc: 0.9587, precision: 0.8571, recall: 0.8493\n",
      "2019-09-23T10:30:56.478572, step: 859, loss: 56.645721435546875, acc: 0.8359, auc: 0.9407, precision: 0.8757, recall: 0.8175\n",
      "2019-09-23T10:30:58.271775, step: 860, loss: 48.85390853881836, acc: 0.8516, auc: 0.9204, precision: 0.8587, recall: 0.8516\n",
      "2019-09-23T10:31:00.065976, step: 861, loss: 40.81908416748047, acc: 0.8672, auc: 0.942, precision: 0.8663, recall: 0.8682\n",
      "2019-09-23T10:31:01.879126, step: 862, loss: 35.308380126953125, acc: 0.8828, auc: 0.9602, precision: 0.8892, recall: 0.8778\n",
      "2019-09-23T10:31:03.697262, step: 863, loss: 34.24163055419922, acc: 0.8906, auc: 0.9577, precision: 0.8907, recall: 0.8876\n",
      "2019-09-23T10:31:05.507432, step: 864, loss: 22.962100982666016, acc: 0.9219, auc: 0.9827, precision: 0.9224, recall: 0.9216\n",
      "2019-09-23T10:31:07.324560, step: 865, loss: 48.49950408935547, acc: 0.8203, auc: 0.9284, precision: 0.8203, recall: 0.8199\n",
      "2019-09-23T10:31:09.151673, step: 866, loss: 38.04928970336914, acc: 0.875, auc: 0.9455, precision: 0.877, recall: 0.8718\n",
      "2019-09-23T10:31:10.941885, step: 867, loss: 37.081207275390625, acc: 0.8672, auc: 0.948, precision: 0.8681, recall: 0.8668\n",
      "2019-09-23T10:31:12.759024, step: 868, loss: 35.31945037841797, acc: 0.8828, auc: 0.9534, precision: 0.8838, recall: 0.8824\n",
      "2019-09-23T10:31:14.536270, step: 869, loss: 31.7440185546875, acc: 0.9141, auc: 0.9687, precision: 0.9157, recall: 0.9152\n",
      "2019-09-23T10:31:16.386321, step: 870, loss: 48.50001525878906, acc: 0.8125, auc: 0.9501, precision: 0.8601, recall: 0.7978\n",
      "2019-09-23T10:31:18.182516, step: 871, loss: 67.40217590332031, acc: 0.75, auc: 0.9457, precision: 0.813, recall: 0.7669\n",
      "2019-09-23T10:31:19.991676, step: 872, loss: 51.34586715698242, acc: 0.8047, auc: 0.9436, precision: 0.8451, recall: 0.7936\n",
      "2019-09-23T10:31:21.781888, step: 873, loss: 49.36329650878906, acc: 0.8359, auc: 0.9228, precision: 0.8368, recall: 0.8355\n",
      "2019-09-23T10:31:23.588057, step: 874, loss: 40.75395202636719, acc: 0.8594, auc: 0.9397, precision: 0.8565, recall: 0.8565\n",
      "2019-09-23T10:31:25.426139, step: 875, loss: 31.934066772460938, acc: 0.9219, auc: 0.9758, precision: 0.9351, recall: 0.918\n",
      "2019-09-23T10:31:27.243296, step: 876, loss: 37.54622268676758, acc: 0.8594, auc: 0.9502, precision: 0.8629, recall: 0.8586\n",
      "2019-09-23T10:31:29.029518, step: 877, loss: 32.403106689453125, acc: 0.8906, auc: 0.9619, precision: 0.8936, recall: 0.8913\n",
      "2019-09-23T10:31:30.867601, step: 878, loss: 37.80152893066406, acc: 0.8672, auc: 0.9439, precision: 0.8707, recall: 0.8459\n",
      "2019-09-23T10:31:32.658810, step: 879, loss: 39.15349578857422, acc: 0.8438, auc: 0.9387, precision: 0.8449, recall: 0.8442\n",
      "2019-09-23T10:31:34.468968, step: 880, loss: 34.79463195800781, acc: 0.875, auc: 0.9553, precision: 0.8856, recall: 0.8724\n",
      "2019-09-23T10:31:36.273142, step: 881, loss: 31.084781646728516, acc: 0.875, auc: 0.9687, precision: 0.8755, recall: 0.8762\n",
      "2019-09-23T10:31:38.084297, step: 882, loss: 34.95423126220703, acc: 0.8828, auc: 0.9768, precision: 0.8972, recall: 0.8859\n",
      "2019-09-23T10:31:39.897447, step: 883, loss: 121.62966918945312, acc: 0.6953, auc: 0.915, precision: 0.8088, recall: 0.7\n",
      "2019-09-23T10:31:41.672699, step: 884, loss: 73.1752700805664, acc: 0.7109, auc: 0.9057, precision: 0.7855, recall: 0.7187\n",
      "2019-09-23T10:31:43.480862, step: 885, loss: 54.752166748046875, acc: 0.8359, auc: 0.9265, precision: 0.8525, recall: 0.8289\n",
      "2019-09-23T10:31:45.287030, step: 886, loss: 53.758846282958984, acc: 0.8281, auc: 0.9142, precision: 0.8304, recall: 0.8294\n",
      "2019-09-23T10:31:47.118133, step: 887, loss: 50.157562255859375, acc: 0.8203, auc: 0.9209, precision: 0.8242, recall: 0.8203\n",
      "2019-09-23T10:31:48.904369, step: 888, loss: 40.31035614013672, acc: 0.8672, auc: 0.9458, precision: 0.8701, recall: 0.8623\n",
      "2019-09-23T10:31:50.672625, step: 889, loss: 45.11088180541992, acc: 0.8438, auc: 0.9298, precision: 0.846, recall: 0.8512\n",
      "2019-09-23T10:31:52.465828, step: 890, loss: 43.300926208496094, acc: 0.8125, auc: 0.9548, precision: 0.8289, recall: 0.8212\n",
      "2019-09-23T10:31:54.288953, step: 891, loss: 36.12779235839844, acc: 0.9062, auc: 0.9642, precision: 0.9076, recall: 0.9088\n",
      "2019-09-23T10:31:56.101104, step: 892, loss: 36.7658576965332, acc: 0.8672, auc: 0.9539, precision: 0.868, recall: 0.8712\n",
      "2019-09-23T10:31:57.876356, step: 893, loss: 43.58013153076172, acc: 0.8438, auc: 0.946, precision: 0.8512, recall: 0.846\n",
      "2019-09-23T10:31:59.687510, step: 894, loss: 50.321678161621094, acc: 0.8125, auc: 0.9478, precision: 0.8489, recall: 0.8225\n",
      "2019-09-23T10:32:01.500661, step: 895, loss: 31.776227951049805, acc: 0.9219, auc: 0.9778, precision: 0.9293, recall: 0.9209\n",
      "2019-09-23T10:32:03.334755, step: 896, loss: 40.498779296875, acc: 0.8281, auc: 0.9633, precision: 0.8472, recall: 0.8336\n",
      "2019-09-23T10:32:05.157878, step: 897, loss: 39.380638122558594, acc: 0.875, auc: 0.9509, precision: 0.8798, recall: 0.8768\n",
      "2019-09-23T10:32:06.942105, step: 898, loss: 46.24431610107422, acc: 0.8672, auc: 0.9462, precision: 0.869, recall: 0.8874\n",
      "2019-09-23T10:32:08.729325, step: 899, loss: 53.585960388183594, acc: 0.8047, auc: 0.9402, precision: 0.8511, recall: 0.8018\n",
      "2019-09-23T10:32:10.540481, step: 900, loss: 56.29808807373047, acc: 0.8281, auc: 0.9084, precision: 0.875, recall: 0.8226\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:33:18.251144, step: 900, loss: 48.28625811063326, acc: 0.8281333333333334, auc: 0.9136512820512822, precision: 0.8289897435897436, recall: 0.828623076923077\n",
      "2019-09-23T10:33:20.050331, step: 901, loss: 38.38460159301758, acc: 0.875, auc: 0.9566, precision: 0.8754, recall: 0.8735\n",
      "2019-09-23T10:33:21.890409, step: 902, loss: 31.321552276611328, acc: 0.8984, auc: 0.9688, precision: 0.8998, recall: 0.896\n",
      "2019-09-23T10:33:23.727495, step: 903, loss: 34.277687072753906, acc: 0.9141, auc: 0.9654, precision: 0.9123, recall: 0.9202\n",
      "2019-09-23T10:33:25.578544, step: 904, loss: 39.13483428955078, acc: 0.8516, auc: 0.9754, precision: 0.8834, recall: 0.8402\n",
      "2019-09-23T10:33:27.377731, step: 905, loss: 53.49946594238281, acc: 0.8281, auc: 0.9323, precision: 0.8515, recall: 0.824\n",
      "2019-09-23T10:33:29.218807, step: 906, loss: 41.2176513671875, acc: 0.8359, auc: 0.9427, precision: 0.8406, recall: 0.833\n",
      "2019-09-23T10:33:31.019988, step: 907, loss: 35.47386169433594, acc: 0.8672, auc: 0.9534, precision: 0.8669, recall: 0.8611\n",
      "2019-09-23T10:33:32.801230, step: 908, loss: 47.7188720703125, acc: 0.8359, auc: 0.9127, precision: 0.8352, recall: 0.8358\n",
      "2019-09-23T10:33:34.610384, step: 909, loss: 40.544410705566406, acc: 0.8516, auc: 0.9345, precision: 0.8495, recall: 0.8506\n",
      "2019-09-23T10:33:36.372671, step: 910, loss: 33.56925582885742, acc: 0.875, auc: 0.9607, precision: 0.8754, recall: 0.875\n",
      "2019-09-23T10:33:38.162883, step: 911, loss: 36.42534637451172, acc: 0.8984, auc: 0.9534, precision: 0.8964, recall: 0.9018\n",
      "2019-09-23T10:33:39.961786, step: 912, loss: 47.1829833984375, acc: 0.8438, auc: 0.9559, precision: 0.8623, recall: 0.851\n",
      "2019-09-23T10:33:41.775932, step: 913, loss: 72.46156311035156, acc: 0.6953, auc: 0.9396, precision: 0.798, recall: 0.6707\n",
      "2019-09-23T10:33:43.647926, step: 914, loss: 48.70215606689453, acc: 0.8516, auc: 0.9413, precision: 0.8688, recall: 0.8465\n",
      "2019-09-23T10:33:45.489998, step: 915, loss: 35.27655792236328, acc: 0.8984, auc: 0.961, precision: 0.9023, recall: 0.8929\n",
      "2019-09-23T10:33:47.293174, step: 916, loss: 37.18807601928711, acc: 0.8828, auc: 0.9484, precision: 0.884, recall: 0.8815\n",
      "2019-09-23T10:33:49.084384, step: 917, loss: 36.328643798828125, acc: 0.8594, auc: 0.9507, precision: 0.8596, recall: 0.8596\n",
      "2019-09-23T10:33:50.887560, step: 918, loss: 37.7116584777832, acc: 0.875, auc: 0.9502, precision: 0.8767, recall: 0.8745\n",
      "2019-09-23T10:33:52.678773, step: 919, loss: 49.697731018066406, acc: 0.8281, auc: 0.9213, precision: 0.8313, recall: 0.8265\n",
      "2019-09-23T10:33:54.491919, step: 920, loss: 33.863014221191406, acc: 0.8828, auc: 0.9534, precision: 0.8827, recall: 0.8819\n",
      "2019-09-23T10:33:56.621223, step: 921, loss: 47.91196060180664, acc: 0.8516, auc: 0.9287, precision: 0.8522, recall: 0.8519\n",
      "2019-09-23T10:33:58.402458, step: 922, loss: 45.10160827636719, acc: 0.8047, auc: 0.9551, precision: 0.831, recall: 0.8224\n",
      "2019-09-23T10:34:00.218612, step: 923, loss: 59.798988342285156, acc: 0.7344, auc: 0.9404, precision: 0.8004, recall: 0.7344\n",
      "2019-09-23T10:34:02.064662, step: 924, loss: 50.41348648071289, acc: 0.8125, auc: 0.914, precision: 0.8243, recall: 0.811\n",
      "2019-09-23T10:34:03.874820, step: 925, loss: 42.42926025390625, acc: 0.8672, auc: 0.9375, precision: 0.8681, recall: 0.8668\n",
      "2019-09-23T10:34:05.656056, step: 926, loss: 43.50939178466797, acc: 0.8516, auc: 0.9419, precision: 0.8629, recall: 0.8502\n",
      "2019-09-23T10:34:07.446275, step: 927, loss: 46.82823181152344, acc: 0.8203, auc: 0.9541, precision: 0.8462, recall: 0.816\n",
      "2019-09-23T10:34:09.251438, step: 928, loss: 43.2497673034668, acc: 0.8594, auc: 0.959, precision: 0.8822, recall: 0.8613\n",
      "2019-09-23T10:34:11.059602, step: 929, loss: 43.209312438964844, acc: 0.8516, auc: 0.9306, precision: 0.8497, recall: 0.8482\n",
      "2019-09-23T10:34:12.856794, step: 930, loss: 36.424598693847656, acc: 0.8828, auc: 0.955, precision: 0.8839, recall: 0.8803\n",
      "2019-09-23T10:34:14.720809, step: 931, loss: 33.26893615722656, acc: 0.8828, auc: 0.9591, precision: 0.8823, recall: 0.881\n",
      "2019-09-23T10:34:16.611750, step: 932, loss: 38.5853271484375, acc: 0.8828, auc: 0.9451, precision: 0.8753, recall: 0.8895\n",
      "2019-09-23T10:34:18.467792, step: 933, loss: 40.801326751708984, acc: 0.8125, auc: 0.9612, precision: 0.8536, recall: 0.7828\n",
      "2019-09-23T10:34:20.267971, step: 934, loss: 66.4312744140625, acc: 0.7109, auc: 0.9234, precision: 0.765, recall: 0.6914\n",
      "2019-09-23T10:34:22.086108, step: 935, loss: 44.3515625, acc: 0.8281, auc: 0.9596, precision: 0.8682, recall: 0.8176\n",
      "2019-09-23T10:34:23.933166, step: 936, loss: 43.578369140625, acc: 0.8594, auc: 0.9331, precision: 0.8482, recall: 0.8583\n",
      "start to train models...\n",
      "2019-09-23T10:34:25.869986, step: 937, loss: 35.1019401550293, acc: 0.8906, auc: 0.9673, precision: 0.9018, recall: 0.8881\n",
      "2019-09-23T10:34:27.670170, step: 938, loss: 29.89575958251953, acc: 0.9141, auc: 0.9665, precision: 0.9155, recall: 0.9128\n",
      "2019-09-23T10:34:29.449411, step: 939, loss: 31.427513122558594, acc: 0.9062, auc: 0.9784, precision: 0.907, recall: 0.9094\n",
      "2019-09-23T10:34:31.280513, step: 940, loss: 35.84504699707031, acc: 0.8594, auc: 0.9713, precision: 0.8734, recall: 0.8657\n",
      "2019-09-23T10:34:33.063743, step: 941, loss: 50.65068054199219, acc: 0.8438, auc: 0.9575, precision: 0.8936, recall: 0.8148\n",
      "2019-09-23T10:34:34.854953, step: 942, loss: 46.48780822753906, acc: 0.8047, auc: 0.971, precision: 0.8344, recall: 0.8286\n",
      "2019-09-23T10:34:36.672091, step: 943, loss: 36.60559844970703, acc: 0.875, auc: 0.9608, precision: 0.8754, recall: 0.8735\n",
      "2019-09-23T10:34:38.512170, step: 944, loss: 33.988990783691406, acc: 0.8828, auc: 0.9631, precision: 0.8825, recall: 0.8829\n",
      "2019-09-23T10:34:40.336290, step: 945, loss: 25.421260833740234, acc: 0.9219, auc: 0.9865, precision: 0.9246, recall: 0.9196\n",
      "2019-09-23T10:34:42.142458, step: 946, loss: 20.258798599243164, acc: 0.9375, auc: 0.9894, precision: 0.9381, recall: 0.9328\n",
      "2019-09-23T10:34:43.941645, step: 947, loss: 26.330860137939453, acc: 0.9141, auc: 0.9758, precision: 0.9141, recall: 0.915\n",
      "2019-09-23T10:34:45.748818, step: 948, loss: 28.89114761352539, acc: 0.8984, auc: 0.9775, precision: 0.9033, recall: 0.8984\n",
      "2019-09-23T10:34:47.550991, step: 949, loss: 72.55574035644531, acc: 0.8047, auc: 0.9471, precision: 0.8377, recall: 0.8355\n",
      "2019-09-23T10:34:49.384087, step: 950, loss: 65.53837585449219, acc: 0.7578, auc: 0.9153, precision: 0.7974, recall: 0.7488\n",
      "2019-09-23T10:34:51.228155, step: 951, loss: 47.11476135253906, acc: 0.8359, auc: 0.9399, precision: 0.8403, recall: 0.8481\n",
      "2019-09-23T10:34:53.055269, step: 952, loss: 43.45569610595703, acc: 0.8594, auc: 0.9416, precision: 0.8634, recall: 0.8537\n",
      "2019-09-23T10:34:54.873404, step: 953, loss: 32.4973258972168, acc: 0.9219, auc: 0.9645, precision: 0.9197, recall: 0.9226\n",
      "2019-09-23T10:34:56.662618, step: 954, loss: 27.737789154052734, acc: 0.9297, auc: 0.9817, precision: 0.9361, recall: 0.9279\n",
      "2019-09-23T10:34:58.442856, step: 955, loss: 29.94420623779297, acc: 0.9219, auc: 0.9665, precision: 0.9241, recall: 0.9208\n",
      "2019-09-23T10:35:00.237057, step: 956, loss: 31.960861206054688, acc: 0.9141, auc: 0.9676, precision: 0.9128, recall: 0.9173\n",
      "2019-09-23T10:35:02.011311, step: 957, loss: 53.68168258666992, acc: 0.7891, auc: 0.9578, precision: 0.8267, recall: 0.7779\n",
      "2019-09-23T10:35:03.809501, step: 958, loss: 69.73306274414062, acc: 0.7578, auc: 0.9601, precision: 0.8258, recall: 0.7786\n",
      "2019-09-23T10:35:05.660549, step: 959, loss: 32.713706970214844, acc: 0.9141, auc: 0.9741, precision: 0.9152, recall: 0.9137\n",
      "2019-09-23T10:35:07.504618, step: 960, loss: 38.60237121582031, acc: 0.875, auc: 0.9505, precision: 0.8739, recall: 0.8768\n",
      "2019-09-23T10:35:09.450412, step: 961, loss: 33.40470886230469, acc: 0.8828, auc: 0.9608, precision: 0.8822, recall: 0.8855\n",
      "2019-09-23T10:35:11.277526, step: 962, loss: 45.81060791015625, acc: 0.8906, auc: 0.9539, precision: 0.8941, recall: 0.8941\n",
      "2019-09-23T10:35:13.092670, step: 963, loss: 33.70917892456055, acc: 0.9062, auc: 0.9807, precision: 0.9172, recall: 0.905\n",
      "2019-09-23T10:35:14.909810, step: 964, loss: 43.11687469482422, acc: 0.8516, auc: 0.9568, precision: 0.8627, recall: 0.8641\n",
      "2019-09-23T10:35:16.681071, step: 965, loss: 37.6163330078125, acc: 0.8828, auc: 0.9659, precision: 0.8979, recall: 0.8753\n",
      "2019-09-23T10:35:18.480260, step: 966, loss: 26.23531723022461, acc: 0.9219, auc: 0.9787, precision: 0.9241, recall: 0.9208\n",
      "2019-09-23T10:35:20.291415, step: 967, loss: 31.20855712890625, acc: 0.8828, auc: 0.9659, precision: 0.8827, recall: 0.8819\n",
      "2019-09-23T10:35:22.079631, step: 968, loss: 32.86138153076172, acc: 0.8906, auc: 0.9642, precision: 0.8923, recall: 0.8829\n",
      "2019-09-23T10:35:23.931677, step: 969, loss: 32.22614288330078, acc: 0.8828, auc: 0.9638, precision: 0.8894, recall: 0.8766\n",
      "2019-09-23T10:35:25.773750, step: 970, loss: 30.87458038330078, acc: 0.9219, auc: 0.9692, precision: 0.9203, recall: 0.9266\n",
      "2019-09-23T10:35:27.562963, step: 971, loss: 40.955230712890625, acc: 0.875, auc: 0.9653, precision: 0.8958, recall: 0.8715\n",
      "2019-09-23T10:35:29.342203, step: 972, loss: 33.590065002441406, acc: 0.8984, auc: 0.989, precision: 0.9044, recall: 0.911\n",
      "2019-09-23T10:35:31.160341, step: 973, loss: 35.26922607421875, acc: 0.8906, auc: 0.9674, precision: 0.9083, recall: 0.8843\n",
      "2019-09-23T10:35:32.977480, step: 974, loss: 26.335956573486328, acc: 0.9453, auc: 0.9858, precision: 0.9475, recall: 0.9459\n",
      "2019-09-23T10:35:34.768688, step: 975, loss: 33.136260986328125, acc: 0.8906, auc: 0.9851, precision: 0.9058, recall: 0.8891\n",
      "2019-09-23T10:35:36.574858, step: 976, loss: 54.77250671386719, acc: 0.8281, auc: 0.9212, precision: 0.842, recall: 0.8157\n",
      "2019-09-23T10:35:38.353100, step: 977, loss: 33.539146423339844, acc: 0.8906, auc: 0.9893, precision: 0.9049, recall: 0.8906\n",
      "2019-09-23T10:35:40.173233, step: 978, loss: 52.77263641357422, acc: 0.8672, auc: 0.9309, precision: 0.8821, recall: 0.8687\n",
      "2019-09-23T10:35:41.977416, step: 979, loss: 36.06193161010742, acc: 0.8906, auc: 0.9738, precision: 0.8876, recall: 0.9067\n",
      "2019-09-23T10:35:43.762631, step: 980, loss: 27.580589294433594, acc: 0.9062, auc: 0.9803, precision: 0.9124, recall: 0.8976\n",
      "2019-09-23T10:35:45.583760, step: 981, loss: 34.30174255371094, acc: 0.8906, auc: 0.9609, precision: 0.8882, recall: 0.8951\n",
      "2019-09-23T10:35:47.390129, step: 982, loss: 24.744050979614258, acc: 0.9141, auc: 0.9792, precision: 0.9152, recall: 0.9137\n",
      "2019-09-23T10:35:49.198293, step: 983, loss: 32.90392303466797, acc: 0.8828, auc: 0.9622, precision: 0.882, recall: 0.8839\n",
      "2019-09-23T10:35:50.993491, step: 984, loss: 31.505277633666992, acc: 0.9062, auc: 0.9704, precision: 0.9084, recall: 0.9052\n",
      "2019-09-23T10:35:52.821600, step: 985, loss: 29.60969352722168, acc: 0.8828, auc: 0.9724, precision: 0.8896, recall: 0.8726\n",
      "2019-09-23T10:35:54.667662, step: 986, loss: 22.825363159179688, acc: 0.9141, auc: 0.9924, precision: 0.9253, recall: 0.9093\n",
      "2019-09-23T10:35:56.491784, step: 987, loss: 32.51380157470703, acc: 0.875, auc: 0.974, precision: 0.8917, recall: 0.8686\n",
      "2019-09-23T10:35:58.310917, step: 988, loss: 33.739688873291016, acc: 0.8984, auc: 0.9698, precision: 0.9202, recall: 0.8712\n",
      "2019-09-23T10:36:00.102126, step: 989, loss: 36.65928649902344, acc: 0.875, auc: 0.9685, precision: 0.8928, recall: 0.8767\n",
      "2019-09-23T10:36:01.914279, step: 990, loss: 24.994686126708984, acc: 0.9219, auc: 0.9819, precision: 0.9239, recall: 0.9214\n",
      "2019-09-23T10:36:03.726432, step: 991, loss: 31.09929847717285, acc: 0.8984, auc: 0.9607, precision: 0.8891, recall: 0.8979\n",
      "2019-09-23T10:36:05.556537, step: 992, loss: 33.21413803100586, acc: 0.875, auc: 0.9762, precision: 0.8871, recall: 0.8681\n",
      "2019-09-23T10:36:07.356721, step: 993, loss: 48.310298919677734, acc: 0.8047, auc: 0.9834, precision: 0.8611, recall: 0.8016\n",
      "2019-09-23T10:36:09.145935, step: 994, loss: 52.344154357910156, acc: 0.8203, auc: 0.9566, precision: 0.8536, recall: 0.8299\n",
      "2019-09-23T10:36:10.930174, step: 995, loss: 33.2509651184082, acc: 0.8906, auc: 0.9726, precision: 0.9058, recall: 0.8891\n",
      "2019-09-23T10:36:12.750293, step: 996, loss: 27.908838272094727, acc: 0.9062, auc: 0.9761, precision: 0.9078, recall: 0.9062\n",
      "2019-09-23T10:36:14.559455, step: 997, loss: 30.08838653564453, acc: 0.9141, auc: 0.9668, precision: 0.9144, recall: 0.9147\n",
      "2019-09-23T10:36:16.401528, step: 998, loss: 37.073429107666016, acc: 0.8906, auc: 0.9516, precision: 0.8908, recall: 0.8908\n",
      "2019-09-23T10:36:18.175781, step: 999, loss: 35.70170593261719, acc: 0.875, auc: 0.9528, precision: 0.875, recall: 0.8754\n",
      "2019-09-23T10:36:19.972974, step: 1000, loss: 22.944711685180664, acc: 0.9297, auc: 0.9865, precision: 0.9298, recall: 0.9286\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:37:27.427946, step: 1000, loss: 57.020918430426185, acc: 0.836348717948718, auc: 0.9197538461538459, precision: 0.8401307692307693, recall: 0.8361025641025642\n",
      "2019-09-23T10:37:29.211176, step: 1001, loss: 46.360477447509766, acc: 0.8672, auc: 0.9393, precision: 0.8701, recall: 0.8633\n",
      "2019-09-23T10:37:30.997399, step: 1002, loss: 30.356075286865234, acc: 0.9141, auc: 0.9897, precision: 0.9203, recall: 0.9214\n",
      "2019-09-23T10:37:32.773657, step: 1003, loss: 43.20967102050781, acc: 0.8672, auc: 0.9537, precision: 0.898, recall: 0.8424\n",
      "2019-09-23T10:37:34.616717, step: 1004, loss: 27.843990325927734, acc: 0.9141, auc: 0.9822, precision: 0.9224, recall: 0.9141\n",
      "2019-09-23T10:37:36.451807, step: 1005, loss: 38.667579650878906, acc: 0.8906, auc: 0.9457, precision: 0.8904, recall: 0.8911\n",
      "2019-09-23T10:37:38.230052, step: 1006, loss: 33.77900695800781, acc: 0.9219, auc: 0.9568, precision: 0.9223, recall: 0.9219\n",
      "2019-09-23T10:37:40.049185, step: 1007, loss: 25.018375396728516, acc: 0.9141, auc: 0.9809, precision: 0.9147, recall: 0.9162\n",
      "2019-09-23T10:37:41.845381, step: 1008, loss: 25.76288604736328, acc: 0.875, auc: 0.9792, precision: 0.8752, recall: 0.8752\n",
      "2019-09-23T10:37:43.660525, step: 1009, loss: 35.28932189941406, acc: 0.8984, auc: 0.969, precision: 0.9072, recall: 0.8973\n",
      "2019-09-23T10:37:45.471687, step: 1010, loss: 63.163673400878906, acc: 0.7656, auc: 0.9692, precision: 0.837, recall: 0.7727\n",
      "2019-09-23T10:37:47.281839, step: 1011, loss: 63.64817810058594, acc: 0.7188, auc: 0.9698, precision: 0.8105, recall: 0.7391\n",
      "2019-09-23T10:37:49.099976, step: 1012, loss: 47.04381561279297, acc: 0.8906, auc: 0.9231, precision: 0.8922, recall: 0.8906\n",
      "2019-09-23T10:37:50.875227, step: 1013, loss: 25.521808624267578, acc: 0.9297, auc: 0.9875, precision: 0.9342, recall: 0.9262\n",
      "2019-09-23T10:37:52.691369, step: 1014, loss: 33.754520416259766, acc: 0.9062, auc: 0.9614, precision: 0.9099, recall: 0.9062\n",
      "2019-09-23T10:37:54.502524, step: 1015, loss: 39.762535095214844, acc: 0.875, auc: 0.953, precision: 0.8821, recall: 0.8784\n",
      "2019-09-23T10:37:56.312683, step: 1016, loss: 26.064237594604492, acc: 0.9062, auc: 0.9845, precision: 0.909, recall: 0.9106\n",
      "2019-09-23T10:37:58.101897, step: 1017, loss: 36.51780319213867, acc: 0.8516, auc: 0.9572, precision: 0.8716, recall: 0.8291\n",
      "2019-09-23T10:37:59.898091, step: 1018, loss: 42.59431838989258, acc: 0.8828, auc: 0.9405, precision: 0.8886, recall: 0.88\n",
      "2019-09-23T10:38:01.720218, step: 1019, loss: 35.987186431884766, acc: 0.8672, auc: 0.9562, precision: 0.8665, recall: 0.8697\n",
      "2019-09-23T10:38:03.573261, step: 1020, loss: 31.544612884521484, acc: 0.9375, auc: 0.9745, precision: 0.9385, recall: 0.935\n",
      "2019-09-23T10:38:05.418326, step: 1021, loss: 31.818965911865234, acc: 0.8828, auc: 0.9638, precision: 0.8859, recall: 0.8807\n",
      "2019-09-23T10:38:07.245439, step: 1022, loss: 36.134063720703125, acc: 0.875, auc: 0.965, precision: 0.8828, recall: 0.8828\n",
      "2019-09-23T10:38:09.049612, step: 1023, loss: 39.653079986572266, acc: 0.8672, auc: 0.976, precision: 0.88, recall: 0.8786\n",
      "2019-09-23T10:38:10.855782, step: 1024, loss: 36.98358917236328, acc: 0.8672, auc: 0.9684, precision: 0.8826, recall: 0.8522\n",
      "2019-09-23T10:38:12.656963, step: 1025, loss: 33.09130859375, acc: 0.8984, auc: 0.9712, precision: 0.9106, recall: 0.8984\n",
      "2019-09-23T10:38:14.460140, step: 1026, loss: 31.076053619384766, acc: 0.9141, auc: 0.9816, precision: 0.9181, recall: 0.9191\n",
      "2019-09-23T10:38:16.257332, step: 1027, loss: 39.16975784301758, acc: 0.875, auc: 0.9739, precision: 0.9, recall: 0.875\n",
      "2019-09-23T10:38:18.057518, step: 1028, loss: 45.38710021972656, acc: 0.8672, auc: 0.946, precision: 0.8838, recall: 0.8656\n",
      "2019-09-23T10:38:19.843740, step: 1029, loss: 37.443641662597656, acc: 0.8516, auc: 0.9578, precision: 0.8652, recall: 0.8383\n",
      "2019-09-23T10:38:21.638937, step: 1030, loss: 33.480979919433594, acc: 0.8906, auc: 0.9597, precision: 0.8876, recall: 0.8907\n",
      "2019-09-23T10:38:23.443111, step: 1031, loss: 41.96235275268555, acc: 0.8125, auc: 0.9403, precision: 0.8049, recall: 0.8174\n",
      "2019-09-23T10:38:25.259253, step: 1032, loss: 47.60877227783203, acc: 0.875, auc: 0.9551, precision: 0.8894, recall: 0.8814\n",
      "2019-09-23T10:38:27.088361, step: 1033, loss: 43.429080963134766, acc: 0.8359, auc: 0.9578, precision: 0.8614, recall: 0.8359\n",
      "2019-09-23T10:38:28.883559, step: 1034, loss: 38.08012008666992, acc: 0.875, auc: 0.9469, precision: 0.8742, recall: 0.8742\n",
      "2019-09-23T10:38:30.686735, step: 1035, loss: 29.840946197509766, acc: 0.9062, auc: 0.9736, precision: 0.906, recall: 0.9068\n",
      "2019-09-23T10:38:32.524819, step: 1036, loss: 46.19116973876953, acc: 0.8828, auc: 0.9493, precision: 0.8822, recall: 0.8855\n",
      "2019-09-23T10:38:34.354923, step: 1037, loss: 25.664262771606445, acc: 0.9141, auc: 0.9873, precision: 0.9179, recall: 0.9113\n",
      "2019-09-23T10:38:36.155109, step: 1038, loss: 30.977636337280273, acc: 0.9062, auc: 0.967, precision: 0.9052, recall: 0.9084\n",
      "2019-09-23T10:38:37.935346, step: 1039, loss: 21.14163589477539, acc: 0.9297, auc: 0.9936, precision: 0.9366, recall: 0.9318\n",
      "2019-09-23T10:38:39.741514, step: 1040, loss: 37.72740936279297, acc: 0.8828, auc: 0.9954, precision: 0.9026, recall: 0.8864\n",
      "2019-09-23T10:38:41.548681, step: 1041, loss: 64.49262237548828, acc: 0.7656, auc: 0.9659, precision: 0.8485, recall: 0.7458\n",
      "2019-09-23T10:38:43.361830, step: 1042, loss: 51.397491455078125, acc: 0.7969, auc: 0.9218, precision: 0.8125, recall: 0.7933\n",
      "2019-09-23T10:38:45.165008, step: 1043, loss: 34.376502990722656, acc: 0.8516, auc: 0.9539, precision: 0.8506, recall: 0.8495\n",
      "2019-09-23T10:38:46.977160, step: 1044, loss: 33.68429946899414, acc: 0.875, auc: 0.9593, precision: 0.8754, recall: 0.8735\n",
      "2019-09-23T10:38:48.772358, step: 1045, loss: 25.13494110107422, acc: 0.9531, auc: 0.9799, precision: 0.9543, recall: 0.9516\n",
      "2019-09-23T10:38:50.568554, step: 1046, loss: 29.52711296081543, acc: 0.9141, auc: 0.9722, precision: 0.9139, recall: 0.9126\n",
      "2019-09-23T10:38:52.400653, step: 1047, loss: 26.99842071533203, acc: 0.9219, auc: 0.979, precision: 0.9202, recall: 0.9244\n",
      "2019-09-23T10:38:54.235756, step: 1048, loss: 35.09492874145508, acc: 0.8984, auc: 0.9732, precision: 0.9146, recall: 0.8911\n",
      "2019-09-23T10:38:56.045902, step: 1049, loss: 36.501548767089844, acc: 0.8984, auc: 0.9654, precision: 0.9278, recall: 0.8725\n",
      "2019-09-23T10:38:57.853067, step: 1050, loss: 43.304710388183594, acc: 0.8359, auc: 0.9431, precision: 0.8306, recall: 0.8406\n",
      "2019-09-23T10:38:59.658239, step: 1051, loss: 29.486709594726562, acc: 0.8906, auc: 0.9865, precision: 0.8985, recall: 0.8985\n",
      "2019-09-23T10:39:01.468398, step: 1052, loss: 25.63024139404297, acc: 0.8984, auc: 0.978, precision: 0.8988, recall: 0.8991\n",
      "2019-09-23T10:39:03.277558, step: 1053, loss: 42.97994613647461, acc: 0.8906, auc: 0.9473, precision: 0.8951, recall: 0.897\n",
      "2019-09-23T10:39:05.094698, step: 1054, loss: 55.81562805175781, acc: 0.8203, auc: 0.9536, precision: 0.8577, recall: 0.8228\n",
      "2019-09-23T10:39:06.890893, step: 1055, loss: 30.561464309692383, acc: 0.9141, auc: 0.9951, precision: 0.9236, recall: 0.9179\n",
      "2019-09-23T10:39:08.670133, step: 1056, loss: 33.08319091796875, acc: 0.9297, auc: 0.959, precision: 0.9298, recall: 0.9286\n",
      "2019-09-23T10:39:10.481289, step: 1057, loss: 30.649961471557617, acc: 0.875, auc: 0.98, precision: 0.8878, recall: 0.8764\n",
      "2019-09-23T10:39:12.333336, step: 1058, loss: 29.99028205871582, acc: 0.8906, auc: 0.9735, precision: 0.9031, recall: 0.8853\n",
      "2019-09-23T10:39:14.271152, step: 1059, loss: 39.052223205566406, acc: 0.8594, auc: 0.957, precision: 0.8677, recall: 0.8606\n",
      "2019-09-23T10:39:16.241880, step: 1060, loss: 34.04077911376953, acc: 0.8594, auc: 0.9727, precision: 0.8647, recall: 0.8647\n",
      "2019-09-23T10:39:18.099911, step: 1061, loss: 34.13970947265625, acc: 0.875, auc: 0.965, precision: 0.8929, recall: 0.865\n",
      "2019-09-23T10:39:20.102553, step: 1062, loss: 35.22791290283203, acc: 0.875, auc: 0.961, precision: 0.8737, recall: 0.885\n",
      "2019-09-23T10:39:22.237842, step: 1063, loss: 34.32018280029297, acc: 0.8516, auc: 0.9553, precision: 0.8537, recall: 0.8454\n",
      "2019-09-23T10:39:24.435962, step: 1064, loss: 26.94316864013672, acc: 0.8984, auc: 0.9739, precision: 0.8986, recall: 0.8983\n",
      "2019-09-23T10:39:26.641063, step: 1065, loss: 31.210494995117188, acc: 0.8828, auc: 0.9654, precision: 0.8819, recall: 0.8827\n",
      "2019-09-23T10:39:28.791312, step: 1066, loss: 30.841520309448242, acc: 0.9062, auc: 0.9721, precision: 0.9044, recall: 0.9068\n",
      "2019-09-23T10:39:30.861774, step: 1067, loss: 74.79362487792969, acc: 0.7422, auc: 0.9628, precision: 0.7834, recall: 0.7813\n",
      "2019-09-23T10:39:32.881372, step: 1068, loss: 62.40169143676758, acc: 0.7969, auc: 0.9081, precision: 0.8571, recall: 0.753\n",
      "2019-09-23T10:39:34.817192, step: 1069, loss: 47.16716766357422, acc: 0.8594, auc: 0.9443, precision: 0.8609, recall: 0.8552\n",
      "2019-09-23T10:39:36.815846, step: 1070, loss: 45.34193420410156, acc: 0.8516, auc: 0.9426, precision: 0.8575, recall: 0.8536\n",
      "2019-09-23T10:39:38.765631, step: 1071, loss: 43.29644012451172, acc: 0.8516, auc: 0.9302, precision: 0.8487, recall: 0.8566\n",
      "2019-09-23T10:39:40.579727, step: 1072, loss: 33.31047058105469, acc: 0.9141, auc: 0.9579, precision: 0.9104, recall: 0.9071\n",
      "2019-09-23T10:39:42.491537, step: 1073, loss: 27.990209579467773, acc: 0.9141, auc: 0.9839, precision: 0.9185, recall: 0.9087\n",
      "2019-09-23T10:39:44.429685, step: 1074, loss: 37.8968620300293, acc: 0.8594, auc: 0.9614, precision: 0.8732, recall: 0.8579\n",
      "2019-09-23T10:39:46.606136, step: 1075, loss: 46.84440612792969, acc: 0.875, auc: 0.9431, precision: 0.8837, recall: 0.8762\n",
      "2019-09-23T10:39:49.233271, step: 1076, loss: 34.15928268432617, acc: 0.875, auc: 0.9653, precision: 0.8939, recall: 0.8611\n",
      "2019-09-23T10:39:51.552945, step: 1077, loss: 33.72055435180664, acc: 0.8828, auc: 0.9573, precision: 0.8816, recall: 0.88\n",
      "2019-09-23T10:39:53.693136, step: 1078, loss: 25.858421325683594, acc: 0.8984, auc: 0.9782, precision: 0.8996, recall: 0.9\n",
      "2019-09-23T10:39:55.639397, step: 1079, loss: 30.84268569946289, acc: 0.8984, auc: 0.969, precision: 0.9027, recall: 0.8993\n",
      "2019-09-23T10:39:57.577579, step: 1080, loss: 26.68679428100586, acc: 0.9219, auc: 0.9773, precision: 0.9223, recall: 0.9181\n",
      "2019-09-23T10:39:59.560513, step: 1081, loss: 29.429176330566406, acc: 0.9219, auc: 0.9669, precision: 0.9202, recall: 0.9227\n",
      "2019-09-23T10:40:01.386396, step: 1082, loss: 29.525188446044922, acc: 0.9062, auc: 0.9648, precision: 0.906, recall: 0.906\n",
      "2019-09-23T10:40:03.379981, step: 1083, loss: 31.741947174072266, acc: 0.9219, auc: 0.9697, precision: 0.9256, recall: 0.9219\n",
      "2019-09-23T10:40:05.392442, step: 1084, loss: 37.83274841308594, acc: 0.8672, auc: 0.9747, precision: 0.8934, recall: 0.8572\n",
      "2019-09-23T10:40:07.391953, step: 1085, loss: 56.281494140625, acc: 0.8047, auc: 0.952, precision: 0.8458, recall: 0.8127\n",
      "2019-09-23T10:40:09.392065, step: 1086, loss: 51.461944580078125, acc: 0.8125, auc: 0.9511, precision: 0.8568, recall: 0.8069\n",
      "2019-09-23T10:40:11.182531, step: 1087, loss: 43.81153106689453, acc: 0.8516, auc: 0.9456, precision: 0.8599, recall: 0.861\n",
      "2019-09-23T10:40:13.165302, step: 1088, loss: 31.81745147705078, acc: 0.9141, auc: 0.9633, precision: 0.9184, recall: 0.9096\n",
      "2019-09-23T10:40:14.979731, step: 1089, loss: 35.05417251586914, acc: 0.8672, auc: 0.9567, precision: 0.8638, recall: 0.8679\n",
      "2019-09-23T10:40:16.831442, step: 1090, loss: 27.0894832611084, acc: 0.8984, auc: 0.9731, precision: 0.8968, recall: 0.8981\n",
      "2019-09-23T10:40:18.754394, step: 1091, loss: 34.9093017578125, acc: 0.8984, auc: 0.9564, precision: 0.8984, recall: 0.8975\n",
      "2019-09-23T10:40:20.586266, step: 1092, loss: 38.35533142089844, acc: 0.8828, auc: 0.9452, precision: 0.8828, recall: 0.8822\n",
      "start to train models...\n",
      "2019-09-23T10:40:22.728275, step: 1093, loss: 20.59695053100586, acc: 0.9453, auc: 0.9882, precision: 0.9456, recall: 0.9444\n",
      "2019-09-23T10:40:24.638012, step: 1094, loss: 17.489559173583984, acc: 0.9453, auc: 0.9909, precision: 0.9444, recall: 0.9456\n",
      "2019-09-23T10:40:26.445023, step: 1095, loss: 13.192293167114258, acc: 0.9844, auc: 0.9912, precision: 0.9839, recall: 0.9853\n",
      "2019-09-23T10:40:28.432069, step: 1096, loss: 24.009960174560547, acc: 0.9141, auc: 0.981, precision: 0.9162, recall: 0.9147\n",
      "2019-09-23T10:40:30.254629, step: 1097, loss: 29.40265464782715, acc: 0.8906, auc: 0.9912, precision: 0.9039, recall: 0.8921\n",
      "2019-09-23T10:40:32.066655, step: 1098, loss: 48.931278228759766, acc: 0.8281, auc: 0.9763, precision: 0.8764, recall: 0.8197\n",
      "2019-09-23T10:40:34.074800, step: 1099, loss: 47.69245147705078, acc: 0.8203, auc: 0.9537, precision: 0.8296, recall: 0.8396\n",
      "2019-09-23T10:40:35.894210, step: 1100, loss: 40.29289245605469, acc: 0.8359, auc: 0.9709, precision: 0.8659, recall: 0.8404\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:41:46.517345, step: 1100, loss: 54.418795463366386, acc: 0.8152948717948717, auc: 0.9188051282051285, precision: 0.8256256410256412, recall: 0.8159128205128203\n",
      "2019-09-23T10:41:48.517457, step: 1101, loss: 37.26060104370117, acc: 0.8906, auc: 0.9629, precision: 0.8974, recall: 0.8896\n",
      "2019-09-23T10:41:50.293887, step: 1102, loss: 23.727209091186523, acc: 0.9062, auc: 0.9904, precision: 0.9163, recall: 0.8982\n",
      "2019-09-23T10:41:52.128601, step: 1103, loss: 25.00463104248047, acc: 0.8984, auc: 0.9832, precision: 0.9038, recall: 0.8976\n",
      "2019-09-23T10:41:54.130094, step: 1104, loss: 27.716732025146484, acc: 0.9062, auc: 0.9734, precision: 0.9068, recall: 0.906\n",
      "2019-09-23T10:41:55.961559, step: 1105, loss: 12.311279296875, acc: 0.9766, auc: 0.9946, precision: 0.9767, recall: 0.9766\n",
      "2019-09-23T10:41:57.910059, step: 1106, loss: 22.92139434814453, acc: 0.9297, auc: 0.9842, precision: 0.9236, recall: 0.9349\n",
      "2019-09-23T10:42:00.051060, step: 1107, loss: 43.7758674621582, acc: 0.8438, auc: 0.9785, precision: 0.8804, recall: 0.8263\n",
      "2019-09-23T10:42:02.009271, step: 1108, loss: 74.27240753173828, acc: 0.7188, auc: 0.9668, precision: 0.8105, recall: 0.7391\n",
      "2019-09-23T10:42:04.124013, step: 1109, loss: 62.08269500732422, acc: 0.6953, auc: 0.9234, precision: 0.776, recall: 0.7075\n",
      "2019-09-23T10:42:06.138986, step: 1110, loss: 26.66523551940918, acc: 0.9375, auc: 0.9821, precision: 0.937, recall: 0.9382\n",
      "2019-09-23T10:42:08.492452, step: 1111, loss: 28.463157653808594, acc: 0.9062, auc: 0.9768, precision: 0.9106, recall: 0.909\n",
      "2019-09-23T10:42:11.170133, step: 1112, loss: 20.653234481811523, acc: 0.9375, auc: 0.9849, precision: 0.9451, recall: 0.9306\n",
      "2019-09-23T10:42:13.789855, step: 1113, loss: 21.843523025512695, acc: 0.9453, auc: 0.9826, precision: 0.9445, recall: 0.9468\n",
      "2019-09-23T10:42:16.086315, step: 1114, loss: 47.933040618896484, acc: 0.8672, auc: 0.9475, precision: 0.8719, recall: 0.8711\n",
      "2019-09-23T10:42:18.718169, step: 1115, loss: 29.4466495513916, acc: 0.8984, auc: 0.9932, precision: 0.9015, recall: 0.9133\n",
      "2019-09-23T10:42:21.067396, step: 1116, loss: 28.50636100769043, acc: 0.9141, auc: 0.9885, precision: 0.9247, recall: 0.9167\n",
      "2019-09-23T10:42:23.351382, step: 1117, loss: 52.705528259277344, acc: 0.8359, auc: 0.9597, precision: 0.8672, recall: 0.8382\n",
      "2019-09-23T10:42:25.413517, step: 1118, loss: 40.08642578125, acc: 0.8438, auc: 0.9839, precision: 0.878, recall: 0.8485\n",
      "2019-09-23T10:42:27.462739, step: 1119, loss: 42.63284683227539, acc: 0.8594, auc: 0.9452, precision: 0.8617, recall: 0.8671\n",
      "2019-09-23T10:42:29.837938, step: 1120, loss: 30.922794342041016, acc: 0.9141, auc: 0.9719, precision: 0.9137, recall: 0.9142\n",
      "2019-09-23T10:42:32.298715, step: 1121, loss: 20.976675033569336, acc: 0.9219, auc: 0.9919, precision: 0.9255, recall: 0.9255\n",
      "2019-09-23T10:42:34.603640, step: 1122, loss: 21.023107528686523, acc: 0.9766, auc: 0.9824, precision: 0.977, recall: 0.9761\n",
      "2019-09-23T10:42:36.620021, step: 1123, loss: 22.52434539794922, acc: 0.9219, auc: 0.9849, precision: 0.9223, recall: 0.9219\n",
      "2019-09-23T10:42:39.420259, step: 1124, loss: 29.54607582092285, acc: 0.9141, auc: 0.9773, precision: 0.9185, recall: 0.9149\n",
      "2019-09-23T10:42:41.549093, step: 1125, loss: 22.76740264892578, acc: 0.9141, auc: 0.9848, precision: 0.9216, recall: 0.9092\n",
      "2019-09-23T10:42:43.596193, step: 1126, loss: 33.358924865722656, acc: 0.8906, auc: 0.979, precision: 0.9176, recall: 0.8772\n",
      "2019-09-23T10:42:45.812833, step: 1127, loss: 42.11441421508789, acc: 0.8281, auc: 0.9809, precision: 0.8583, recall: 0.8373\n",
      "2019-09-23T10:42:48.327693, step: 1128, loss: 36.11559295654297, acc: 0.9062, auc: 0.9624, precision: 0.9221, recall: 0.9048\n",
      "2019-09-23T10:42:50.215456, step: 1129, loss: 20.81674575805664, acc: 0.9453, auc: 0.99, precision: 0.945, recall: 0.9456\n",
      "2019-09-23T10:42:52.170524, step: 1130, loss: 18.08230209350586, acc: 0.9219, auc: 0.988, precision: 0.9221, recall: 0.9221\n",
      "2019-09-23T10:42:54.207531, step: 1131, loss: 25.524070739746094, acc: 0.9219, auc: 0.9787, precision: 0.9197, recall: 0.9226\n",
      "2019-09-23T10:42:56.087903, step: 1132, loss: 24.09925079345703, acc: 0.9375, auc: 0.984, precision: 0.9487, recall: 0.931\n",
      "2019-09-23T10:42:58.055440, step: 1133, loss: 21.46593475341797, acc: 0.9219, auc: 0.9868, precision: 0.9232, recall: 0.9223\n",
      "2019-09-23T10:42:59.949178, step: 1134, loss: 14.817041397094727, acc: 0.9688, auc: 0.9955, precision: 0.9737, recall: 0.9643\n",
      "2019-09-23T10:43:01.945196, step: 1135, loss: 23.374767303466797, acc: 0.8906, auc: 0.985, precision: 0.8988, recall: 0.8863\n",
      "2019-09-23T10:43:03.991157, step: 1136, loss: 46.930179595947266, acc: 0.8359, auc: 0.9739, precision: 0.8446, recall: 0.8632\n",
      "2019-09-23T10:43:05.849855, step: 1137, loss: 67.69357299804688, acc: 0.7891, auc: 0.9548, precision: 0.8533, recall: 0.7857\n",
      "2019-09-23T10:43:07.673764, step: 1138, loss: 40.16505432128906, acc: 0.8984, auc: 0.9586, precision: 0.8976, recall: 0.9038\n",
      "2019-09-23T10:43:09.683202, step: 1139, loss: 25.96473503112793, acc: 0.9297, auc: 0.979, precision: 0.9297, recall: 0.9306\n",
      "2019-09-23T10:43:11.497216, step: 1140, loss: 21.13412857055664, acc: 0.9375, auc: 0.9863, precision: 0.9382, recall: 0.937\n",
      "2019-09-23T10:43:13.515153, step: 1141, loss: 17.884788513183594, acc: 0.9531, auc: 0.9905, precision: 0.9538, recall: 0.9529\n",
      "2019-09-23T10:43:15.338196, step: 1142, loss: 24.837860107421875, acc: 0.9062, auc: 0.9785, precision: 0.9051, recall: 0.9051\n",
      "2019-09-23T10:43:17.158899, step: 1143, loss: 30.84454345703125, acc: 0.9141, auc: 0.9677, precision: 0.9155, recall: 0.9128\n",
      "2019-09-23T10:43:19.195893, step: 1144, loss: 19.6208553314209, acc: 0.9609, auc: 0.9847, precision: 0.9598, recall: 0.9628\n",
      "2019-09-23T10:43:21.032787, step: 1145, loss: 27.66623306274414, acc: 0.9062, auc: 0.9868, precision: 0.9221, recall: 0.9048\n",
      "2019-09-23T10:43:22.958224, step: 1146, loss: 67.96753692626953, acc: 0.8047, auc: 0.9744, precision: 0.8486, recall: 0.8074\n",
      "2019-09-23T10:43:24.893876, step: 1147, loss: 42.37068176269531, acc: 0.875, auc: 0.9621, precision: 0.8859, recall: 0.8791\n",
      "2019-09-23T10:43:26.739271, step: 1148, loss: 32.97394561767578, acc: 0.9219, auc: 0.9767, precision: 0.9227, recall: 0.9202\n",
      "2019-09-23T10:43:28.732388, step: 1149, loss: 25.648258209228516, acc: 0.9297, auc: 0.9802, precision: 0.9323, recall: 0.9297\n",
      "2019-09-23T10:43:30.538309, step: 1150, loss: 24.59256935119629, acc: 0.9375, auc: 0.9774, precision: 0.9381, recall: 0.9328\n",
      "2019-09-23T10:43:32.353468, step: 1151, loss: 18.737377166748047, acc: 0.9453, auc: 0.9936, precision: 0.9472, recall: 0.9436\n",
      "2019-09-23T10:43:34.326693, step: 1152, loss: 24.026592254638672, acc: 0.9141, auc: 0.9783, precision: 0.9187, recall: 0.9067\n",
      "2019-09-23T10:43:36.134657, step: 1153, loss: 21.456310272216797, acc: 0.9609, auc: 0.9821, precision: 0.9613, recall: 0.9603\n",
      "2019-09-23T10:43:38.093314, step: 1154, loss: 22.20928955078125, acc: 0.9375, auc: 0.9817, precision: 0.9399, recall: 0.9365\n",
      "2019-09-23T10:43:39.992067, step: 1155, loss: 27.61359214782715, acc: 0.9219, auc: 0.9736, precision: 0.9223, recall: 0.9232\n",
      "2019-09-23T10:43:41.862374, step: 1156, loss: 28.671663284301758, acc: 0.9062, auc: 0.9786, precision: 0.9326, recall: 0.8824\n",
      "2019-09-23T10:43:43.903445, step: 1157, loss: 30.866182327270508, acc: 0.8984, auc: 0.9753, precision: 0.9112, recall: 0.8863\n",
      "2019-09-23T10:43:45.712807, step: 1158, loss: 27.722455978393555, acc: 0.9219, auc: 0.9931, precision: 0.9375, recall: 0.9138\n",
      "2019-09-23T10:43:47.527205, step: 1159, loss: 50.336402893066406, acc: 0.7969, auc: 0.9663, precision: 0.8427, recall: 0.8025\n",
      "2019-09-23T10:43:49.535637, step: 1160, loss: 40.27772903442383, acc: 0.8672, auc: 0.971, precision: 0.8934, recall: 0.8572\n",
      "2019-09-23T10:43:51.363525, step: 1161, loss: 30.289323806762695, acc: 0.9062, auc: 0.979, precision: 0.9302, recall: 0.8889\n",
      "2019-09-23T10:43:53.359728, step: 1162, loss: 19.978769302368164, acc: 0.9297, auc: 0.9927, precision: 0.9361, recall: 0.9279\n",
      "2019-09-23T10:43:55.159091, step: 1163, loss: 22.229381561279297, acc: 0.9219, auc: 0.9854, precision: 0.9286, recall: 0.9219\n",
      "2019-09-23T10:43:56.969997, step: 1164, loss: 22.524974822998047, acc: 0.9297, auc: 0.9816, precision: 0.9292, recall: 0.9299\n",
      "2019-09-23T10:43:58.957580, step: 1165, loss: 19.664073944091797, acc: 0.9609, auc: 0.9842, precision: 0.9599, recall: 0.9613\n",
      "2019-09-23T10:44:00.776829, step: 1166, loss: 20.85486602783203, acc: 0.9297, auc: 0.9866, precision: 0.93, recall: 0.9304\n",
      "2019-09-23T10:44:02.618249, step: 1167, loss: 16.191213607788086, acc: 0.9609, auc: 0.9916, precision: 0.9613, recall: 0.9601\n",
      "2019-09-23T10:44:04.579812, step: 1168, loss: 30.695903778076172, acc: 0.9453, auc: 0.9617, precision: 0.9466, recall: 0.9449\n",
      "2019-09-23T10:44:06.414688, step: 1169, loss: 52.238037109375, acc: 0.8281, auc: 0.9565, precision: 0.8381, recall: 0.8505\n",
      "2019-09-23T10:44:08.455444, step: 1170, loss: 81.06884765625, acc: 0.7266, auc: 0.9618, precision: 0.8164, recall: 0.6895\n",
      "2019-09-23T10:44:10.290314, step: 1171, loss: 53.647315979003906, acc: 0.7969, auc: 0.9538, precision: 0.8356, recall: 0.7995\n",
      "2019-09-23T10:44:12.130621, step: 1172, loss: 29.682815551757812, acc: 0.9141, auc: 0.9761, precision: 0.9152, recall: 0.9137\n",
      "2019-09-23T10:44:14.147668, step: 1173, loss: 25.883441925048828, acc: 0.9219, auc: 0.985, precision: 0.921, recall: 0.9226\n",
      "2019-09-23T10:44:15.968032, step: 1174, loss: 28.191852569580078, acc: 0.9141, auc: 0.9716, precision: 0.9137, recall: 0.9142\n",
      "2019-09-23T10:44:17.796846, step: 1175, loss: 29.450942993164062, acc: 0.8906, auc: 0.9683, precision: 0.8959, recall: 0.8829\n",
      "2019-09-23T10:44:19.792577, step: 1176, loss: 22.149263381958008, acc: 0.9219, auc: 0.9889, precision: 0.9318, recall: 0.9165\n",
      "2019-09-23T10:44:21.632508, step: 1177, loss: 18.766136169433594, acc: 0.9297, auc: 0.989, precision: 0.9304, recall: 0.93\n",
      "2019-09-23T10:44:23.631190, step: 1178, loss: 28.10900115966797, acc: 0.9062, auc: 0.9708, precision: 0.9057, recall: 0.9081\n",
      "2019-09-23T10:44:25.513815, step: 1179, loss: 18.8365478515625, acc: 0.9375, auc: 0.9858, precision: 0.9375, recall: 0.9379\n",
      "2019-09-23T10:44:27.337735, step: 1180, loss: 22.681793212890625, acc: 0.9375, auc: 0.9829, precision: 0.9375, recall: 0.9375\n",
      "2019-09-23T10:44:29.357775, step: 1181, loss: 44.73223876953125, acc: 0.8672, auc: 0.9782, precision: 0.9045, recall: 0.8482\n",
      "2019-09-23T10:44:31.192152, step: 1182, loss: 90.72518157958984, acc: 0.75, auc: 0.9185, precision: 0.8431, recall: 0.7241\n",
      "2019-09-23T10:44:33.172061, step: 1183, loss: 43.47736358642578, acc: 0.8516, auc: 0.9585, precision: 0.8681, recall: 0.8482\n",
      "2019-09-23T10:44:35.043780, step: 1184, loss: 37.65900421142578, acc: 0.8672, auc: 0.9607, precision: 0.8712, recall: 0.868\n",
      "2019-09-23T10:44:36.853716, step: 1185, loss: 22.001953125, acc: 0.9531, auc: 0.9824, precision: 0.9589, recall: 0.9508\n",
      "2019-09-23T10:44:38.833275, step: 1186, loss: 28.043949127197266, acc: 0.9062, auc: 0.9729, precision: 0.9065, recall: 0.9065\n",
      "2019-09-23T10:44:40.664688, step: 1187, loss: 24.146656036376953, acc: 0.9531, auc: 0.9803, precision: 0.9526, recall: 0.9553\n",
      "2019-09-23T10:44:42.490578, step: 1188, loss: 28.42111587524414, acc: 0.9141, auc: 0.9751, precision: 0.9134, recall: 0.917\n",
      "2019-09-23T10:44:44.491666, step: 1189, loss: 31.75156021118164, acc: 0.9141, auc: 0.9687, precision: 0.9149, recall: 0.9185\n",
      "2019-09-23T10:44:46.309597, step: 1190, loss: 32.85169982910156, acc: 0.9062, auc: 0.9609, precision: 0.9078, recall: 0.9062\n",
      "2019-09-23T10:44:48.321706, step: 1191, loss: 21.97672462463379, acc: 0.9375, auc: 0.9819, precision: 0.937, recall: 0.9382\n",
      "2019-09-23T10:44:50.153974, step: 1192, loss: 18.72168731689453, acc: 0.9453, auc: 0.9943, precision: 0.9435, recall: 0.9521\n",
      "2019-09-23T10:44:51.968892, step: 1193, loss: 18.152854919433594, acc: 0.9219, auc: 0.9926, precision: 0.9281, recall: 0.9167\n",
      "2019-09-23T10:44:54.000643, step: 1194, loss: 38.259735107421875, acc: 0.8516, auc: 0.9815, precision: 0.8691, recall: 0.8645\n",
      "2019-09-23T10:44:55.831705, step: 1195, loss: 64.70172119140625, acc: 0.8125, auc: 0.9706, precision: 0.8571, recall: 0.8235\n",
      "2019-09-23T10:44:57.641064, step: 1196, loss: 66.16061401367188, acc: 0.7188, auc: 0.8767, precision: 0.7874, recall: 0.7333\n",
      "2019-09-23T10:44:59.663546, step: 1197, loss: 27.44422149658203, acc: 0.9375, auc: 0.9802, precision: 0.9368, recall: 0.9419\n",
      "2019-09-23T10:45:01.489628, step: 1198, loss: 24.250045776367188, acc: 0.9297, auc: 0.9783, precision: 0.9306, recall: 0.9297\n",
      "2019-09-23T10:45:03.481245, step: 1199, loss: 23.576013565063477, acc: 0.9375, auc: 0.9822, precision: 0.9374, recall: 0.9374\n",
      "2019-09-23T10:45:05.304276, step: 1200, loss: 24.849559783935547, acc: 0.8906, auc: 0.977, precision: 0.8928, recall: 0.8858\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:46:15.930860, step: 1200, loss: 52.90516134408804, acc: 0.8391410256410257, auc: 0.9189358974358973, precision: 0.8394589743589744, recall: 0.8397717948717949\n",
      "2019-09-23T10:46:17.769226, step: 1201, loss: 24.24658203125, acc: 0.8906, auc: 0.9787, precision: 0.8906, recall: 0.891\n",
      "2019-09-23T10:46:19.762771, step: 1202, loss: 28.671974182128906, acc: 0.9219, auc: 0.9739, precision: 0.9251, recall: 0.9226\n",
      "2019-09-23T10:46:21.591800, step: 1203, loss: 33.950862884521484, acc: 0.8828, auc: 0.9692, precision: 0.8945, recall: 0.8828\n",
      "2019-09-23T10:46:23.585355, step: 1204, loss: 26.59994888305664, acc: 0.9375, auc: 0.9792, precision: 0.9459, recall: 0.9355\n",
      "2019-09-23T10:46:25.450696, step: 1205, loss: 19.496631622314453, acc: 0.9375, auc: 0.9889, precision: 0.9409, recall: 0.934\n",
      "2019-09-23T10:46:27.249602, step: 1206, loss: 26.5882568359375, acc: 0.8906, auc: 0.9766, precision: 0.8841, recall: 0.8958\n",
      "2019-09-23T10:46:29.255125, step: 1207, loss: 46.932716369628906, acc: 0.8672, auc: 0.9789, precision: 0.8882, recall: 0.8768\n",
      "2019-09-23T10:46:31.062509, step: 1208, loss: 54.07621383666992, acc: 0.8125, auc: 0.9631, precision: 0.8652, recall: 0.8095\n",
      "2019-09-23T10:46:32.914450, step: 1209, loss: 32.970306396484375, acc: 0.9141, auc: 0.9808, precision: 0.9181, recall: 0.9191\n",
      "2019-09-23T10:46:34.912015, step: 1210, loss: 25.737049102783203, acc: 0.9219, auc: 0.986, precision: 0.9219, recall: 0.9235\n",
      "2019-09-23T10:46:36.721488, step: 1211, loss: 19.221818923950195, acc: 0.9609, auc: 0.9946, precision: 0.9632, recall: 0.9615\n",
      "2019-09-23T10:46:38.716780, step: 1212, loss: 23.087581634521484, acc: 0.9141, auc: 0.9858, precision: 0.9196, recall: 0.9132\n",
      "2019-09-23T10:46:40.529959, step: 1213, loss: 30.795442581176758, acc: 0.9062, auc: 0.9883, precision: 0.9178, recall: 0.9104\n",
      "2019-09-23T10:46:42.338645, step: 1214, loss: 55.5110969543457, acc: 0.8203, auc: 0.9682, precision: 0.8647, recall: 0.8258\n",
      "2019-09-23T10:46:44.312915, step: 1215, loss: 78.67744445800781, acc: 0.8125, auc: 0.9244, precision: 0.8458, recall: 0.8271\n",
      "2019-09-23T10:46:46.121386, step: 1216, loss: 44.15953826904297, acc: 0.8516, auc: 0.9533, precision: 0.8746, recall: 0.8575\n",
      "2019-09-23T10:46:47.913124, step: 1217, loss: 23.182395935058594, acc: 0.9531, auc: 0.9851, precision: 0.9531, recall: 0.9536\n",
      "2019-09-23T10:46:49.909576, step: 1218, loss: 18.375282287597656, acc: 0.9453, auc: 0.9954, precision: 0.9475, recall: 0.9459\n",
      "2019-09-23T10:46:51.723550, step: 1219, loss: 26.896177291870117, acc: 0.8984, auc: 0.9815, precision: 0.8973, recall: 0.9072\n",
      "2019-09-23T10:46:53.746711, step: 1220, loss: 36.9668083190918, acc: 0.8906, auc: 0.9704, precision: 0.8951, recall: 0.897\n",
      "2019-09-23T10:46:55.551034, step: 1221, loss: 30.722755432128906, acc: 0.8906, auc: 0.9807, precision: 0.9004, recall: 0.8906\n",
      "2019-09-23T10:46:57.364916, step: 1222, loss: 24.186813354492188, acc: 0.9375, auc: 0.9848, precision: 0.9399, recall: 0.9365\n",
      "2019-09-23T10:46:59.442965, step: 1223, loss: 23.283565521240234, acc: 0.9375, auc: 0.9819, precision: 0.9356, recall: 0.9356\n",
      "2019-09-23T10:47:01.293093, step: 1224, loss: 33.210845947265625, acc: 0.8984, auc: 0.9614, precision: 0.8997, recall: 0.8976\n",
      "2019-09-23T10:47:03.283155, step: 1225, loss: 30.828540802001953, acc: 0.8906, auc: 0.9672, precision: 0.8929, recall: 0.8867\n",
      "2019-09-23T10:47:05.215833, step: 1226, loss: 23.94131088256836, acc: 0.9219, auc: 0.9938, precision: 0.9219, recall: 0.9286\n",
      "2019-09-23T10:47:07.077455, step: 1227, loss: 21.847190856933594, acc: 0.9297, auc: 0.9893, precision: 0.9349, recall: 0.9297\n",
      "2019-09-23T10:47:09.123425, step: 1228, loss: 27.27043914794922, acc: 0.8984, auc: 0.9784, precision: 0.909, recall: 0.8936\n",
      "2019-09-23T10:47:10.992361, step: 1229, loss: 53.39219665527344, acc: 0.7969, auc: 0.964, precision: 0.8332, recall: 0.8045\n",
      "2019-09-23T10:47:12.903604, step: 1230, loss: 87.8040542602539, acc: 0.7031, auc: 0.9384, precision: 0.8241, recall: 0.6724\n",
      "2019-09-23T10:47:14.974267, step: 1231, loss: 59.32511520385742, acc: 0.7344, auc: 0.9395, precision: 0.8034, recall: 0.7186\n",
      "2019-09-23T10:47:16.792746, step: 1232, loss: 34.445228576660156, acc: 0.9219, auc: 0.9702, precision: 0.9212, recall: 0.9212\n",
      "2019-09-23T10:47:18.986580, step: 1233, loss: 30.165193557739258, acc: 0.9219, auc: 0.9736, precision: 0.9235, recall: 0.9219\n",
      "2019-09-23T10:47:21.210970, step: 1234, loss: 23.114233016967773, acc: 0.9375, auc: 0.9807, precision: 0.9384, recall: 0.9384\n",
      "2019-09-23T10:47:23.132958, step: 1235, loss: 24.99420166015625, acc: 0.9141, auc: 0.9829, precision: 0.9152, recall: 0.9157\n",
      "2019-09-23T10:47:25.712775, step: 1236, loss: 27.880891799926758, acc: 0.9297, auc: 0.9729, precision: 0.9323, recall: 0.9297\n",
      "2019-09-23T10:47:28.514453, step: 1237, loss: 13.4512357711792, acc: 0.9766, auc: 0.9993, precision: 0.9783, recall: 0.9758\n",
      "2019-09-23T10:47:30.789361, step: 1238, loss: 14.714513778686523, acc: 0.9609, auc: 0.9935, precision: 0.9612, recall: 0.959\n",
      "2019-09-23T10:47:32.870572, step: 1239, loss: 26.855762481689453, acc: 0.9141, auc: 0.9876, precision: 0.9, recall: 0.9345\n",
      "2019-09-23T10:47:34.911829, step: 1240, loss: 83.27780151367188, acc: 0.7891, auc: 0.9469, precision: 0.8594, recall: 0.7712\n",
      "2019-09-23T10:47:36.745135, step: 1241, loss: 75.44227600097656, acc: 0.6641, auc: 0.9171, precision: 0.7656, recall: 0.7032\n",
      "2019-09-23T10:47:38.745966, step: 1242, loss: 34.537315368652344, acc: 0.9297, auc: 0.9713, precision: 0.9293, recall: 0.9309\n",
      "2019-09-23T10:47:40.563376, step: 1243, loss: 26.430824279785156, acc: 0.9219, auc: 0.9786, precision: 0.9227, recall: 0.9202\n",
      "2019-09-23T10:47:42.387299, step: 1244, loss: 25.50241470336914, acc: 0.9297, auc: 0.9758, precision: 0.9314, recall: 0.9308\n",
      "2019-09-23T10:47:44.408929, step: 1245, loss: 20.597036361694336, acc: 0.9297, auc: 0.987, precision: 0.9262, recall: 0.9342\n",
      "2019-09-23T10:47:46.229330, step: 1246, loss: 37.37615203857422, acc: 0.8828, auc: 0.9748, precision: 0.8936, recall: 0.8841\n",
      "2019-09-23T10:47:48.034325, step: 1247, loss: 68.8831787109375, acc: 0.7891, auc: 0.95, precision: 0.8448, recall: 0.8015\n",
      "2019-09-23T10:47:50.232861, step: 1248, loss: 36.92182922363281, acc: 0.8906, auc: 0.9625, precision: 0.9031, recall: 0.8853\n",
      "start to train models...\n",
      "2019-09-23T10:47:52.382993, step: 1249, loss: 23.280710220336914, acc: 0.9453, auc: 0.9912, precision: 0.9457, recall: 0.946\n",
      "2019-09-23T10:47:54.440567, step: 1250, loss: 11.941642761230469, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2019-09-23T10:47:56.346381, step: 1251, loss: 19.87203598022461, acc: 0.9375, auc: 0.988, precision: 0.9375, recall: 0.9379\n",
      "2019-09-23T10:47:58.266531, step: 1252, loss: 16.629684448242188, acc: 0.9531, auc: 0.9915, precision: 0.9531, recall: 0.9531\n",
      "2019-09-23T10:48:00.277549, step: 1253, loss: 20.909873962402344, acc: 0.9609, auc: 0.9828, precision: 0.9568, recall: 0.9641\n",
      "2019-09-23T10:48:02.193766, step: 1254, loss: 20.161218643188477, acc: 0.9375, auc: 0.9914, precision: 0.9425, recall: 0.936\n",
      "2019-09-23T10:48:04.251228, step: 1255, loss: 22.02880096435547, acc: 0.9531, auc: 0.9826, precision: 0.9542, recall: 0.952\n",
      "2019-09-23T10:48:06.124316, step: 1256, loss: 16.35301971435547, acc: 0.9531, auc: 0.9929, precision: 0.9536, recall: 0.9545\n",
      "2019-09-23T10:48:07.950888, step: 1257, loss: 26.579601287841797, acc: 0.9219, auc: 0.9862, precision: 0.9246, recall: 0.9263\n",
      "2019-09-23T10:48:09.968373, step: 1258, loss: 16.1672420501709, acc: 0.9609, auc: 0.9915, precision: 0.959, recall: 0.9612\n",
      "2019-09-23T10:48:11.922404, step: 1259, loss: 13.237691879272461, acc: 0.9688, auc: 0.9971, precision: 0.9683, recall: 0.971\n",
      "2019-09-23T10:48:14.041007, step: 1260, loss: 17.353294372558594, acc: 0.9453, auc: 0.9929, precision: 0.9475, recall: 0.9459\n",
      "2019-09-23T10:48:15.957755, step: 1261, loss: 33.95180892944336, acc: 0.9062, auc: 0.9714, precision: 0.9088, recall: 0.9076\n",
      "2019-09-23T10:48:17.845749, step: 1262, loss: 24.194257736206055, acc: 0.9062, auc: 0.9941, precision: 0.9167, recall: 0.9118\n",
      "2019-09-23T10:48:19.953633, step: 1263, loss: 35.54403305053711, acc: 0.8906, auc: 0.9748, precision: 0.8916, recall: 0.8962\n",
      "2019-09-23T10:48:21.848891, step: 1264, loss: 33.80864715576172, acc: 0.8828, auc: 0.9782, precision: 0.9021, recall: 0.8778\n",
      "2019-09-23T10:48:23.936463, step: 1265, loss: 35.35691833496094, acc: 0.8828, auc: 0.9794, precision: 0.8961, recall: 0.8873\n",
      "2019-09-23T10:48:26.100795, step: 1266, loss: 24.63629150390625, acc: 0.9297, auc: 0.9864, precision: 0.9277, recall: 0.9335\n",
      "2019-09-23T10:48:28.136973, step: 1267, loss: 12.94902229309082, acc: 0.9609, auc: 0.9978, precision: 0.9648, recall: 0.9597\n",
      "2019-09-23T10:48:30.147629, step: 1268, loss: 23.400066375732422, acc: 0.9531, auc: 0.9865, precision: 0.9545, recall: 0.9559\n",
      "2019-09-23T10:48:31.971562, step: 1269, loss: 23.36125373840332, acc: 0.9219, auc: 0.9922, precision: 0.9312, recall: 0.9176\n",
      "2019-09-23T10:48:34.002778, step: 1270, loss: 17.932865142822266, acc: 0.9531, auc: 0.9912, precision: 0.9541, recall: 0.9541\n",
      "2019-09-23T10:48:35.886749, step: 1271, loss: 11.727070808410645, acc: 0.9531, auc: 0.9965, precision: 0.9504, recall: 0.9545\n",
      "2019-09-23T10:48:37.888752, step: 1272, loss: 21.870073318481445, acc: 0.9297, auc: 0.9838, precision: 0.9339, recall: 0.927\n",
      "2019-09-23T10:48:40.180779, step: 1273, loss: 11.409798622131348, acc: 0.9766, auc: 0.9973, precision: 0.977, recall: 0.976\n",
      "2019-09-23T10:48:43.010004, step: 1274, loss: 12.507339477539062, acc: 0.9844, auc: 0.9917, precision: 0.9857, recall: 0.9833\n",
      "2019-09-23T10:48:45.135204, step: 1275, loss: 22.216522216796875, acc: 0.9609, auc: 0.9822, precision: 0.962, recall: 0.9609\n",
      "2019-09-23T10:48:47.283875, step: 1276, loss: 31.541540145874023, acc: 0.8984, auc: 0.9963, precision: 0.9145, recall: 0.9\n",
      "2019-09-23T10:48:49.329978, step: 1277, loss: 67.25672912597656, acc: 0.7578, auc: 0.9776, precision: 0.8258, recall: 0.7786\n",
      "2019-09-23T10:48:51.129370, step: 1278, loss: 51.083335876464844, acc: 0.8281, auc: 0.9535, precision: 0.8711, recall: 0.8087\n",
      "2019-09-23T10:48:52.945922, step: 1279, loss: 24.48341941833496, acc: 0.9375, auc: 0.9809, precision: 0.9399, recall: 0.9365\n",
      "2019-09-23T10:48:54.940228, step: 1280, loss: 19.753761291503906, acc: 0.9297, auc: 0.9858, precision: 0.9355, recall: 0.9195\n",
      "2019-09-23T10:48:56.806999, step: 1281, loss: 12.085243225097656, acc: 0.9688, auc: 0.996, precision: 0.9704, recall: 0.9666\n",
      "2019-09-23T10:48:58.988635, step: 1282, loss: 18.314830780029297, acc: 0.9531, auc: 0.9868, precision: 0.9542, recall: 0.952\n",
      "2019-09-23T10:49:00.936127, step: 1283, loss: 18.134498596191406, acc: 0.9609, auc: 0.9863, precision: 0.9609, recall: 0.9611\n",
      "2019-09-23T10:49:02.876054, step: 1284, loss: 23.59735870361328, acc: 0.9531, auc: 0.9836, precision: 0.9549, recall: 0.9531\n",
      "2019-09-23T10:49:04.999203, step: 1285, loss: 10.84782886505127, acc: 0.9844, auc: 0.997, precision: 0.987, recall: 0.9811\n",
      "2019-09-23T10:49:06.905401, step: 1286, loss: 11.113182067871094, acc: 0.9688, auc: 0.9966, precision: 0.9685, recall: 0.9685\n",
      "2019-09-23T10:49:09.041417, step: 1287, loss: 16.85811996459961, acc: 0.9688, auc: 0.9857, precision: 0.9655, recall: 0.973\n",
      "2019-09-23T10:49:10.886146, step: 1288, loss: 45.23231506347656, acc: 0.8594, auc: 0.9628, precision: 0.8637, recall: 0.8655\n",
      "2019-09-23T10:49:12.896181, step: 1289, loss: 38.789085388183594, acc: 0.8516, auc: 0.9816, precision: 0.8666, recall: 0.8599\n",
      "2019-09-23T10:49:14.897585, step: 1290, loss: 43.00115203857422, acc: 0.8359, auc: 0.9699, precision: 0.8807, recall: 0.8279\n",
      "2019-09-23T10:49:16.765388, step: 1291, loss: 30.12557601928711, acc: 0.9219, auc: 0.9773, precision: 0.9228, recall: 0.9278\n",
      "2019-09-23T10:49:18.844824, step: 1292, loss: 21.05512046813965, acc: 0.9141, auc: 0.9944, precision: 0.9216, recall: 0.9151\n",
      "2019-09-23T10:49:20.673333, step: 1293, loss: 21.621566772460938, acc: 0.9297, auc: 0.9974, precision: 0.9489, recall: 0.9082\n",
      "2019-09-23T10:49:22.570211, step: 1294, loss: 26.40321922302246, acc: 0.9141, auc: 0.9831, precision: 0.9229, recall: 0.9045\n",
      "2019-09-23T10:49:24.618638, step: 1295, loss: 21.684791564941406, acc: 0.9453, auc: 0.9921, precision: 0.9453, recall: 0.9507\n",
      "2019-09-23T10:49:26.597783, step: 1296, loss: 18.95233154296875, acc: 0.9375, auc: 0.9905, precision: 0.9384, recall: 0.9384\n",
      "2019-09-23T10:49:28.669682, step: 1297, loss: 15.275186538696289, acc: 0.9453, auc: 0.9922, precision: 0.9457, recall: 0.946\n",
      "2019-09-23T10:49:30.567504, step: 1298, loss: 23.402400970458984, acc: 0.9375, auc: 0.9783, precision: 0.9382, recall: 0.937\n",
      "2019-09-23T10:49:32.522787, step: 1299, loss: 17.886247634887695, acc: 0.9688, auc: 0.9929, precision: 0.9692, recall: 0.9688\n",
      "2019-09-23T10:49:34.600221, step: 1300, loss: 26.95934295654297, acc: 0.9219, auc: 0.9932, precision: 0.9324, recall: 0.9219\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:50:49.024726, step: 1300, loss: 92.23639268141527, acc: 0.7317692307692308, auc: 0.9029948717948716, precision: 0.7860923076923078, recall: 0.7335692307692308\n",
      "2019-09-23T10:50:51.056085, step: 1301, loss: 41.720428466796875, acc: 0.8359, auc: 0.9758, precision: 0.8614, recall: 0.8359\n",
      "2019-09-23T10:50:53.095074, step: 1302, loss: 25.26032066345215, acc: 0.9219, auc: 0.9802, precision: 0.9225, recall: 0.9192\n",
      "2019-09-23T10:50:55.192064, step: 1303, loss: 19.125398635864258, acc: 0.9531, auc: 0.9873, precision: 0.9534, recall: 0.9534\n",
      "2019-09-23T10:50:57.092367, step: 1304, loss: 16.05625343322754, acc: 0.9766, auc: 0.9902, precision: 0.9769, recall: 0.9773\n",
      "2019-09-23T10:50:59.106828, step: 1305, loss: 15.36155891418457, acc: 0.9609, auc: 0.9941, precision: 0.9627, recall: 0.9621\n",
      "2019-09-23T10:51:01.009059, step: 1306, loss: 9.610885620117188, acc: 0.9688, auc: 0.9983, precision: 0.9685, recall: 0.9685\n",
      "2019-09-23T10:51:02.854520, step: 1307, loss: 23.790355682373047, acc: 0.9375, auc: 0.9773, precision: 0.9375, recall: 0.9375\n",
      "2019-09-23T10:51:04.897400, step: 1308, loss: 16.243820190429688, acc: 0.9531, auc: 0.9895, precision: 0.9545, recall: 0.9499\n",
      "2019-09-23T10:51:06.797263, step: 1309, loss: 15.792439460754395, acc: 0.9453, auc: 0.9961, precision: 0.9447, recall: 0.9485\n",
      "2019-09-23T10:51:08.889279, step: 1310, loss: 29.508094787597656, acc: 0.9375, auc: 0.9823, precision: 0.9524, recall: 0.9231\n",
      "2019-09-23T10:51:10.801949, step: 1311, loss: 45.58332061767578, acc: 0.8594, auc: 0.9689, precision: 0.8941, recall: 0.8525\n",
      "2019-09-23T10:51:12.732080, step: 1312, loss: 30.135433197021484, acc: 0.9062, auc: 0.9883, precision: 0.9147, recall: 0.9086\n",
      "2019-09-23T10:51:14.742236, step: 1313, loss: 20.01068115234375, acc: 0.9375, auc: 0.9897, precision: 0.937, recall: 0.9382\n",
      "2019-09-23T10:51:16.674868, step: 1314, loss: 20.833370208740234, acc: 0.9219, auc: 0.9883, precision: 0.9286, recall: 0.9219\n",
      "2019-09-23T10:51:18.738785, step: 1315, loss: 17.837080001831055, acc: 0.9531, auc: 0.9914, precision: 0.9521, recall: 0.9557\n",
      "2019-09-23T10:51:20.700772, step: 1316, loss: 19.49046516418457, acc: 0.9375, auc: 0.9961, precision: 0.9382, recall: 0.9408\n",
      "2019-09-23T10:51:22.659379, step: 1317, loss: 28.194860458374023, acc: 0.9141, auc: 0.9897, precision: 0.92, recall: 0.9172\n",
      "2019-09-23T10:51:24.766857, step: 1318, loss: 31.39940643310547, acc: 0.8906, auc: 0.9728, precision: 0.8984, recall: 0.8874\n",
      "2019-09-23T10:51:26.676197, step: 1319, loss: 18.869842529296875, acc: 0.9219, auc: 0.9883, precision: 0.9203, recall: 0.9203\n",
      "2019-09-23T10:51:28.798602, step: 1320, loss: 23.113679885864258, acc: 0.9375, auc: 0.9812, precision: 0.9379, recall: 0.9375\n",
      "2019-09-23T10:51:31.345331, step: 1321, loss: 16.534414291381836, acc: 0.9531, auc: 0.9914, precision: 0.9529, recall: 0.9529\n",
      "2019-09-23T10:51:33.799442, step: 1322, loss: 13.114124298095703, acc: 0.9688, auc: 0.9946, precision: 0.9702, recall: 0.967\n",
      "2019-09-23T10:51:35.908779, step: 1323, loss: 16.380563735961914, acc: 0.9453, auc: 0.9878, precision: 0.9452, recall: 0.9455\n",
      "2019-09-23T10:51:37.810491, step: 1324, loss: 16.83734893798828, acc: 0.9609, auc: 0.9897, precision: 0.9613, recall: 0.9616\n",
      "2019-09-23T10:51:39.849945, step: 1325, loss: 15.761938095092773, acc: 0.9609, auc: 0.9903, precision: 0.9612, recall: 0.959\n",
      "2019-09-23T10:51:41.710059, step: 1326, loss: 13.857730865478516, acc: 0.9609, auc: 0.9934, precision: 0.9633, recall: 0.9589\n",
      "2019-09-23T10:51:43.709569, step: 1327, loss: 26.1016788482666, acc: 0.9453, auc: 0.9783, precision: 0.9468, recall: 0.9445\n",
      "2019-09-23T10:51:45.712966, step: 1328, loss: 9.566975593566895, acc: 0.9766, auc: 0.9975, precision: 0.9771, recall: 0.9758\n",
      "2019-09-23T10:51:47.935982, step: 1329, loss: 12.798112869262695, acc: 0.9688, auc: 0.9916, precision: 0.9704, recall: 0.9666\n",
      "2019-09-23T10:51:50.115673, step: 1330, loss: 18.25020408630371, acc: 0.9453, auc: 0.9869, precision: 0.9441, recall: 0.9408\n",
      "2019-09-23T10:51:52.246453, step: 1331, loss: 22.68041229248047, acc: 0.9297, auc: 0.9812, precision: 0.9297, recall: 0.9306\n",
      "2019-09-23T10:51:54.336268, step: 1332, loss: 21.368181228637695, acc: 0.9375, auc: 0.99, precision: 0.9375, recall: 0.9392\n",
      "2019-09-23T10:51:56.254175, step: 1333, loss: 18.27383041381836, acc: 0.9375, auc: 0.9934, precision: 0.9388, recall: 0.938\n",
      "2019-09-23T10:51:58.148371, step: 1334, loss: 35.180206298828125, acc: 0.8594, auc: 0.9803, precision: 0.8884, recall: 0.8487\n",
      "2019-09-23T10:52:00.213951, step: 1335, loss: 60.5662727355957, acc: 0.75, auc: 0.9805, precision: 0.8049, recall: 0.7949\n",
      "2019-09-23T10:52:02.043331, step: 1336, loss: 71.69659423828125, acc: 0.7188, auc: 0.9094, precision: 0.8218, recall: 0.7143\n",
      "2019-09-23T10:52:04.298779, step: 1337, loss: 23.912578582763672, acc: 0.9219, auc: 0.989, precision: 0.9225, recall: 0.9213\n",
      "2019-09-23T10:52:06.773673, step: 1338, loss: 12.914794921875, acc: 0.9844, auc: 0.9951, precision: 0.9839, recall: 0.9853\n",
      "2019-09-23T10:52:09.012217, step: 1339, loss: 15.673173904418945, acc: 0.9453, auc: 0.9905, precision: 0.9454, recall: 0.9453\n",
      "2019-09-23T10:52:10.959324, step: 1340, loss: 31.136638641357422, acc: 0.8984, auc: 0.9712, precision: 0.8981, recall: 0.8968\n",
      "2019-09-23T10:52:12.971103, step: 1341, loss: 11.588786125183105, acc: 0.9766, auc: 0.998, precision: 0.9767, recall: 0.9766\n",
      "2019-09-23T10:52:15.098010, step: 1342, loss: 39.766639709472656, acc: 0.8828, auc: 0.9817, precision: 0.8873, recall: 0.8961\n",
      "2019-09-23T10:52:16.980832, step: 1343, loss: 51.06132507324219, acc: 0.7969, auc: 0.9647, precision: 0.8404, recall: 0.7853\n",
      "2019-09-23T10:52:19.130316, step: 1344, loss: 34.95263671875, acc: 0.8906, auc: 0.967, precision: 0.9042, recall: 0.8823\n",
      "2019-09-23T10:52:21.037049, step: 1345, loss: 17.631017684936523, acc: 0.9609, auc: 0.9943, precision: 0.9593, recall: 0.9631\n",
      "2019-09-23T10:52:22.939297, step: 1346, loss: 24.188461303710938, acc: 0.9297, auc: 0.9806, precision: 0.9315, recall: 0.9274\n",
      "2019-09-23T10:52:24.992150, step: 1347, loss: 18.49757194519043, acc: 0.9609, auc: 0.9888, precision: 0.9639, recall: 0.9573\n",
      "2019-09-23T10:52:26.779395, step: 1348, loss: 22.26868438720703, acc: 0.9297, auc: 0.9836, precision: 0.9299, recall: 0.9295\n",
      "2019-09-23T10:52:28.717271, step: 1349, loss: 15.617904663085938, acc: 0.9609, auc: 0.9976, precision: 0.9643, recall: 0.9603\n",
      "2019-09-23T10:52:30.591658, step: 1350, loss: 24.14507484436035, acc: 0.9297, auc: 0.9848, precision: 0.9328, recall: 0.9321\n",
      "2019-09-23T10:52:32.395084, step: 1351, loss: 39.04364776611328, acc: 0.8984, auc: 0.981, precision: 0.9156, recall: 0.8984\n",
      "2019-09-23T10:52:34.408419, step: 1352, loss: 31.102096557617188, acc: 0.8906, auc: 0.985, precision: 0.9099, recall: 0.8808\n",
      "2019-09-23T10:52:36.265300, step: 1353, loss: 19.775253295898438, acc: 0.9453, auc: 0.9909, precision: 0.9449, recall: 0.9466\n",
      "2019-09-23T10:52:38.135351, step: 1354, loss: 22.478607177734375, acc: 0.9297, auc: 0.9846, precision: 0.9319, recall: 0.9303\n",
      "2019-09-23T10:52:40.239059, step: 1355, loss: 18.026512145996094, acc: 0.9453, auc: 0.9915, precision: 0.9466, recall: 0.9449\n",
      "2019-09-23T10:52:42.113831, step: 1356, loss: 15.871661186218262, acc: 0.9453, auc: 0.995, precision: 0.9444, recall: 0.9514\n",
      "2019-09-23T10:52:44.208274, step: 1357, loss: 17.580554962158203, acc: 0.9453, auc: 0.9954, precision: 0.949, recall: 0.944\n",
      "2019-09-23T10:52:46.126504, step: 1358, loss: 17.868309020996094, acc: 0.9453, auc: 0.9895, precision: 0.9515, recall: 0.9386\n",
      "2019-09-23T10:52:47.989086, step: 1359, loss: 27.9902286529541, acc: 0.9453, auc: 0.9753, precision: 0.9468, recall: 0.9445\n",
      "2019-09-23T10:52:50.029302, step: 1360, loss: 39.23541259765625, acc: 0.8906, auc: 0.9648, precision: 0.9011, recall: 0.8894\n",
      "2019-09-23T10:52:51.963461, step: 1361, loss: 39.07379150390625, acc: 0.875, auc: 0.9872, precision: 0.8933, recall: 0.8841\n",
      "2019-09-23T10:52:54.471325, step: 1362, loss: 27.07122230529785, acc: 0.9219, auc: 0.9889, precision: 0.9275, recall: 0.9275\n",
      "2019-09-23T10:52:56.542258, step: 1363, loss: 25.475719451904297, acc: 0.8984, auc: 0.9858, precision: 0.9072, recall: 0.8973\n",
      "2019-09-23T10:52:58.871505, step: 1364, loss: 19.978355407714844, acc: 0.9531, auc: 0.988, precision: 0.9534, recall: 0.9534\n",
      "2019-09-23T10:53:00.787274, step: 1365, loss: 15.668498992919922, acc: 0.9531, auc: 0.9912, precision: 0.9528, recall: 0.9528\n",
      "2019-09-23T10:53:02.739021, step: 1366, loss: 16.077682495117188, acc: 0.9609, auc: 0.9909, precision: 0.9613, recall: 0.9599\n",
      "2019-09-23T10:53:04.804762, step: 1367, loss: 22.395606994628906, acc: 0.9219, auc: 0.9872, precision: 0.9226, recall: 0.9251\n",
      "2019-09-23T10:53:06.937490, step: 1368, loss: 18.588794708251953, acc: 0.9531, auc: 0.9902, precision: 0.956, recall: 0.9516\n",
      "2019-09-23T10:53:09.122671, step: 1369, loss: 13.209775924682617, acc: 0.9609, auc: 0.9936, precision: 0.9606, recall: 0.9623\n",
      "2019-09-23T10:53:11.316075, step: 1370, loss: 10.218120574951172, acc: 0.9844, auc: 0.9985, precision: 0.9804, recall: 0.9873\n",
      "2019-09-23T10:53:13.417902, step: 1371, loss: 31.483245849609375, acc: 0.9062, auc: 0.9779, precision: 0.9098, recall: 0.9098\n",
      "2019-09-23T10:53:15.555372, step: 1372, loss: 21.25244140625, acc: 0.9375, auc: 0.9988, precision: 0.9467, recall: 0.9344\n",
      "2019-09-23T10:53:17.405236, step: 1373, loss: 31.189258575439453, acc: 0.8906, auc: 0.9712, precision: 0.8906, recall: 0.8968\n",
      "2019-09-23T10:53:19.640341, step: 1374, loss: 34.58570861816406, acc: 0.8828, auc: 0.9829, precision: 0.9021, recall: 0.8778\n",
      "2019-09-23T10:53:21.586936, step: 1375, loss: 67.07102966308594, acc: 0.7734, auc: 0.9587, precision: 0.8457, recall: 0.7698\n",
      "2019-09-23T10:53:23.447918, step: 1376, loss: 62.08924865722656, acc: 0.7969, auc: 0.9641, precision: 0.8523, recall: 0.803\n",
      "2019-09-23T10:53:25.928996, step: 1377, loss: 38.8990364074707, acc: 0.8906, auc: 0.9773, precision: 0.8924, recall: 0.8901\n",
      "2019-09-23T10:53:28.249021, step: 1378, loss: 20.675033569335938, acc: 0.9531, auc: 0.9934, precision: 0.9529, recall: 0.9538\n",
      "2019-09-23T10:53:30.809312, step: 1379, loss: 20.700693130493164, acc: 0.9375, auc: 0.9875, precision: 0.9375, recall: 0.9392\n",
      "2019-09-23T10:53:32.967442, step: 1380, loss: 21.506820678710938, acc: 0.9375, auc: 0.9946, precision: 0.942, recall: 0.9403\n",
      "2019-09-23T10:53:35.563180, step: 1381, loss: 22.409732818603516, acc: 0.9141, auc: 0.9902, precision: 0.9259, recall: 0.908\n",
      "2019-09-23T10:53:38.136447, step: 1382, loss: 32.47381591796875, acc: 0.8828, auc: 0.9738, precision: 0.8876, recall: 0.8868\n",
      "2019-09-23T10:53:40.750444, step: 1383, loss: 19.55370330810547, acc: 0.9453, auc: 0.9899, precision: 0.9506, recall: 0.9411\n",
      "2019-09-23T10:53:43.557280, step: 1384, loss: 19.501689910888672, acc: 0.9219, auc: 0.9883, precision: 0.9225, recall: 0.9213\n",
      "2019-09-23T10:53:46.299198, step: 1385, loss: 24.430837631225586, acc: 0.9297, auc: 0.9795, precision: 0.9319, recall: 0.9303\n",
      "2019-09-23T10:53:48.292264, step: 1386, loss: 34.11119842529297, acc: 0.8828, auc: 0.9736, precision: 0.899, recall: 0.8719\n",
      "2019-09-23T10:53:50.720957, step: 1387, loss: 35.69154357910156, acc: 0.8906, auc: 0.982, precision: 0.8971, recall: 0.9054\n",
      "2019-09-23T10:53:52.847750, step: 1388, loss: 44.32356262207031, acc: 0.8203, auc: 0.9699, precision: 0.855, recall: 0.8276\n",
      "2019-09-23T10:53:55.216144, step: 1389, loss: 34.64556884765625, acc: 0.875, auc: 0.9783, precision: 0.8816, recall: 0.8839\n",
      "2019-09-23T10:53:57.385310, step: 1390, loss: 18.533267974853516, acc: 0.9531, auc: 0.9924, precision: 0.9536, recall: 0.9545\n",
      "2019-09-23T10:53:59.697318, step: 1391, loss: 16.1351375579834, acc: 0.9531, auc: 0.9956, precision: 0.9531, recall: 0.9549\n",
      "2019-09-23T10:54:01.712634, step: 1392, loss: 32.88273239135742, acc: 0.8672, auc: 0.9782, precision: 0.8854, recall: 0.8621\n",
      "2019-09-23T10:54:03.704992, step: 1393, loss: 36.60009002685547, acc: 0.8594, auc: 0.9953, precision: 0.88, recall: 0.8732\n",
      "2019-09-23T10:54:05.836279, step: 1394, loss: 33.812767028808594, acc: 0.8828, auc: 0.9799, precision: 0.9021, recall: 0.8778\n",
      "2019-09-23T10:54:08.287552, step: 1395, loss: 28.064388275146484, acc: 0.9297, auc: 0.9686, precision: 0.9292, recall: 0.9299\n",
      "2019-09-23T10:54:10.765902, step: 1396, loss: 20.08448028564453, acc: 0.9453, auc: 0.9825, precision: 0.9476, recall: 0.9426\n",
      "2019-09-23T10:54:13.123752, step: 1397, loss: 15.752344131469727, acc: 0.9609, auc: 0.9914, precision: 0.9608, recall: 0.9611\n",
      "2019-09-23T10:54:15.417707, step: 1398, loss: 16.898555755615234, acc: 0.9375, auc: 0.9928, precision: 0.9359, recall: 0.9402\n",
      "2019-09-23T10:54:17.474373, step: 1399, loss: 20.994205474853516, acc: 0.9141, auc: 0.991, precision: 0.9239, recall: 0.9118\n",
      "2019-09-23T10:54:19.594253, step: 1400, loss: 32.734832763671875, acc: 0.8828, auc: 0.9725, precision: 0.8868, recall: 0.8876\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T10:55:40.551718, step: 1400, loss: 54.40384341508914, acc: 0.817502564102564, auc: 0.9165717948717952, precision: 0.8250128205128207, recall: 0.8193641025641025\n",
      "2019-09-23T10:55:42.938264, step: 1401, loss: 20.83237648010254, acc: 0.9688, auc: 0.9917, precision: 0.97, recall: 0.9676\n",
      "2019-09-23T10:55:45.276676, step: 1402, loss: 19.240097045898438, acc: 0.9297, auc: 0.989, precision: 0.9311, recall: 0.9289\n",
      "2019-09-23T10:55:47.473156, step: 1403, loss: 16.213592529296875, acc: 0.9609, auc: 0.9919, precision: 0.9598, recall: 0.9628\n",
      "2019-09-23T10:55:49.898045, step: 1404, loss: 18.219762802124023, acc: 0.9453, auc: 0.9884, precision: 0.9388, recall: 0.9479\n",
      "start to train models...\n",
      "2019-09-23T10:55:52.261258, step: 1405, loss: 14.715666770935059, acc: 0.9531, auc: 0.9993, precision: 0.9615, recall: 0.9464\n",
      "2019-09-23T10:55:54.858997, step: 1406, loss: 14.458803176879883, acc: 0.9766, auc: 0.9993, precision: 0.9762, recall: 0.9779\n",
      "2019-09-23T10:55:56.887173, step: 1407, loss: 12.613788604736328, acc: 0.9766, auc: 0.9973, precision: 0.9762, recall: 0.9779\n",
      "2019-09-23T10:55:59.222317, step: 1408, loss: 9.495969772338867, acc: 0.9844, auc: 0.9966, precision: 0.9843, recall: 0.9843\n",
      "2019-09-23T10:56:01.567401, step: 1409, loss: 9.233951568603516, acc: 0.9688, auc: 0.9988, precision: 0.9677, recall: 0.9714\n",
      "2019-09-23T10:56:03.570862, step: 1410, loss: 6.1098527908325195, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2019-09-23T10:56:05.694755, step: 1411, loss: 11.443876266479492, acc: 0.9766, auc: 0.9956, precision: 0.9764, recall: 0.9768\n",
      "2019-09-23T10:56:07.843870, step: 1412, loss: 9.171721458435059, acc: 0.9844, auc: 0.998, precision: 0.9842, recall: 0.9842\n",
      "2019-09-23T10:56:10.189635, step: 1413, loss: 4.293835639953613, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-09-23T10:56:12.072915, step: 1414, loss: 9.270028114318848, acc: 0.9688, auc: 0.9963, precision: 0.9705, recall: 0.9663\n",
      "2019-09-23T10:56:14.251712, step: 1415, loss: 12.885801315307617, acc: 0.9766, auc: 0.9944, precision: 0.9768, recall: 0.9764\n",
      "2019-09-23T10:56:16.274049, step: 1416, loss: 9.761322975158691, acc: 0.9766, auc: 0.9995, precision: 0.9786, recall: 0.9754\n",
      "2019-09-23T10:56:18.359012, step: 1417, loss: 41.4815788269043, acc: 0.8906, auc: 0.9973, precision: 0.9114, recall: 0.8889\n",
      "2019-09-23T10:56:20.447007, step: 1418, loss: 222.73995971679688, acc: 0.5781, auc: 0.9198, precision: 0.775, recall: 0.5645\n",
      "2019-09-23T10:56:22.382758, step: 1419, loss: 55.04951477050781, acc: 0.7969, auc: 0.9315, precision: 0.8117, recall: 0.8212\n",
      "2019-09-23T10:56:24.444615, step: 1420, loss: 50.43801498413086, acc: 0.875, auc: 0.9523, precision: 0.8871, recall: 0.8681\n",
      "2019-09-23T10:56:26.349395, step: 1421, loss: 44.729209899902344, acc: 0.8828, auc: 0.9658, precision: 0.8864, recall: 0.8844\n",
      "2019-09-23T10:56:28.233722, step: 1422, loss: 34.47178649902344, acc: 0.9141, auc: 0.981, precision: 0.922, recall: 0.9081\n",
      "2019-09-23T10:56:30.315834, step: 1423, loss: 34.415252685546875, acc: 0.9297, auc: 0.9688, precision: 0.928, recall: 0.9296\n",
      "2019-09-23T10:56:32.216597, step: 1424, loss: 31.45814323425293, acc: 0.9219, auc: 0.9736, precision: 0.9219, recall: 0.9219\n",
      "2019-09-23T10:56:34.299086, step: 1425, loss: 33.85455322265625, acc: 0.8672, auc: 0.9713, precision: 0.8672, recall: 0.8694\n",
      "2019-09-23T10:56:36.392795, step: 1426, loss: 25.177566528320312, acc: 0.9375, auc: 0.9821, precision: 0.9373, recall: 0.9373\n",
      "2019-09-23T10:56:38.383567, step: 1427, loss: 18.868492126464844, acc: 0.9375, auc: 0.9926, precision: 0.9366, recall: 0.9383\n",
      "2019-09-23T10:56:40.417665, step: 1428, loss: 14.763019561767578, acc: 0.9688, auc: 0.9951, precision: 0.9694, recall: 0.9685\n",
      "2019-09-23T10:56:42.376403, step: 1429, loss: 19.881362915039062, acc: 0.9297, auc: 0.983, precision: 0.9271, recall: 0.9292\n",
      "2019-09-23T10:56:44.540663, step: 1430, loss: 13.263185501098633, acc: 0.9688, auc: 0.9937, precision: 0.9688, recall: 0.9688\n",
      "2019-09-23T10:56:46.488735, step: 1431, loss: 12.547028541564941, acc: 0.9766, auc: 0.9958, precision: 0.9771, recall: 0.9758\n",
      "2019-09-23T10:56:48.412980, step: 1432, loss: 13.897077560424805, acc: 0.9609, auc: 0.9929, precision: 0.9611, recall: 0.9609\n",
      "2019-09-23T10:56:50.461934, step: 1433, loss: 10.465250015258789, acc: 0.9688, auc: 0.9971, precision: 0.9726, recall: 0.9661\n",
      "2019-09-23T10:56:52.431401, step: 1434, loss: 17.558692932128906, acc: 0.9375, auc: 0.9919, precision: 0.9325, recall: 0.9443\n",
      "2019-09-23T10:56:54.620673, step: 1435, loss: 17.092998504638672, acc: 0.9453, auc: 0.9988, precision: 0.947, recall: 0.9493\n",
      "2019-09-23T10:56:56.482414, step: 1436, loss: 7.428987503051758, acc: 0.9766, auc: 0.9985, precision: 0.981, recall: 0.9712\n",
      "2019-09-23T10:56:58.363633, step: 1437, loss: 8.230768203735352, acc: 0.9844, auc: 0.9966, precision: 0.9844, recall: 0.9844\n",
      "2019-09-23T10:57:00.485582, step: 1438, loss: 16.14863395690918, acc: 0.9766, auc: 0.9868, precision: 0.9745, recall: 0.9772\n",
      "2019-09-23T10:57:02.341005, step: 1439, loss: 13.467689514160156, acc: 0.9531, auc: 0.9936, precision: 0.957, recall: 0.9498\n",
      "2019-09-23T10:57:04.459570, step: 1440, loss: 9.905130386352539, acc: 0.9609, auc: 0.998, precision: 0.9609, recall: 0.962\n",
      "2019-09-23T10:57:06.305397, step: 1441, loss: 6.607306480407715, acc: 0.9766, auc: 0.9993, precision: 0.9783, recall: 0.9758\n",
      "2019-09-23T10:57:08.230971, step: 1442, loss: 7.982897758483887, acc: 0.9688, auc: 0.999, precision: 0.9688, recall: 0.9692\n",
      "2019-09-23T10:57:10.329226, step: 1443, loss: 11.122997283935547, acc: 0.9766, auc: 0.9968, precision: 0.9786, recall: 0.9754\n",
      "2019-09-23T10:57:12.200923, step: 1444, loss: 15.604721069335938, acc: 0.9219, auc: 1.0, precision: 0.9275, recall: 0.9275\n",
      "2019-09-23T10:57:14.270762, step: 1445, loss: 85.62479400634766, acc: 0.7812, auc: 0.9702, precision: 0.8571, recall: 0.7586\n",
      "2019-09-23T10:57:16.152679, step: 1446, loss: 37.176979064941406, acc: 0.8672, auc: 0.9873, precision: 0.8976, recall: 0.8629\n",
      "2019-09-23T10:57:18.036872, step: 1447, loss: 44.22840118408203, acc: 0.8203, auc: 0.976, precision: 0.8402, recall: 0.8456\n",
      "2019-09-23T10:57:20.229170, step: 1448, loss: 19.629920959472656, acc: 0.9297, auc: 0.9907, precision: 0.9335, recall: 0.9277\n",
      "2019-09-23T10:57:22.096812, step: 1449, loss: 8.674644470214844, acc: 0.9844, auc: 0.999, precision: 0.9848, recall: 0.9844\n",
      "2019-09-23T10:57:24.122276, step: 1450, loss: 11.792630195617676, acc: 0.9688, auc: 0.9961, precision: 0.9692, recall: 0.9688\n",
      "2019-09-23T10:57:26.042629, step: 1451, loss: 10.176563262939453, acc: 0.9688, auc: 0.9971, precision: 0.9686, recall: 0.9686\n",
      "2019-09-23T10:57:27.992268, step: 1452, loss: 6.1482086181640625, acc: 0.9844, auc: 0.9995, precision: 0.9848, recall: 0.9844\n",
      "2019-09-23T10:57:30.038623, step: 1453, loss: 13.502192497253418, acc: 0.9453, auc: 0.9953, precision: 0.9426, recall: 0.9498\n",
      "2019-09-23T10:57:31.921062, step: 1454, loss: 13.053787231445312, acc: 0.9531, auc: 0.998, precision: 0.9595, recall: 0.95\n",
      "2019-09-23T10:57:33.916572, step: 1455, loss: 16.86664581298828, acc: 0.9531, auc: 0.998, precision: 0.9545, recall: 0.9559\n",
      "2019-09-23T10:57:36.232448, step: 1456, loss: 25.240787506103516, acc: 0.9297, auc: 0.9881, precision: 0.9387, recall: 0.9228\n",
      "2019-09-23T10:57:38.235567, step: 1457, loss: 13.8808012008667, acc: 0.9688, auc: 0.9944, precision: 0.9679, recall: 0.9698\n",
      "2019-09-23T10:57:40.313893, step: 1458, loss: 7.0617523193359375, acc: 0.9844, auc: 0.9993, precision: 0.9843, recall: 0.9843\n",
      "2019-09-23T10:57:42.275800, step: 1459, loss: 9.732397079467773, acc: 0.9688, auc: 0.9966, precision: 0.9698, recall: 0.9679\n",
      "2019-09-23T10:57:44.728698, step: 1460, loss: 17.030597686767578, acc: 0.9844, auc: 0.9871, precision: 0.9841, recall: 0.9841\n",
      "2019-09-23T10:57:47.102816, step: 1461, loss: 13.96657943725586, acc: 0.9609, auc: 0.997, precision: 0.9603, recall: 0.9643\n",
      "2019-09-23T10:57:48.998915, step: 1462, loss: 18.239328384399414, acc: 0.9531, auc: 0.9948, precision: 0.962, recall: 0.9455\n",
      "2019-09-23T10:57:51.220556, step: 1463, loss: 19.481807708740234, acc: 0.9375, auc: 1.0, precision: 0.9394, recall: 0.9429\n",
      "2019-09-23T10:57:53.106867, step: 1464, loss: 31.12771987915039, acc: 0.9219, auc: 0.988, precision: 0.9324, recall: 0.9219\n",
      "2019-09-23T10:57:55.283607, step: 1465, loss: 29.19020652770996, acc: 0.875, auc: 0.9816, precision: 0.8773, recall: 0.8829\n",
      "2019-09-23T10:57:57.165413, step: 1466, loss: 9.505253791809082, acc: 0.9766, auc: 0.9993, precision: 0.9792, recall: 0.9746\n",
      "2019-09-23T10:57:59.229791, step: 1467, loss: 13.3898344039917, acc: 0.9609, auc: 0.9975, precision: 0.9576, recall: 0.9662\n",
      "2019-09-23T10:58:01.561270, step: 1468, loss: 6.545973777770996, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2019-09-23T10:58:03.953450, step: 1469, loss: 12.220556259155273, acc: 0.9688, auc: 0.9983, precision: 0.9701, recall: 0.9692\n",
      "2019-09-23T10:58:06.317178, step: 1470, loss: 14.54099178314209, acc: 0.9766, auc: 0.9978, precision: 0.9769, recall: 0.9773\n",
      "2019-09-23T10:58:08.268759, step: 1471, loss: 22.10634994506836, acc: 0.9219, auc: 0.9909, precision: 0.9318, recall: 0.9165\n",
      "2019-09-23T10:58:10.549786, step: 1472, loss: 38.65650177001953, acc: 0.8672, auc: 0.9875, precision: 0.8896, recall: 0.875\n",
      "2019-09-23T10:58:12.414495, step: 1473, loss: 55.6287956237793, acc: 0.7969, auc: 0.9635, precision: 0.8617, recall: 0.7833\n",
      "2019-09-23T10:58:14.514618, step: 1474, loss: 10.622871398925781, acc: 0.9688, auc: 0.9987, precision: 0.9708, recall: 0.9646\n",
      "2019-09-23T10:58:16.368960, step: 1475, loss: 10.62101936340332, acc: 0.9766, auc: 0.9973, precision: 0.9769, recall: 0.9773\n",
      "2019-09-23T10:58:18.233752, step: 1476, loss: 8.720220565795898, acc: 0.9844, auc: 0.9995, precision: 0.9839, recall: 0.9853\n",
      "2019-09-23T10:58:20.297617, step: 1477, loss: 12.270559310913086, acc: 0.9609, auc: 0.9956, precision: 0.962, recall: 0.9609\n",
      "2019-09-23T10:58:22.137132, step: 1478, loss: 17.91895866394043, acc: 0.9297, auc: 0.9892, precision: 0.9332, recall: 0.9284\n",
      "2019-09-23T10:58:24.359448, step: 1479, loss: 32.11333465576172, acc: 0.9141, auc: 0.9938, precision: 0.9203, recall: 0.9214\n",
      "2019-09-23T10:58:26.295385, step: 1480, loss: 22.108476638793945, acc: 0.9453, auc: 0.9889, precision: 0.9545, recall: 0.9397\n",
      "2019-09-23T10:58:28.145757, step: 1481, loss: 20.401805877685547, acc: 0.9297, auc: 0.9836, precision: 0.9299, recall: 0.9289\n",
      "2019-09-23T10:58:30.234594, step: 1482, loss: 15.695005416870117, acc: 0.9453, auc: 0.9915, precision: 0.9455, recall: 0.9452\n",
      "2019-09-23T10:58:32.211270, step: 1483, loss: 11.343290328979492, acc: 0.9766, auc: 0.9946, precision: 0.975, recall: 0.9789\n",
      "2019-09-23T10:58:34.421490, step: 1484, loss: 9.13583755493164, acc: 0.9766, auc: 0.9998, precision: 0.975, recall: 0.9789\n",
      "2019-09-23T10:58:36.318042, step: 1485, loss: 10.091597557067871, acc: 0.9766, auc: 0.998, precision: 0.9771, recall: 0.9758\n",
      "2019-09-23T10:58:38.279626, step: 1486, loss: 7.363009452819824, acc: 0.9844, auc: 0.999, precision: 0.987, recall: 0.9811\n",
      "2019-09-23T10:58:40.443340, step: 1487, loss: 27.79157066345215, acc: 0.9297, auc: 0.981, precision: 0.9291, recall: 0.9328\n",
      "2019-09-23T10:58:42.755635, step: 1488, loss: 38.257118225097656, acc: 0.8828, auc: 0.9762, precision: 0.8973, recall: 0.877\n",
      "2019-09-23T10:58:45.235997, step: 1489, loss: 24.484811782836914, acc: 0.9219, auc: 0.9941, precision: 0.9342, recall: 0.9194\n",
      "2019-09-23T10:58:47.217134, step: 1490, loss: 21.887197494506836, acc: 0.9219, auc: 0.9868, precision: 0.9167, recall: 0.9251\n",
      "2019-09-23T10:58:49.350185, step: 1491, loss: 13.43442153930664, acc: 0.9531, auc: 0.9953, precision: 0.9542, recall: 0.952\n",
      "2019-09-23T10:58:51.253871, step: 1492, loss: 11.603544235229492, acc: 0.9688, auc: 0.9922, precision: 0.9698, recall: 0.9679\n",
      "2019-09-23T10:58:53.133807, step: 1493, loss: 8.305904388427734, acc: 0.9844, auc: 0.9985, precision: 0.9843, recall: 0.9843\n",
      "2019-09-23T10:58:55.228271, step: 1494, loss: 8.01881217956543, acc: 0.9844, auc: 0.9976, precision: 0.9853, recall: 0.9839\n",
      "2019-09-23T10:58:57.103118, step: 1495, loss: 11.799781799316406, acc: 0.9766, auc: 0.9958, precision: 0.9776, recall: 0.9766\n",
      "2019-09-23T10:58:59.040170, step: 1496, loss: 18.54213523864746, acc: 0.9062, auc: 0.9995, precision: 0.9189, recall: 0.9091\n",
      "2019-09-23T10:59:01.212787, step: 1497, loss: 59.1329231262207, acc: 0.7969, auc: 0.9834, precision: 0.8713, recall: 0.7547\n",
      "2019-09-23T10:59:03.080076, step: 1498, loss: 42.449127197265625, acc: 0.9141, auc: 0.9697, precision: 0.9129, recall: 0.914\n",
      "2019-09-23T10:59:05.097496, step: 1499, loss: 35.266807556152344, acc: 0.8906, auc: 0.9762, precision: 0.8873, recall: 0.8954\n",
      "2019-09-23T10:59:07.011789, step: 1500, loss: 22.1541805267334, acc: 0.9375, auc: 0.9882, precision: 0.9407, recall: 0.9347\n",
      "\n",
      " Evaluation: \n",
      "2019-09-23T11:00:23.711056, step: 1500, loss: 59.80829092172476, acc: 0.8199256410256412, auc: 0.9051461538461536, precision: 0.8199538461538463, recall: 0.819246153846154\n",
      "2019-09-23T11:00:26.070527, step: 1501, loss: 14.71702766418457, acc: 0.9688, auc: 0.9919, precision: 0.9701, recall: 0.9692\n",
      "2019-09-23T11:00:28.522314, step: 1502, loss: 13.56375503540039, acc: 0.9688, auc: 0.9957, precision: 0.9747, recall: 0.9623\n",
      "2019-09-23T11:00:31.014733, step: 1503, loss: 23.817251205444336, acc: 0.9219, auc: 0.9866, precision: 0.9271, recall: 0.9238\n",
      "2019-09-23T11:00:33.012229, step: 1504, loss: 47.940975189208984, acc: 0.8516, auc: 0.9868, precision: 0.8827, recall: 0.8561\n",
      "2019-09-23T11:00:35.304713, step: 1505, loss: 65.8960952758789, acc: 0.8047, auc: 0.9653, precision: 0.8457, recall: 0.8264\n",
      "2019-09-23T11:00:37.216550, step: 1506, loss: 30.357763290405273, acc: 0.9219, auc: 0.9745, precision: 0.9227, recall: 0.9202\n",
      "2019-09-23T11:00:39.209918, step: 1507, loss: 12.771997451782227, acc: 0.9766, auc: 0.9961, precision: 0.9769, recall: 0.9763\n",
      "2019-09-23T11:00:41.152142, step: 1508, loss: 17.502174377441406, acc: 0.9609, auc: 0.9911, precision: 0.9528, recall: 0.9688\n",
      "2019-09-23T11:00:42.960069, step: 1509, loss: 14.193115234375, acc: 0.9531, auc: 0.9936, precision: 0.9544, recall: 0.9512\n",
      "2019-09-23T11:00:44.970739, step: 1510, loss: 10.489784240722656, acc: 0.9688, auc: 0.9983, precision: 0.9692, recall: 0.9688\n",
      "2019-09-23T11:00:46.800592, step: 1511, loss: 8.499584197998047, acc: 0.9766, auc: 0.998, precision: 0.977, recall: 0.9761\n",
      "2019-09-23T11:00:48.637502, step: 1512, loss: 5.757632255554199, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2019-09-23T11:00:50.648033, step: 1513, loss: 12.187332153320312, acc: 0.9531, auc: 0.9954, precision: 0.9538, recall: 0.9529\n",
      "2019-09-23T11:00:52.456280, step: 1514, loss: 20.958587646484375, acc: 0.9375, auc: 0.9896, precision: 0.936, recall: 0.9425\n",
      "2019-09-23T11:00:54.406964, step: 1515, loss: 28.26526641845703, acc: 0.8828, auc: 0.9936, precision: 0.895, recall: 0.8887\n",
      "2019-09-23T11:00:56.363162, step: 1516, loss: 27.357389450073242, acc: 0.8984, auc: 0.9919, precision: 0.9167, recall: 0.8968\n",
      "2019-09-23T11:00:58.161294, step: 1517, loss: 12.042644500732422, acc: 0.9609, auc: 0.9954, precision: 0.962, recall: 0.9609\n",
      "2019-09-23T11:01:00.284226, step: 1518, loss: 16.120849609375, acc: 0.9453, auc: 0.99, precision: 0.9441, recall: 0.9408\n",
      "2019-09-23T11:01:02.348985, step: 1519, loss: 15.68589973449707, acc: 0.9453, auc: 0.9939, precision: 0.9449, recall: 0.9466\n",
      "2019-09-23T11:01:04.317779, step: 1520, loss: 20.81841468811035, acc: 0.9297, auc: 0.9863, precision: 0.9332, recall: 0.9284\n",
      "2019-09-23T11:01:06.309399, step: 1521, loss: 36.452857971191406, acc: 0.9141, auc: 0.9873, precision: 0.9167, recall: 0.9247\n",
      "2019-09-23T11:01:08.216250, step: 1522, loss: 91.94602966308594, acc: 0.7812, auc: 0.9658, precision: 0.8511, recall: 0.7742\n",
      "2019-09-23T11:01:10.434443, step: 1523, loss: 32.67103958129883, acc: 0.9453, auc: 0.9877, precision: 0.9456, recall: 0.9444\n",
      "2019-09-23T11:01:12.463320, step: 1524, loss: 36.55767059326172, acc: 0.8828, auc: 0.9825, precision: 0.8926, recall: 0.8914\n",
      "2019-09-23T11:01:14.550785, step: 1525, loss: 29.471044540405273, acc: 0.9375, auc: 0.9804, precision: 0.9375, recall: 0.9392\n",
      "2019-09-23T11:01:16.667557, step: 1526, loss: 26.573720932006836, acc: 0.9219, auc: 0.981, precision: 0.9358, recall: 0.9036\n",
      "2019-09-23T11:01:18.481722, step: 1527, loss: 23.57553482055664, acc: 0.9375, auc: 0.9863, precision: 0.9379, recall: 0.9322\n",
      "2019-09-23T11:01:20.465904, step: 1528, loss: 16.433998107910156, acc: 0.9375, auc: 0.9941, precision: 0.937, recall: 0.9382\n",
      "2019-09-23T11:01:22.313772, step: 1529, loss: 17.56544303894043, acc: 0.9531, auc: 0.9898, precision: 0.9545, recall: 0.9504\n",
      "2019-09-23T11:01:24.147884, step: 1530, loss: 9.802932739257812, acc: 0.9844, auc: 0.9973, precision: 0.9841, recall: 0.9841\n",
      "2019-09-23T11:01:26.236668, step: 1531, loss: 10.356382369995117, acc: 0.9766, auc: 0.9946, precision: 0.9768, recall: 0.9764\n",
      "2019-09-23T11:01:28.125134, step: 1532, loss: 9.756614685058594, acc: 0.9531, auc: 0.9966, precision: 0.9527, recall: 0.9527\n",
      "2019-09-23T11:01:30.156162, step: 1533, loss: 13.733075141906738, acc: 0.9609, auc: 0.9917, precision: 0.9607, recall: 0.9612\n",
      "2019-09-23T11:01:32.004942, step: 1534, loss: 14.376978874206543, acc: 0.9609, auc: 0.9917, precision: 0.9608, recall: 0.9611\n",
      "2019-09-23T11:01:33.966068, step: 1535, loss: 14.795499801635742, acc: 0.9531, auc: 0.9934, precision: 0.9523, recall: 0.9541\n",
      "2019-09-23T11:01:36.251192, step: 1536, loss: 18.48666763305664, acc: 0.9453, auc: 0.9855, precision: 0.9456, recall: 0.9446\n",
      "2019-09-23T11:01:38.094086, step: 1537, loss: 14.86793327331543, acc: 0.9609, auc: 0.9933, precision: 0.9637, recall: 0.9579\n",
      "2019-09-23T11:01:40.088609, step: 1538, loss: 18.787700653076172, acc: 0.9219, auc: 0.9924, precision: 0.9266, recall: 0.9203\n",
      "2019-09-23T11:01:41.988345, step: 1539, loss: 20.419296264648438, acc: 0.9297, auc: 0.9985, precision: 0.9375, recall: 0.9308\n",
      "2019-09-23T11:01:43.887074, step: 1540, loss: 42.15758514404297, acc: 0.8594, auc: 0.9918, precision: 0.88, recall: 0.8732\n",
      "2019-09-23T11:01:45.908580, step: 1541, loss: 21.508378982543945, acc: 0.9219, auc: 0.9958, precision: 0.9275, recall: 0.9275\n",
      "2019-09-23T11:01:47.971123, step: 1542, loss: 20.258743286132812, acc: 0.9141, auc: 0.9939, precision: 0.9216, recall: 0.9151\n",
      "2019-09-23T11:01:50.130793, step: 1543, loss: 24.96068572998047, acc: 0.9219, auc: 0.9931, precision: 0.9275, recall: 0.9275\n",
      "2019-09-23T11:01:52.023931, step: 1544, loss: 35.687381744384766, acc: 0.8828, auc: 0.9916, precision: 0.9138, recall: 0.8661\n",
      "2019-09-23T11:01:53.858638, step: 1545, loss: 21.548236846923828, acc: 0.9531, auc: 0.9851, precision: 0.9545, recall: 0.9536\n",
      "2019-09-23T11:01:55.864222, step: 1546, loss: 16.521446228027344, acc: 0.9609, auc: 0.9915, precision: 0.9623, recall: 0.9606\n",
      "2019-09-23T11:01:57.776797, step: 1547, loss: 12.135763168334961, acc: 0.9844, auc: 0.9936, precision: 0.9841, recall: 0.9841\n",
      "2019-09-23T11:01:59.888819, step: 1548, loss: 11.365715980529785, acc: 0.9844, auc: 0.9934, precision: 0.9843, recall: 0.9843\n",
      "2019-09-23T11:02:01.837075, step: 1549, loss: 9.457024574279785, acc: 0.9766, auc: 0.998, precision: 0.9771, recall: 0.9756\n",
      "2019-09-23T11:02:03.831082, step: 1550, loss: 11.965362548828125, acc: 0.9688, auc: 0.9956, precision: 0.9704, recall: 0.9666\n",
      "2019-09-23T11:02:05.905933, step: 1551, loss: 7.415099143981934, acc: 0.9844, auc: 0.9985, precision: 0.9833, recall: 0.9857\n",
      "2019-09-23T11:02:07.842164, step: 1552, loss: 10.101362228393555, acc: 0.9688, auc: 0.9961, precision: 0.9687, recall: 0.9687\n",
      "2019-09-23T11:02:10.001043, step: 1553, loss: 20.70907211303711, acc: 0.9297, auc: 0.9956, precision: 0.9366, recall: 0.9318\n",
      "2019-09-23T11:02:11.928309, step: 1554, loss: 137.34365844726562, acc: 0.7344, auc: 0.939, precision: 0.7981, recall: 0.7649\n",
      "2019-09-23T11:02:13.780074, step: 1555, loss: 47.169517517089844, acc: 0.875, auc: 0.959, precision: 0.8887, recall: 0.875\n",
      "2019-09-23T11:02:15.828482, step: 1556, loss: 38.98260498046875, acc: 0.8906, auc: 0.959, precision: 0.8911, recall: 0.8904\n",
      "2019-09-23T11:02:17.656360, step: 1557, loss: 39.210845947265625, acc: 0.8359, auc: 0.9455, precision: 0.836, recall: 0.8358\n",
      "2019-09-23T11:02:19.764883, step: 1558, loss: 35.95830535888672, acc: 0.8984, auc: 0.9625, precision: 0.899, recall: 0.9005\n",
      "2019-09-23T11:02:21.656571, step: 1559, loss: 36.951175689697266, acc: 0.875, auc: 0.9619, precision: 0.8856, recall: 0.8724\n",
      "2019-09-23T11:02:23.541778, step: 1560, loss: 23.82101821899414, acc: 0.9219, auc: 0.9914, precision: 0.9255, recall: 0.9255\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "charEmbedding = data.charEmbedding\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    \n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        cnn = CharCNN(config, charEmbedding)\n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        \n",
    "        optimizer = tf.train.RMSPropOptimizer(config.training.learningRate)\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 使用 summary 绘制 tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"write to {} \\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"trainLoss\", cnn.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        def trainStep(batchX, batchY):\n",
    "            feed_dict = {\n",
    "                cnn.inputX: batchX, \n",
    "                cnn.inputY: batchY, \n",
    "                cnn.dropoutKeepProb: config.model.dropoutKeepProb, \n",
    "                cnn.isTraining: True\n",
    "            }\n",
    "            \n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run([trainOp, \n",
    "                                                                        summaryOp, \n",
    "                                                                        globalStep, \n",
    "                                                                        cnn.loss, \n",
    "                                                                        cnn.predictions, \n",
    "                                                                        cnn.binaryPreds], feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = getMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(\n",
    "                timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "        \n",
    "        \n",
    "        def devStep(batchX, batchY):\n",
    "            feed_dict = {\n",
    "                cnn.inputX: batchX, \n",
    "                cnn.inputY: batchY, \n",
    "                cnn.dropoutKeepProb: 1.0, \n",
    "                cnn.isTraining: False\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run([\n",
    "                    summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds\n",
    "                ], feed_dict)\n",
    "            acc, auc, precision, recall = getMetrics(batchY, predictions, binaryPreds)\n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        \n",
    "        \n",
    "        for index in range(config.training.epochs):\n",
    "            print(\"start to train models...\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "                currentStep = tf.train.global_step(sess, globalStep)\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\n Evaluation: \")\n",
    "                    losses, accs, aucs, precisions, recalls = [], [], [], [], []\n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                    \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(\n",
    "                        time_str, currentStep, mean(losses), mean(accs), mean(aucs), mean(precisions), mean(recalls)))\n",
    "                    \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
