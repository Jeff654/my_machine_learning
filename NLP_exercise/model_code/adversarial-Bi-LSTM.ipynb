{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os, datetime, time, random, json, warnings\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = \"\\\\\".join(os.getcwd().split(\"\\\\\")[:-1]) + \"\\\\data\"\n",
    "os.path.exists(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TrainingConfig(object):\n",
    "    epochs = 5\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "\n",
    "\n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    hiddenSizes = 128\n",
    "    dropoutKeepProb = 0.5\n",
    "    leRegLambda = 0.0\n",
    "    epsilon = 5\n",
    "\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200\n",
    "    batchSize = 128\n",
    "    dataSource = directory_path + \"\\\\preProcess\\\\labeledTrain.csv\"\n",
    "    stopWordSource = directory_path + \"\\\\english\"\n",
    "    \n",
    "    # 二分类设置为 1，多分类设置为其他数字\n",
    "    numClasses = 1\n",
    "    rate = 0.8\n",
    "    training = TrainingConfig()\n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words index freqd compute start with: 2019-09-26T18:58:24.416667, end with: 2019-09-26T21:45:30.625821\n"
     ]
    }
   ],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource\n",
    "        self._sequenceLength = config.sequenceLength\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = dict()\n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding = None\n",
    "        \n",
    "        # 统计词在多少个 review 中出现过\n",
    "        self.indexFreqs = []\n",
    "        self.labelList = []\n",
    "    \n",
    "    \n",
    "    def _readData(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if self.config.numClasses == 1:\n",
    "            labels = df[\"sentiment\"].tolist()\n",
    "        elif self.config.numClasses > 1:\n",
    "            labels =df[\"rate\"].tolist()\n",
    "        \n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "        \n",
    "        return reviews, labels\n",
    "    \n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        return labelIds\n",
    "    \n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "    \n",
    "    \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        reviews = []\n",
    "        for review in x:\n",
    "            if len(review) >= self._sequenceLength:\n",
    "                reviews.append(review[:self._sequenceLength])\n",
    "            else:\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "        \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels= np.array(y[trainIndex:], dtype=\"float32\")\n",
    "        \n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _getWordEmbedding(self, words):\n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(directory_path + \"\\\\word2vec\\\\word2Vec.bin\", \n",
    "                                                                 binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(\"{} is not exist...\")\n",
    "        \n",
    "        return vocab, wordEmbedding\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _getWordIndexFreqs(self, vocab, reviews, word2idx):\n",
    "        \"\"\"\n",
    "            本函数旨在统计词汇空间内各个词出现在多少个文本中，亦即：如\"科技\"出现在多少个review中\n",
    "        \"\"\"\n",
    "        reviewDicts = [dict(zip(review, range(len(review)))) for review in reviews]\n",
    "        indexFreqs = [0] * len(vocab)\n",
    "        \n",
    "        start_time = datetime.datetime.now().isoformat()\n",
    "        for word in vocab:\n",
    "            count = 0\n",
    "            for review in reviews:\n",
    "                if word in set(review):\n",
    "                    count += 1\n",
    "                \n",
    "            indexFreqs[word2idx[word]] = count\n",
    "        end_time = datetime.datetime.now().isoformat()\n",
    "        print(\"words index freqd compute start with: {}, end with: {}\".format(start_time, end_time))\n",
    "        \n",
    "        self.indexFreqs = indexFreqs\n",
    "      \n",
    "    \n",
    "    def _genVocabulary(self, reviews, labels):\n",
    "        all_words = [word for review in reviews for word in review]\n",
    "        subWords = [word for word in all_words if word not in self.stopWordDict]\n",
    "        wordCount = Counter(subWords)\n",
    "        sortedWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        words = [item[0] for item in sortedWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._getWordIndexFreqs(vocab, reviews, word2idx)\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        with open(directory_path + \"\\\\wordJson\\\\word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        with open(directory_path + \"\\\\wordJson\\\\label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "        \n",
    "    \n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "    \n",
    "    \n",
    "    def dataGen(self):\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        self.trainReviews, self.trainLabels, self.evalReviews, self.evalLabels = self._genTrainEvalData(reviewIds, \n",
    "                                                                                                       labelIds, \n",
    "                                                                                                       word2idx, \n",
    "                                                                                                       self._rate)\n",
    "    \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextBatch(x,y, batchSize):\n",
    "    perm = np.arange(len(x))\n",
    "    np.random.shuffle(perm)\n",
    "    x = x[perm]\n",
    "    y = y[perm]\n",
    "    numBatches = len(x) // batchSize\n",
    "    \n",
    "    for index in range(numBatches):\n",
    "        start = index * batchSize\n",
    "        end = start + batchSize\n",
    "        batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "        batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "        \n",
    "        yield batchX, batchY\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(item: list) -> float:\n",
    "    \"\"\"\n",
    "    计算列表中元素的平均值\n",
    "    :param item: 列表对象\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    \"\"\"\n",
    "    计算二类和多类的准确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == true_y[i]:\n",
    "            corr += 1\n",
    "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "    return acc\n",
    "\n",
    "\n",
    "def binary_precision(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的精确率计算\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    pred_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == positive:\n",
    "            pred_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "    return prec\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    true_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if true_y[i] == positive:\n",
    "            true_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    rec = corr / true_corr if true_corr > 0 else 0\n",
    "    return rec\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    \"\"\"\n",
    "    二类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param beta: beta值\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    try:\n",
    "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    except:\n",
    "        f_b = 0\n",
    "    return f_b\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的精确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    \"\"\"\n",
    "    多类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :param beta: beta值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到二分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到多分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param labels:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = multi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdversarialLSTM(object):\n",
    "    def __init__(self, config, wordEmbedding, indexFreqs):\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        self.dropoutKeepProb= tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.config = config\n",
    "        \n",
    "        # 根据词频计算权重\n",
    "#         print(\"the index of frequence is: \", indexFreqs)\n",
    "        indexFreqs[0], indexFreqs[1] = 20000, 10000\n",
    "        weights = tf.cast(tf.reshape(indexFreqs / tf.reduce_sum(indexFreqs), [1, len(indexFreqs)]), dtype=tf.float32)\n",
    "        \n",
    "#         print(\"the value of weights: \", weights)\n",
    "#         print(\"the value of wordEmbedding: \", wordEmbedding)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # word-embedding layer\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            # 利用词频计算新的 word-embedding matrix\n",
    "            normWordEmbedding = self._normalize(tf.cast(np.array(wordEmbedding), dtype=tf.float32, name=\"word2vec\"), weights)\n",
    "            # 利用 word-embedding matrix 将输入的数据中的词转换成 word-vector，维度为：\n",
    "            # [batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(normWordEmbedding, self.inputX)\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n",
    "                self.logits = self._Bi_LSTMAttention(self.embeddedWords)\n",
    "                if config.numClasses == 1:\n",
    "                    self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.float32, name=\"predictions\")\n",
    "                    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                    labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                                  dtype=tf.float32))\n",
    "                elif config.numClasses > 1:\n",
    "                    self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "                    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "                \n",
    "                loss = tf.reduce_mean(losses)\n",
    "       \n",
    "        \n",
    "        with tf.name_scope(\"perturLoss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n",
    "                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n",
    "                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n",
    "                perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, \n",
    "                                                                      labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                                    dtype=tf.float32))\n",
    "                perturLoss = tf.reduce_mean(perturLosses)\n",
    "        \n",
    "        self.loss = loss + perturLoss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _Bi_LSTMAttention(self, embeddedWords):\n",
    "        config = self.config\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=config.model.hiddenSizes, \n",
    "                                                                              state_is_tuple=True), \n",
    "                                                      output_keep_prob=self.dropoutKeepProb)\n",
    "            lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=config.model.hiddenSizes, \n",
    "                                                                              state_is_tuple=True), \n",
    "                                                      output_keep_prob=self.dropoutKeepProb)\n",
    "            \n",
    "            outputs_, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                          self.embeddedWords, dtype=tf.float32, \n",
    "                                                                          scope=\"bi-lstm\")\n",
    "        \n",
    "        # 论文中是将前向和后向相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs_[0] + outputs_[1]\n",
    "            output = self._attention(H)\n",
    "            outputSize = config.model.hiddenSizes\n",
    "        \n",
    "        # FC layer\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\"outputW\", shape=[outputSize, config.numClasses], \n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB = tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "        \n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def _attention(self, H):\n",
    "        hiddenSize = config.model.hiddenSizes\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # newM shape = [batch_size, time_step, 1]\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # restoreM shape = [batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 针对alpha对H进行加权\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成为二维[batch_size, time_step]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        sentence_representation = tf.tanh(sequeezeR)\n",
    "        output = tf.nn.dropout(sentence_representation, self.dropoutKeepProb)\n",
    "        \n",
    "        return output\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _normalize(self, wordEmbedding, weights):\n",
    "        \"\"\"\n",
    "            对 wordEmbedding 结合权重做标准化处理\n",
    "        \"\"\"\n",
    "        mean = tf.matmul(weights, wordEmbedding)\n",
    "#         print(\"the weight mean of wordEmbedding: \", mean)\n",
    "        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n",
    "        \n",
    "        var = tf.matmul(weights, powWordEmbedding)\n",
    "#         print(\"the weight var of powWordEmbedding is: \", var)\n",
    "        stddev = tf.sqrt(1e-6 + var)\n",
    "        \n",
    "        return (wordEmbedding - mean) / stddev\n",
    "    \n",
    "    \n",
    "    def _addPerturbation(self, embedded, loss):\n",
    "        \"\"\"\n",
    "            在原始的word-embedding中添加扰动\n",
    "        \"\"\"\n",
    "        grad, = tf.gradients(loss, embedded, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n",
    "        grad = tf.stop_gradient(grad)\n",
    "        perturb = self._scaleL2(grad, self.config.model.epsilon)\n",
    "    \n",
    "        return embedded + perturb\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _scaleL2(self, x, norm_length):\n",
    "        # x.shape = [batch, num_time_steps, dim]\n",
    "        alpha =tf.reduce_max(tf.abs(x), (1, 2), keepdims=True) + 1e-12\n",
    "        l2_norm = alpha * tf.sqrt(tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keepdims=True) + 1e-6)\n",
    "        x_unit = x / l2_norm\n",
    "        \n",
    "        return norm_length * x_unit\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "wordEmbedding = data.wordEmbedding\n",
    "indexFreqs = data.indexFreqs\n",
    "labelList = data.labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write to C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model_code\\summary \n",
      "\n",
      "begin to train model...\n",
      "the 0-th / 5 epoch\n",
      "training: time: 2019-09-27T11:27:05.772773, step: 1, loss: 1.5065041780471802, acc: 0.515625, recall: 0.9230769230769231, precision: 0.5128205128205128, f_beta: 0.6593406593406593\n",
      "training: time: 2019-09-27T11:27:09.458243, step: 2, loss: 2.1197540760040283, acc: 0.4921875, recall: 0.0, precision: 0, f_beta: 0\n",
      "training: time: 2019-09-27T11:27:13.345452, step: 3, loss: 1.64058256149292, acc: 0.5, recall: 0.015384615384615385, precision: 1.0, f_beta: 0.030303030303030307\n",
      "training: time: 2019-09-27T11:27:17.736116, step: 4, loss: 1.352006196975708, acc: 0.5859375, recall: 0.4915254237288136, precision: 0.5576923076923077, f_beta: 0.5225225225225225\n",
      "training: time: 2019-09-27T11:27:21.861287, step: 5, loss: 1.4735063314437866, acc: 0.546875, recall: 0.8805970149253731, precision: 0.5412844036697247, f_beta: 0.6704545454545454\n",
      "training: time: 2019-09-27T11:27:25.738556, step: 6, loss: 1.3882077932357788, acc: 0.59375, recall: 0.9324324324324325, precision: 0.5948275862068966, f_beta: 0.7263157894736842\n",
      "training: time: 2019-09-27T11:27:29.688224, step: 7, loss: 1.4449055194854736, acc: 0.5859375, recall: 0.9264705882352942, precision: 0.5675675675675675, f_beta: 0.7039106145251396\n",
      "training: time: 2019-09-27T11:27:33.455194, step: 8, loss: 1.6224358081817627, acc: 0.4296875, recall: 0.9347826086956522, precision: 0.3805309734513274, f_beta: 0.5408805031446541\n",
      "training: time: 2019-09-27T11:27:37.041910, step: 9, loss: 1.4385485649108887, acc: 0.484375, recall: 0.7586206896551724, precision: 0.4583333333333333, f_beta: 0.5714285714285715\n",
      "training: time: 2019-09-27T11:27:41.065895, step: 10, loss: 1.3875563144683838, acc: 0.484375, recall: 0.4576271186440678, precision: 0.4426229508196721, f_beta: 0.45\n",
      "training: time: 2019-09-27T11:27:44.951315, step: 11, loss: 1.4365415573120117, acc: 0.4609375, recall: 0.3246753246753247, precision: 0.5952380952380952, f_beta: 0.42016806722689076\n",
      "training: time: 2019-09-27T11:27:48.823667, step: 12, loss: 1.3395739793777466, acc: 0.6171875, recall: 0.3018867924528302, precision: 0.5714285714285714, f_beta: 0.3950617283950617\n",
      "training: time: 2019-09-27T11:27:52.934448, step: 13, loss: 1.3993147611618042, acc: 0.546875, recall: 0.21212121212121213, precision: 0.7, f_beta: 0.3255813953488372\n",
      "training: time: 2019-09-27T11:27:56.886214, step: 14, loss: 1.3201217651367188, acc: 0.578125, recall: 0.19672131147540983, precision: 0.7058823529411765, f_beta: 0.30769230769230765\n",
      "training: time: 2019-09-27T11:28:00.740512, step: 15, loss: 1.4368934631347656, acc: 0.46875, recall: 0.14492753623188406, precision: 0.5263157894736842, f_beta: 0.2272727272727273\n",
      "training: time: 2019-09-27T11:28:04.247340, step: 16, loss: 1.334934949874878, acc: 0.53125, recall: 0.26153846153846155, precision: 0.5862068965517241, f_beta: 0.36170212765957444\n",
      "training: time: 2019-09-27T11:28:07.911212, step: 17, loss: 1.3252627849578857, acc: 0.546875, recall: 0.36231884057971014, precision: 0.6410256410256411, f_beta: 0.46296296296296297\n",
      "training: time: 2019-09-27T11:28:11.509300, step: 18, loss: 1.2964649200439453, acc: 0.6015625, recall: 0.48484848484848486, precision: 0.6530612244897959, f_beta: 0.5565217391304348\n",
      "training: time: 2019-09-27T11:28:15.119430, step: 19, loss: 1.232079029083252, acc: 0.609375, recall: 0.5303030303030303, precision: 0.6481481481481481, f_beta: 0.5833333333333333\n",
      "training: time: 2019-09-27T11:28:18.640828, step: 20, loss: 1.334572196006775, acc: 0.6171875, recall: 0.711864406779661, precision: 0.5675675675675675, f_beta: 0.631578947368421\n",
      "training: time: 2019-09-27T11:28:22.109755, step: 21, loss: 1.2314667701721191, acc: 0.6328125, recall: 0.7666666666666667, precision: 0.5822784810126582, f_beta: 0.6618705035971223\n",
      "training: time: 2019-09-27T11:28:25.495681, step: 22, loss: 1.2479145526885986, acc: 0.671875, recall: 0.7833333333333333, precision: 0.618421052631579, f_beta: 0.6911764705882353\n",
      "training: time: 2019-09-27T11:28:28.922366, step: 23, loss: 1.2511200904846191, acc: 0.671875, recall: 0.746031746031746, precision: 0.6438356164383562, f_beta: 0.6911764705882354\n",
      "training: time: 2019-09-27T11:28:32.216026, step: 24, loss: 1.1960220336914062, acc: 0.7265625, recall: 0.7142857142857143, precision: 0.625, f_beta: 0.6666666666666666\n",
      "training: time: 2019-09-27T11:28:35.566480, step: 25, loss: 1.1737481355667114, acc: 0.671875, recall: 0.6417910447761194, precision: 0.7049180327868853, f_beta: 0.671875\n",
      "training: time: 2019-09-27T11:28:38.874944, step: 26, loss: 1.1671640872955322, acc: 0.6796875, recall: 0.6666666666666666, precision: 0.6333333333333333, f_beta: 0.6495726495726496\n",
      "training: time: 2019-09-27T11:28:42.159717, step: 27, loss: 1.0892329216003418, acc: 0.765625, recall: 0.6617647058823529, precision: 0.8653846153846154, f_beta: 0.75\n",
      "training: time: 2019-09-27T11:28:45.506123, step: 28, loss: 1.1542117595672607, acc: 0.78125, recall: 0.7121212121212122, precision: 0.8392857142857143, f_beta: 0.7704918032786886\n",
      "training: time: 2019-09-27T11:28:48.853620, step: 29, loss: 1.1272192001342773, acc: 0.71875, recall: 0.620253164556962, precision: 0.8909090909090909, f_beta: 0.7313432835820896\n",
      "training: time: 2019-09-27T11:28:52.216981, step: 30, loss: 0.9923490285873413, acc: 0.84375, recall: 0.7846153846153846, precision: 0.8947368421052632, f_beta: 0.8360655737704918\n",
      "training: time: 2019-09-27T11:28:55.541835, step: 31, loss: 1.0697391033172607, acc: 0.765625, recall: 0.734375, precision: 0.7833333333333333, f_beta: 0.7580645161290323\n",
      "training: time: 2019-09-27T11:28:58.876104, step: 32, loss: 0.949709415435791, acc: 0.828125, recall: 0.8194444444444444, precision: 0.8676470588235294, f_beta: 0.8428571428571429\n",
      "training: time: 2019-09-27T11:29:02.248716, step: 33, loss: 0.8979711532592773, acc: 0.796875, recall: 0.7922077922077922, precision: 0.8591549295774648, f_beta: 0.8243243243243243\n",
      "training: time: 2019-09-27T11:29:05.551920, step: 34, loss: 1.014697551727295, acc: 0.78125, recall: 0.8571428571428571, precision: 0.7692307692307693, f_beta: 0.8108108108108107\n",
      "training: time: 2019-09-27T11:29:08.839921, step: 35, loss: 1.128366231918335, acc: 0.703125, recall: 0.8307692307692308, precision: 0.6666666666666666, f_beta: 0.7397260273972603\n",
      "training: time: 2019-09-27T11:29:12.188490, step: 36, loss: 0.9525878429412842, acc: 0.796875, recall: 0.8428571428571429, precision: 0.7972972972972973, f_beta: 0.8194444444444444\n",
      "training: time: 2019-09-27T11:29:15.475040, step: 37, loss: 1.0815109014511108, acc: 0.7109375, recall: 0.8181818181818182, precision: 0.6835443037974683, f_beta: 0.7448275862068965\n",
      "training: time: 2019-09-27T11:29:18.725041, step: 38, loss: 1.0038586854934692, acc: 0.7734375, recall: 0.8070175438596491, precision: 0.71875, f_beta: 0.7603305785123967\n",
      "training: time: 2019-09-27T11:29:22.123086, step: 39, loss: 0.9099410772323608, acc: 0.8203125, recall: 0.7714285714285715, precision: 0.8852459016393442, f_beta: 0.8244274809160306\n",
      "training: time: 2019-09-27T11:29:25.972668, step: 40, loss: 0.9903258085250854, acc: 0.765625, recall: 0.6730769230769231, precision: 0.7291666666666666, f_beta: 0.7\n",
      "training: time: 2019-09-27T11:29:29.308000, step: 41, loss: 0.9354175329208374, acc: 0.796875, recall: 0.7910447761194029, precision: 0.8153846153846154, f_beta: 0.803030303030303\n",
      "training: time: 2019-09-27T11:29:32.834934, step: 42, loss: 0.8627283573150635, acc: 0.8046875, recall: 0.7540983606557377, precision: 0.8214285714285714, f_beta: 0.7863247863247863\n",
      "training: time: 2019-09-27T11:29:36.242885, step: 43, loss: 0.8448084592819214, acc: 0.796875, recall: 0.7049180327868853, precision: 0.8431372549019608, f_beta: 0.7678571428571429\n",
      "training: time: 2019-09-27T11:29:40.259068, step: 44, loss: 0.9327555894851685, acc: 0.8125, recall: 0.7586206896551724, precision: 0.8148148148148148, f_beta: 0.7857142857142857\n",
      "training: time: 2019-09-27T11:29:44.060137, step: 45, loss: 0.9919462203979492, acc: 0.734375, recall: 0.71875, precision: 0.7419354838709677, f_beta: 0.7301587301587302\n",
      "training: time: 2019-09-27T11:29:47.561769, step: 46, loss: 0.7342328429222107, acc: 0.84375, recall: 0.85, precision: 0.8225806451612904, f_beta: 0.8360655737704918\n",
      "training: time: 2019-09-27T11:29:51.285012, step: 47, loss: 0.8371452689170837, acc: 0.7890625, recall: 0.8194444444444444, precision: 0.8082191780821918, f_beta: 0.8137931034482759\n",
      "training: time: 2019-09-27T11:29:54.892015, step: 48, loss: 0.8181593418121338, acc: 0.8125, recall: 0.8985507246376812, precision: 0.7848101265822784, f_beta: 0.8378378378378378\n",
      "training: time: 2019-09-27T11:29:58.172561, step: 49, loss: 0.9282349348068237, acc: 0.7890625, recall: 0.8656716417910447, precision: 0.7631578947368421, f_beta: 0.8111888111888113\n",
      "training: time: 2019-09-27T11:30:01.577765, step: 50, loss: 0.8478007912635803, acc: 0.8046875, recall: 0.864406779661017, precision: 0.75, f_beta: 0.8031496062992127\n",
      "training: time: 2019-09-27T11:30:04.935582, step: 51, loss: 0.8220786452293396, acc: 0.8515625, recall: 0.9322033898305084, precision: 0.7857142857142857, f_beta: 0.8527131782945736\n",
      "training: time: 2019-09-27T11:30:08.510260, step: 52, loss: 0.58587646484375, acc: 0.875, recall: 0.8805970149253731, precision: 0.8805970149253731, f_beta: 0.8805970149253731\n",
      "training: time: 2019-09-27T11:30:12.694489, step: 53, loss: 0.9091055393218994, acc: 0.8125, recall: 0.7794117647058824, precision: 0.8548387096774194, f_beta: 0.8153846153846154\n",
      "training: time: 2019-09-27T11:30:17.162313, step: 54, loss: 0.7477006912231445, acc: 0.828125, recall: 0.847457627118644, precision: 0.7936507936507936, f_beta: 0.819672131147541\n",
      "training: time: 2019-09-27T11:30:20.880768, step: 55, loss: 0.7618248462677002, acc: 0.84375, recall: 0.8181818181818182, precision: 0.8709677419354839, f_beta: 0.84375\n",
      "training: time: 2019-09-27T11:30:24.682679, step: 56, loss: 0.6590379476547241, acc: 0.875, recall: 0.8695652173913043, precision: 0.8955223880597015, f_beta: 0.8823529411764706\n",
      "training: time: 2019-09-27T11:30:28.557233, step: 57, loss: 0.6209423542022705, acc: 0.8984375, recall: 0.8666666666666667, precision: 0.9122807017543859, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T11:30:32.000428, step: 58, loss: 0.6769810914993286, acc: 0.84375, recall: 0.8421052631578947, precision: 0.8135593220338984, f_beta: 0.8275862068965518\n",
      "training: time: 2019-09-27T11:30:35.415222, step: 59, loss: 0.8146440982818604, acc: 0.8046875, recall: 0.8307692307692308, precision: 0.7941176470588235, f_beta: 0.8120300751879699\n",
      "training: time: 2019-09-27T11:30:38.961119, step: 60, loss: 0.6566694974899292, acc: 0.859375, recall: 0.8529411764705882, precision: 0.8787878787878788, f_beta: 0.8656716417910447\n",
      "training: time: 2019-09-27T11:30:42.393643, step: 61, loss: 0.5452691316604614, acc: 0.890625, recall: 0.8985507246376812, precision: 0.8985507246376812, f_beta: 0.8985507246376812\n",
      "training: time: 2019-09-27T11:30:45.948630, step: 62, loss: 0.7049437761306763, acc: 0.859375, recall: 0.8636363636363636, precision: 0.8636363636363636, f_beta: 0.8636363636363636\n",
      "training: time: 2019-09-27T11:30:49.658108, step: 63, loss: 0.6983820199966431, acc: 0.828125, recall: 0.8676470588235294, precision: 0.8194444444444444, f_beta: 0.8428571428571429\n",
      "training: time: 2019-09-27T11:30:52.947417, step: 64, loss: 0.7322502136230469, acc: 0.8046875, recall: 0.9180327868852459, precision: 0.7368421052631579, f_beta: 0.8175182481751824\n",
      "training: time: 2019-09-27T11:30:56.673541, step: 65, loss: 0.9318868517875671, acc: 0.8359375, recall: 0.8208955223880597, precision: 0.859375, f_beta: 0.8396946564885497\n",
      "training: time: 2019-09-27T11:31:00.248657, step: 66, loss: 0.8031579256057739, acc: 0.84375, recall: 0.8666666666666667, precision: 0.8125, f_beta: 0.8387096774193549\n",
      "training: time: 2019-09-27T11:31:03.815488, step: 67, loss: 0.5427716970443726, acc: 0.90625, recall: 0.875, precision: 0.9333333333333333, f_beta: 0.9032258064516129\n",
      "training: time: 2019-09-27T11:31:07.286029, step: 68, loss: 0.5547194480895996, acc: 0.875, recall: 0.828125, precision: 0.9137931034482759, f_beta: 0.8688524590163935\n",
      "training: time: 2019-09-27T11:31:10.695505, step: 69, loss: 0.6333315372467041, acc: 0.84375, recall: 0.8947368421052632, precision: 0.85, f_beta: 0.8717948717948718\n",
      "training: time: 2019-09-27T11:31:14.215901, step: 70, loss: 0.8047705292701721, acc: 0.828125, recall: 0.8461538461538461, precision: 0.8208955223880597, f_beta: 0.8333333333333334\n",
      "training: time: 2019-09-27T11:31:17.599844, step: 71, loss: 0.5815533399581909, acc: 0.890625, recall: 0.9, precision: 0.8709677419354839, f_beta: 0.8852459016393444\n",
      "training: time: 2019-09-27T11:31:21.269795, step: 72, loss: 0.6174095869064331, acc: 0.859375, recall: 0.873015873015873, precision: 0.8461538461538461, f_beta: 0.859375\n",
      "training: time: 2019-09-27T11:31:25.001421, step: 73, loss: 0.6279588341712952, acc: 0.8828125, recall: 0.8709677419354839, precision: 0.8852459016393442, f_beta: 0.8780487804878049\n",
      "training: time: 2019-09-27T11:31:28.433230, step: 74, loss: 0.6243658065795898, acc: 0.875, recall: 0.9411764705882353, precision: 0.8421052631578947, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T11:31:33.431071, step: 75, loss: 0.6493545770645142, acc: 0.8203125, recall: 0.8153846153846154, precision: 0.828125, f_beta: 0.8217054263565892\n",
      "training: time: 2019-09-27T11:31:37.655818, step: 76, loss: 0.654014527797699, acc: 0.8515625, recall: 0.8181818181818182, precision: 0.8852459016393442, f_beta: 0.8503937007874016\n",
      "training: time: 2019-09-27T11:31:41.373490, step: 77, loss: 0.7297213077545166, acc: 0.859375, recall: 0.8103448275862069, precision: 0.8703703703703703, f_beta: 0.8392857142857144\n",
      "training: time: 2019-09-27T11:31:44.893152, step: 78, loss: 0.707446813583374, acc: 0.8515625, recall: 0.859375, precision: 0.8461538461538461, f_beta: 0.8527131782945736\n",
      "training: time: 2019-09-27T11:31:48.177232, step: 79, loss: 0.7945809364318848, acc: 0.8046875, recall: 0.7692307692307693, precision: 0.8333333333333334, f_beta: 0.8\n",
      "training: time: 2019-09-27T11:31:51.673636, step: 80, loss: 0.6985078454017639, acc: 0.8515625, recall: 0.8955223880597015, precision: 0.8333333333333334, f_beta: 0.8633093525179857\n",
      "training: time: 2019-09-27T11:31:55.057547, step: 81, loss: 0.5204063653945923, acc: 0.921875, recall: 0.9558823529411765, precision: 0.9027777777777778, f_beta: 0.9285714285714286\n",
      "training: time: 2019-09-27T11:31:58.393172, step: 82, loss: 0.7754956483840942, acc: 0.828125, recall: 0.8636363636363636, precision: 0.8142857142857143, f_beta: 0.8382352941176471\n",
      "training: time: 2019-09-27T11:32:01.812816, step: 83, loss: 0.6847835779190063, acc: 0.8515625, recall: 0.9354838709677419, precision: 0.7945205479452054, f_beta: 0.8592592592592593\n",
      "training: time: 2019-09-27T11:32:05.297484, step: 84, loss: 0.5936983227729797, acc: 0.8671875, recall: 0.8970588235294118, precision: 0.8591549295774648, f_beta: 0.8776978417266187\n",
      "training: time: 2019-09-27T11:32:09.080511, step: 85, loss: 0.7802456617355347, acc: 0.796875, recall: 0.7704918032786885, precision: 0.7966101694915254, f_beta: 0.7833333333333333\n",
      "training: time: 2019-09-27T11:32:12.742002, step: 86, loss: 0.8279459476470947, acc: 0.8125, recall: 0.8679245283018868, precision: 0.7301587301587301, f_beta: 0.7931034482758621\n",
      "training: time: 2019-09-27T11:32:16.202961, step: 87, loss: 0.7217058539390564, acc: 0.859375, recall: 0.8166666666666667, precision: 0.875, f_beta: 0.8448275862068966\n",
      "training: time: 2019-09-27T11:32:19.945021, step: 88, loss: 0.6237065196037292, acc: 0.890625, recall: 0.8604651162790697, precision: 0.8222222222222222, f_beta: 0.8409090909090908\n",
      "training: time: 2019-09-27T11:32:23.705467, step: 89, loss: 0.8245236873626709, acc: 0.796875, recall: 0.6944444444444444, precision: 0.9259259259259259, f_beta: 0.7936507936507936\n",
      "training: time: 2019-09-27T11:32:27.104611, step: 90, loss: 0.9544326066970825, acc: 0.796875, recall: 0.6923076923076923, precision: 0.8823529411764706, f_beta: 0.7758620689655172\n",
      "training: time: 2019-09-27T11:32:30.409480, step: 91, loss: 0.6342276930809021, acc: 0.8671875, recall: 0.8387096774193549, precision: 0.8813559322033898, f_beta: 0.859504132231405\n",
      "training: time: 2019-09-27T11:32:33.965260, step: 92, loss: 0.7836718559265137, acc: 0.8515625, recall: 0.8615384615384616, precision: 0.8484848484848485, f_beta: 0.8549618320610687\n",
      "training: time: 2019-09-27T11:32:37.398235, step: 93, loss: 0.610741138458252, acc: 0.8515625, recall: 0.9354838709677419, precision: 0.7945205479452054, f_beta: 0.8592592592592593\n",
      "training: time: 2019-09-27T11:32:40.647649, step: 94, loss: 0.8076753616333008, acc: 0.84375, recall: 0.9078947368421053, precision: 0.8414634146341463, f_beta: 0.8734177215189874\n",
      "training: time: 2019-09-27T11:32:43.981041, step: 95, loss: 0.6483398675918579, acc: 0.859375, recall: 0.9428571428571428, precision: 0.825, f_beta: 0.88\n",
      "training: time: 2019-09-27T11:32:47.519211, step: 96, loss: 0.7109971046447754, acc: 0.8671875, recall: 0.9324324324324325, precision: 0.8518518518518519, f_beta: 0.8903225806451613\n",
      "training: time: 2019-09-27T11:32:50.895808, step: 97, loss: 0.6819809079170227, acc: 0.875, recall: 0.9137931034482759, precision: 0.828125, f_beta: 0.8688524590163935\n",
      "training: time: 2019-09-27T11:32:54.168847, step: 98, loss: 0.8128814697265625, acc: 0.78125, recall: 0.7536231884057971, precision: 0.8253968253968254, f_beta: 0.787878787878788\n",
      "training: time: 2019-09-27T11:32:57.435156, step: 99, loss: 0.7357637286186218, acc: 0.8203125, recall: 0.8064516129032258, precision: 0.819672131147541, f_beta: 0.8130081300813008\n",
      "training: time: 2019-09-27T11:33:00.760766, step: 100, loss: 0.49635642766952515, acc: 0.875, recall: 0.8571428571428571, precision: 0.9090909090909091, f_beta: 0.8823529411764706\n",
      "begin to evaluation: \n",
      "\n",
      "evaluation: time: 2019-09-27T11:35:22.859914, step: 100, loss: 0.6408616243264614, acc: 0.8635817307692307, recall: 0.8562594182644844, precision: 0.873365101642605, f_beta: 0.8636119315170557\n",
      "save model checkpoint to C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\adversarialLSTM\\model\\adversarialLSTM-100 \n",
      "\n",
      "training: time: 2019-09-27T11:35:30.983967, step: 101, loss: 0.5874234437942505, acc: 0.921875, recall: 0.875, precision: 0.9423076923076923, f_beta: 0.9074074074074073\n",
      "training: time: 2019-09-27T11:35:34.342459, step: 102, loss: 0.6655499339103699, acc: 0.875, recall: 0.864406779661017, precision: 0.864406779661017, f_beta: 0.864406779661017\n",
      "training: time: 2019-09-27T11:35:37.805915, step: 103, loss: 0.6523396968841553, acc: 0.859375, recall: 0.8709677419354839, precision: 0.84375, f_beta: 0.8571428571428571\n",
      "training: time: 2019-09-27T11:35:41.301627, step: 104, loss: 0.6828960180282593, acc: 0.8203125, recall: 0.828125, precision: 0.8153846153846154, f_beta: 0.8217054263565892\n",
      "training: time: 2019-09-27T11:35:44.919958, step: 105, loss: 0.7301944494247437, acc: 0.828125, recall: 0.7391304347826086, precision: 0.9272727272727272, f_beta: 0.8225806451612903\n",
      "training: time: 2019-09-27T11:35:48.230750, step: 106, loss: 0.6305167078971863, acc: 0.890625, recall: 0.9253731343283582, precision: 0.8732394366197183, f_beta: 0.8985507246376812\n",
      "training: time: 2019-09-27T11:35:51.708175, step: 107, loss: 0.6450549960136414, acc: 0.8515625, recall: 0.84375, precision: 0.8571428571428571, f_beta: 0.8503937007874015\n",
      "training: time: 2019-09-27T11:35:55.516891, step: 108, loss: 0.7422789931297302, acc: 0.8203125, recall: 0.8688524590163934, precision: 0.7794117647058824, f_beta: 0.8217054263565892\n",
      "training: time: 2019-09-27T11:36:00.138054, step: 109, loss: 0.7234044671058655, acc: 0.8515625, recall: 0.9354838709677419, precision: 0.7945205479452054, f_beta: 0.8592592592592593\n",
      "training: time: 2019-09-27T11:36:03.670042, step: 110, loss: 0.73179030418396, acc: 0.8515625, recall: 0.9142857142857143, precision: 0.8311688311688312, f_beta: 0.8707482993197279\n",
      "training: time: 2019-09-27T11:36:07.339106, step: 111, loss: 0.7200591564178467, acc: 0.84375, recall: 0.859375, precision: 0.8333333333333334, f_beta: 0.8461538461538461\n",
      "training: time: 2019-09-27T11:36:10.903061, step: 112, loss: 0.6347184181213379, acc: 0.875, recall: 0.9298245614035088, precision: 0.8153846153846154, f_beta: 0.8688524590163934\n",
      "training: time: 2019-09-27T11:36:14.565411, step: 113, loss: 0.8617895841598511, acc: 0.8359375, recall: 0.8596491228070176, precision: 0.7903225806451613, f_beta: 0.823529411764706\n",
      "training: time: 2019-09-27T11:36:18.813378, step: 114, loss: 0.5339722037315369, acc: 0.90625, recall: 0.9272727272727272, precision: 0.864406779661017, f_beta: 0.8947368421052632\n",
      "training: time: 2019-09-27T11:36:22.401280, step: 115, loss: 0.47694045305252075, acc: 0.90625, recall: 0.8571428571428571, precision: 0.9473684210526315, f_beta: 0.9\n",
      "training: time: 2019-09-27T11:36:25.930342, step: 116, loss: 0.5861222743988037, acc: 0.890625, recall: 0.7936507936507936, precision: 0.9803921568627451, f_beta: 0.8771929824561403\n",
      "training: time: 2019-09-27T11:36:29.561634, step: 117, loss: 0.6488783359527588, acc: 0.8515625, recall: 0.7586206896551724, precision: 0.8979591836734694, f_beta: 0.822429906542056\n",
      "training: time: 2019-09-27T11:36:32.840964, step: 118, loss: 0.5835629105567932, acc: 0.875, recall: 0.859375, precision: 0.8870967741935484, f_beta: 0.8730158730158729\n",
      "training: time: 2019-09-27T11:36:36.508431, step: 119, loss: 0.6856533288955688, acc: 0.8671875, recall: 0.8153846153846154, precision: 0.9137931034482759, f_beta: 0.8617886178861788\n",
      "training: time: 2019-09-27T11:36:40.076921, step: 120, loss: 0.5901403427124023, acc: 0.8671875, recall: 0.9090909090909091, precision: 0.8450704225352113, f_beta: 0.8759124087591241\n",
      "training: time: 2019-09-27T11:36:43.513439, step: 121, loss: 0.651706337928772, acc: 0.8671875, recall: 0.8695652173913043, precision: 0.8823529411764706, f_beta: 0.8759124087591241\n",
      "training: time: 2019-09-27T11:36:46.852012, step: 122, loss: 0.6134326457977295, acc: 0.8828125, recall: 0.8928571428571429, precision: 0.847457627118644, f_beta: 0.8695652173913044\n",
      "training: time: 2019-09-27T11:36:50.382264, step: 123, loss: 0.7342173457145691, acc: 0.8515625, recall: 0.8245614035087719, precision: 0.8392857142857143, f_beta: 0.8318584070796461\n",
      "training: time: 2019-09-27T11:36:54.803375, step: 124, loss: 0.3780970573425293, acc: 0.9296875, recall: 0.95, precision: 0.9047619047619048, f_beta: 0.9268292682926829\n",
      "training: time: 2019-09-27T11:36:58.357753, step: 125, loss: 0.6457341909408569, acc: 0.8515625, recall: 0.8873239436619719, precision: 0.8513513513513513, f_beta: 0.8689655172413793\n",
      "training: time: 2019-09-27T11:37:01.841560, step: 126, loss: 0.5578396320343018, acc: 0.921875, recall: 0.9354838709677419, precision: 0.90625, f_beta: 0.9206349206349206\n",
      "training: time: 2019-09-27T11:37:05.354878, step: 127, loss: 0.6462298631668091, acc: 0.84375, recall: 0.8493150684931506, precision: 0.8732394366197183, f_beta: 0.861111111111111\n",
      "training: time: 2019-09-27T11:37:08.954314, step: 128, loss: 0.504083514213562, acc: 0.90625, recall: 0.8727272727272727, precision: 0.9056603773584906, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T11:37:12.591290, step: 129, loss: 0.6091833710670471, acc: 0.90625, recall: 0.8571428571428571, precision: 0.9230769230769231, f_beta: 0.888888888888889\n",
      "training: time: 2019-09-27T11:37:16.051955, step: 130, loss: 0.518720805644989, acc: 0.8828125, recall: 0.8923076923076924, precision: 0.8787878787878788, f_beta: 0.8854961832061069\n",
      "training: time: 2019-09-27T11:37:19.495419, step: 131, loss: 0.742046594619751, acc: 0.8671875, recall: 0.8333333333333334, precision: 0.8771929824561403, f_beta: 0.8547008547008548\n",
      "training: time: 2019-09-27T11:37:23.062383, step: 132, loss: 0.6682270169258118, acc: 0.890625, recall: 0.8507462686567164, precision: 0.9344262295081968, f_beta: 0.8906250000000001\n",
      "training: time: 2019-09-27T11:37:26.444219, step: 133, loss: 0.6563432812690735, acc: 0.875, recall: 0.8225806451612904, precision: 0.9107142857142857, f_beta: 0.864406779661017\n",
      "training: time: 2019-09-27T11:37:29.894519, step: 134, loss: 0.7343971729278564, acc: 0.8359375, recall: 0.7910447761194029, precision: 0.8833333333333333, f_beta: 0.8346456692913385\n",
      "training: time: 2019-09-27T11:37:33.348292, step: 135, loss: 0.5769509077072144, acc: 0.875, recall: 0.896551724137931, precision: 0.8387096774193549, f_beta: 0.8666666666666666\n",
      "training: time: 2019-09-27T11:37:36.732349, step: 136, loss: 0.4418438971042633, acc: 0.921875, recall: 0.9545454545454546, precision: 0.9, f_beta: 0.9264705882352942\n",
      "training: time: 2019-09-27T11:37:40.246899, step: 137, loss: 0.6489400863647461, acc: 0.8515625, recall: 0.9166666666666666, precision: 0.7971014492753623, f_beta: 0.8527131782945736\n",
      "training: time: 2019-09-27T11:37:43.701604, step: 138, loss: 0.6273618340492249, acc: 0.875, recall: 0.9420289855072463, precision: 0.8441558441558441, f_beta: 0.8904109589041096\n",
      "training: time: 2019-09-27T11:37:47.445282, step: 139, loss: 0.8262666463851929, acc: 0.8046875, recall: 0.8769230769230769, precision: 0.7702702702702703, f_beta: 0.8201438848920863\n",
      "training: time: 2019-09-27T11:37:50.962349, step: 140, loss: 0.5880813598632812, acc: 0.875, recall: 0.8307692307692308, precision: 0.9152542372881356, f_beta: 0.870967741935484\n",
      "training: time: 2019-09-27T11:37:54.343256, step: 141, loss: 0.4670702815055847, acc: 0.9140625, recall: 0.9166666666666666, precision: 0.9016393442622951, f_beta: 0.9090909090909091\n",
      "training: time: 2019-09-27T11:37:57.705323, step: 142, loss: 0.5855094194412231, acc: 0.8828125, recall: 0.8235294117647058, precision: 0.9491525423728814, f_beta: 0.8818897637795277\n",
      "training: time: 2019-09-27T11:38:01.056272, step: 143, loss: 0.5956483483314514, acc: 0.84375, recall: 0.7321428571428571, precision: 0.8913043478260869, f_beta: 0.8039215686274508\n",
      "training: time: 2019-09-27T11:38:04.374578, step: 144, loss: 0.6115099191665649, acc: 0.8515625, recall: 0.7777777777777778, precision: 0.9074074074074074, f_beta: 0.8376068376068377\n",
      "training: time: 2019-09-27T11:38:07.837999, step: 145, loss: 0.4920155704021454, acc: 0.90625, recall: 0.8695652173913043, precision: 0.9523809523809523, f_beta: 0.909090909090909\n",
      "training: time: 2019-09-27T11:38:11.276610, step: 146, loss: 0.8376766443252563, acc: 0.8359375, recall: 0.8333333333333334, precision: 0.819672131147541, f_beta: 0.8264462809917356\n",
      "training: time: 2019-09-27T11:38:14.618930, step: 147, loss: 0.4795321822166443, acc: 0.90625, recall: 0.9295774647887324, precision: 0.9041095890410958, f_beta: 0.9166666666666666\n",
      "training: time: 2019-09-27T11:38:17.984694, step: 148, loss: 0.6411114931106567, acc: 0.859375, recall: 0.8918918918918919, precision: 0.868421052631579, f_beta: 0.88\n",
      "training: time: 2019-09-27T11:38:21.329631, step: 149, loss: 0.8401337265968323, acc: 0.796875, recall: 0.9090909090909091, precision: 0.704225352112676, f_beta: 0.7936507936507936\n",
      "training: time: 2019-09-27T11:38:24.672290, step: 150, loss: 0.7352001070976257, acc: 0.8203125, recall: 0.8771929824561403, precision: 0.7575757575757576, f_beta: 0.8130081300813008\n",
      "training: time: 2019-09-27T11:38:28.002254, step: 151, loss: 0.6137259006500244, acc: 0.8671875, recall: 0.8947368421052632, precision: 0.8225806451612904, f_beta: 0.8571428571428571\n",
      "training: time: 2019-09-27T11:38:31.556833, step: 152, loss: 0.5106073617935181, acc: 0.8828125, recall: 0.90625, precision: 0.8656716417910447, f_beta: 0.8854961832061069\n",
      "training: time: 2019-09-27T11:38:35.131532, step: 153, loss: 0.679439902305603, acc: 0.8515625, recall: 0.8412698412698413, precision: 0.8548387096774194, f_beta: 0.848\n",
      "training: time: 2019-09-27T11:38:38.694516, step: 154, loss: 0.636673629283905, acc: 0.8671875, recall: 0.8260869565217391, precision: 0.9193548387096774, f_beta: 0.8702290076335878\n",
      "training: time: 2019-09-27T11:38:42.093764, step: 155, loss: 0.7254060506820679, acc: 0.8515625, recall: 0.803030303030303, precision: 0.8983050847457628, f_beta: 0.8480000000000001\n",
      "training: time: 2019-09-27T11:38:45.460522, step: 156, loss: 0.5669150948524475, acc: 0.8984375, recall: 0.8769230769230769, precision: 0.9193548387096774, f_beta: 0.8976377952755904\n",
      "begin to train model...\n",
      "the 1-th / 5 epoch\n",
      "training: time: 2019-09-27T11:38:49.257059, step: 157, loss: 0.7467868328094482, acc: 0.828125, recall: 0.8, precision: 0.8524590163934426, f_beta: 0.8253968253968254\n",
      "training: time: 2019-09-27T11:38:52.576155, step: 158, loss: 0.5608553886413574, acc: 0.8828125, recall: 0.8823529411764706, precision: 0.8955223880597015, f_beta: 0.888888888888889\n",
      "training: time: 2019-09-27T11:38:55.914348, step: 159, loss: 0.5309014320373535, acc: 0.875, recall: 0.8571428571428571, precision: 0.9090909090909091, f_beta: 0.8823529411764706\n",
      "training: time: 2019-09-27T11:38:59.365316, step: 160, loss: 0.6959608793258667, acc: 0.8359375, recall: 0.85, precision: 0.8095238095238095, f_beta: 0.8292682926829269\n",
      "training: time: 2019-09-27T11:39:02.882075, step: 161, loss: 0.5290485620498657, acc: 0.8984375, recall: 0.9130434782608695, precision: 0.9, f_beta: 0.9064748201438848\n",
      "training: time: 2019-09-27T11:39:06.257802, step: 162, loss: 0.6292336583137512, acc: 0.8828125, recall: 0.890625, precision: 0.8769230769230769, f_beta: 0.883720930232558\n",
      "training: time: 2019-09-27T11:39:09.893821, step: 163, loss: 0.48829329013824463, acc: 0.890625, recall: 0.9285714285714286, precision: 0.8783783783783784, f_beta: 0.9027777777777779\n",
      "training: time: 2019-09-27T11:39:13.393913, step: 164, loss: 0.6696123480796814, acc: 0.8515625, recall: 0.9253731343283582, precision: 0.8157894736842105, f_beta: 0.8671328671328671\n",
      "training: time: 2019-09-27T11:39:17.720199, step: 165, loss: 0.6499148011207581, acc: 0.84375, recall: 0.9242424242424242, precision: 0.8026315789473685, f_beta: 0.8591549295774648\n",
      "training: time: 2019-09-27T11:39:22.010973, step: 166, loss: 0.701156735420227, acc: 0.828125, recall: 0.9090909090909091, precision: 0.746268656716418, f_beta: 0.819672131147541\n",
      "training: time: 2019-09-27T11:39:25.851662, step: 167, loss: 0.5231001377105713, acc: 0.8984375, recall: 0.9056603773584906, precision: 0.8571428571428571, f_beta: 0.8807339449541285\n",
      "training: time: 2019-09-27T11:39:29.854326, step: 168, loss: 0.573727011680603, acc: 0.8984375, recall: 0.9047619047619048, precision: 0.890625, f_beta: 0.8976377952755906\n",
      "training: time: 2019-09-27T11:39:33.943879, step: 169, loss: 0.582258939743042, acc: 0.8671875, recall: 0.7794117647058824, precision: 0.9636363636363636, f_beta: 0.8617886178861789\n",
      "training: time: 2019-09-27T11:39:37.479462, step: 170, loss: 0.6020827293395996, acc: 0.875, recall: 0.8035714285714286, precision: 0.9, f_beta: 0.8490566037735849\n",
      "training: time: 2019-09-27T11:39:40.928167, step: 171, loss: 0.8112630248069763, acc: 0.796875, recall: 0.5862068965517241, precision: 0.9444444444444444, f_beta: 0.7234042553191489\n",
      "training: time: 2019-09-27T11:39:44.336461, step: 172, loss: 0.7999175786972046, acc: 0.84375, recall: 0.7571428571428571, precision: 0.9464285714285714, f_beta: 0.8412698412698413\n",
      "training: time: 2019-09-27T11:39:47.696344, step: 173, loss: 0.5468963384628296, acc: 0.8984375, recall: 0.9047619047619048, precision: 0.890625, f_beta: 0.8976377952755906\n",
      "training: time: 2019-09-27T11:39:51.224471, step: 174, loss: 0.5693665742874146, acc: 0.890625, recall: 0.8823529411764706, precision: 0.9090909090909091, f_beta: 0.8955223880597014\n",
      "training: time: 2019-09-27T11:39:54.909549, step: 175, loss: 0.7705425024032593, acc: 0.8046875, recall: 0.8666666666666667, precision: 0.7536231884057971, f_beta: 0.8062015503875969\n",
      "training: time: 2019-09-27T11:39:58.702996, step: 176, loss: 0.5473282933235168, acc: 0.90625, recall: 0.9701492537313433, precision: 0.8666666666666667, f_beta: 0.915492957746479\n",
      "training: time: 2019-09-27T11:40:02.187938, step: 177, loss: 0.7032818794250488, acc: 0.8828125, recall: 0.9375, precision: 0.8450704225352113, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T11:40:05.601069, step: 178, loss: 0.6793869137763977, acc: 0.8828125, recall: 0.9253731343283582, precision: 0.8611111111111112, f_beta: 0.8920863309352519\n",
      "training: time: 2019-09-27T11:40:08.979501, step: 179, loss: 0.930370569229126, acc: 0.7734375, recall: 0.8, precision: 0.7887323943661971, f_beta: 0.7943262411347518\n",
      "training: time: 2019-09-27T11:40:12.382729, step: 180, loss: 0.6059091091156006, acc: 0.859375, recall: 0.8793103448275862, precision: 0.8225806451612904, f_beta: 0.8500000000000001\n",
      "training: time: 2019-09-27T11:40:15.713475, step: 181, loss: 0.6391576528549194, acc: 0.859375, recall: 0.8813559322033898, precision: 0.8253968253968254, f_beta: 0.8524590163934426\n",
      "training: time: 2019-09-27T11:40:19.230468, step: 182, loss: 0.5081555247306824, acc: 0.90625, recall: 0.9032258064516129, precision: 0.9032258064516129, f_beta: 0.9032258064516129\n",
      "training: time: 2019-09-27T11:40:22.624189, step: 183, loss: 0.5214433073997498, acc: 0.90625, recall: 0.8888888888888888, precision: 0.9180327868852459, f_beta: 0.9032258064516128\n",
      "training: time: 2019-09-27T11:40:26.038977, step: 184, loss: 0.5656858682632446, acc: 0.8671875, recall: 0.9047619047619048, precision: 0.8382352941176471, f_beta: 0.8702290076335878\n",
      "training: time: 2019-09-27T11:40:29.584189, step: 185, loss: 0.6602449417114258, acc: 0.859375, recall: 0.7605633802816901, precision: 0.9818181818181818, f_beta: 0.8571428571428571\n",
      "training: time: 2019-09-27T11:40:33.081564, step: 186, loss: 0.58604896068573, acc: 0.8671875, recall: 0.8125, precision: 0.9122807017543859, f_beta: 0.859504132231405\n",
      "training: time: 2019-09-27T11:40:36.450238, step: 187, loss: 0.539065420627594, acc: 0.890625, recall: 0.8461538461538461, precision: 0.9322033898305084, f_beta: 0.8870967741935484\n",
      "training: time: 2019-09-27T11:40:39.849637, step: 188, loss: 0.44045549631118774, acc: 0.9296875, recall: 0.9180327868852459, precision: 0.9333333333333333, f_beta: 0.9256198347107439\n",
      "training: time: 2019-09-27T11:40:43.251434, step: 189, loss: 0.6010611653327942, acc: 0.8671875, recall: 0.8769230769230769, precision: 0.8636363636363636, f_beta: 0.8702290076335878\n",
      "training: time: 2019-09-27T11:40:46.656714, step: 190, loss: 0.6813313961029053, acc: 0.8671875, recall: 0.9, precision: 0.8307692307692308, f_beta: 0.8640000000000001\n",
      "training: time: 2019-09-27T11:40:50.083907, step: 191, loss: 0.6890996694564819, acc: 0.8359375, recall: 0.9, precision: 0.8181818181818182, f_beta: 0.8571428571428572\n",
      "training: time: 2019-09-27T11:40:53.643288, step: 192, loss: 0.5490838885307312, acc: 0.9140625, recall: 0.953125, precision: 0.8840579710144928, f_beta: 0.9172932330827068\n",
      "training: time: 2019-09-27T11:40:57.157742, step: 193, loss: 0.6441783905029297, acc: 0.859375, recall: 0.8701298701298701, precision: 0.8933333333333333, f_beta: 0.881578947368421\n",
      "training: time: 2019-09-27T11:41:00.654439, step: 194, loss: 0.731296181678772, acc: 0.8515625, recall: 0.9545454545454546, precision: 0.7974683544303798, f_beta: 0.8689655172413794\n",
      "training: time: 2019-09-27T11:41:04.210555, step: 195, loss: 0.476152241230011, acc: 0.921875, recall: 0.9315068493150684, precision: 0.9315068493150684, f_beta: 0.9315068493150684\n",
      "training: time: 2019-09-27T11:41:07.860147, step: 196, loss: 0.6524755358695984, acc: 0.875, recall: 0.9076923076923077, precision: 0.855072463768116, f_beta: 0.8805970149253731\n",
      "training: time: 2019-09-27T11:41:11.299937, step: 197, loss: 0.5780313611030579, acc: 0.8828125, recall: 0.9344262295081968, precision: 0.8382352941176471, f_beta: 0.8837209302325582\n",
      "training: time: 2019-09-27T11:41:14.795127, step: 198, loss: 0.49472856521606445, acc: 0.890625, recall: 0.927536231884058, precision: 0.8767123287671232, f_beta: 0.9014084507042253\n",
      "training: time: 2019-09-27T11:41:18.374791, step: 199, loss: 0.5660545825958252, acc: 0.90625, recall: 0.9180327868852459, precision: 0.8888888888888888, f_beta: 0.9032258064516128\n",
      "training: time: 2019-09-27T11:41:21.855157, step: 200, loss: 0.6114493608474731, acc: 0.8984375, recall: 0.8571428571428571, precision: 0.9056603773584906, f_beta: 0.8807339449541285\n",
      "begin to evaluation: \n",
      "\n",
      "evaluation: time: 2019-09-27T11:43:44.411362, step: 200, loss: 0.6231826436825287, acc: 0.8705929487179487, recall: 0.8443785774358494, precision: 0.8941456404468396, f_beta: 0.8675094099107914\n",
      "save model checkpoint to C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\adversarialLSTM\\model\\adversarialLSTM-200 \n",
      "\n",
      "training: time: 2019-09-27T11:43:49.333633, step: 201, loss: 0.694959819316864, acc: 0.8125, recall: 0.7941176470588235, precision: 0.84375, f_beta: 0.8181818181818182\n",
      "training: time: 2019-09-27T11:43:52.872865, step: 202, loss: 0.6221223473548889, acc: 0.875, recall: 0.8615384615384616, precision: 0.8888888888888888, f_beta: 0.8750000000000001\n",
      "training: time: 2019-09-27T11:43:56.481483, step: 203, loss: 0.7166638374328613, acc: 0.859375, recall: 0.875, precision: 0.8484848484848485, f_beta: 0.8615384615384615\n",
      "training: time: 2019-09-27T11:43:59.895364, step: 204, loss: 0.6356711983680725, acc: 0.859375, recall: 0.8032786885245902, precision: 0.8909090909090909, f_beta: 0.8448275862068965\n",
      "training: time: 2019-09-27T11:44:03.523109, step: 205, loss: 0.5139771699905396, acc: 0.890625, recall: 0.9242424242424242, precision: 0.8714285714285714, f_beta: 0.8970588235294117\n",
      "training: time: 2019-09-27T11:44:07.769710, step: 206, loss: 0.4491296410560608, acc: 0.9296875, recall: 0.9436619718309859, precision: 0.9305555555555556, f_beta: 0.9370629370629372\n",
      "training: time: 2019-09-27T11:44:11.731900, step: 207, loss: 0.5435572266578674, acc: 0.8828125, recall: 0.9142857142857143, precision: 0.8767123287671232, f_beta: 0.8951048951048951\n",
      "training: time: 2019-09-27T11:44:15.368567, step: 208, loss: 0.5397951602935791, acc: 0.90625, recall: 0.9206349206349206, precision: 0.8923076923076924, f_beta: 0.90625\n",
      "training: time: 2019-09-27T11:44:18.742537, step: 209, loss: 0.6598027944564819, acc: 0.875, recall: 0.8688524590163934, precision: 0.8688524590163934, f_beta: 0.8688524590163934\n",
      "training: time: 2019-09-27T11:44:22.259484, step: 210, loss: 0.7152783870697021, acc: 0.828125, recall: 0.8360655737704918, precision: 0.8095238095238095, f_beta: 0.8225806451612904\n",
      "training: time: 2019-09-27T11:44:25.865167, step: 211, loss: 0.46537312865257263, acc: 0.8984375, recall: 0.9666666666666667, precision: 0.8405797101449275, f_beta: 0.8992248062015503\n",
      "training: time: 2019-09-27T11:44:29.260845, step: 212, loss: 0.507611870765686, acc: 0.9296875, recall: 0.9315068493150684, precision: 0.9444444444444444, f_beta: 0.9379310344827586\n",
      "training: time: 2019-09-27T11:44:32.737150, step: 213, loss: 0.4814850687980652, acc: 0.921875, recall: 0.9565217391304348, precision: 0.9041095890410958, f_beta: 0.9295774647887325\n",
      "training: time: 2019-09-27T11:44:36.144130, step: 214, loss: 0.5184333324432373, acc: 0.8984375, recall: 0.921875, precision: 0.8805970149253731, f_beta: 0.9007633587786259\n",
      "training: time: 2019-09-27T11:44:39.551319, step: 215, loss: 0.6693444848060608, acc: 0.875, recall: 0.8939393939393939, precision: 0.8676470588235294, f_beta: 0.8805970149253731\n",
      "training: time: 2019-09-27T11:44:42.967427, step: 216, loss: 0.6450555920600891, acc: 0.8671875, recall: 0.8615384615384616, precision: 0.875, f_beta: 0.8682170542635659\n",
      "training: time: 2019-09-27T11:44:46.387836, step: 217, loss: 0.4213939905166626, acc: 0.921875, recall: 0.896551724137931, precision: 0.9285714285714286, f_beta: 0.912280701754386\n",
      "training: time: 2019-09-27T11:44:49.767869, step: 218, loss: 0.616072952747345, acc: 0.8671875, recall: 0.8709677419354839, precision: 0.8571428571428571, f_beta: 0.864\n",
      "training: time: 2019-09-27T11:44:53.286533, step: 219, loss: 0.4914604723453522, acc: 0.8984375, recall: 0.8666666666666667, precision: 0.9122807017543859, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T11:44:56.799664, step: 220, loss: 0.5456864237785339, acc: 0.8828125, recall: 0.8412698412698413, precision: 0.9137931034482759, f_beta: 0.8760330578512397\n",
      "training: time: 2019-09-27T11:45:00.216770, step: 221, loss: 0.6941201090812683, acc: 0.828125, recall: 0.8793103448275862, precision: 0.7727272727272727, f_beta: 0.8225806451612904\n",
      "training: time: 2019-09-27T11:45:03.592025, step: 222, loss: 0.46689683198928833, acc: 0.90625, recall: 0.9365079365079365, precision: 0.8805970149253731, f_beta: 0.9076923076923077\n",
      "training: time: 2019-09-27T11:45:07.049909, step: 223, loss: 0.6568142175674438, acc: 0.875, recall: 0.8771929824561403, precision: 0.847457627118644, f_beta: 0.8620689655172413\n",
      "training: time: 2019-09-27T11:45:10.450461, step: 224, loss: 0.43126410245895386, acc: 0.9296875, recall: 0.9384615384615385, precision: 0.9242424242424242, f_beta: 0.9312977099236641\n",
      "training: time: 2019-09-27T11:45:13.898535, step: 225, loss: 0.4712863564491272, acc: 0.8984375, recall: 0.8805970149253731, precision: 0.921875, f_beta: 0.9007633587786259\n",
      "training: time: 2019-09-27T11:45:17.425982, step: 226, loss: 0.8055803775787354, acc: 0.8515625, recall: 0.8059701492537313, precision: 0.9, f_beta: 0.8503937007874016\n",
      "training: time: 2019-09-27T11:45:20.889663, step: 227, loss: 0.7221170663833618, acc: 0.8515625, recall: 0.9253731343283582, precision: 0.8157894736842105, f_beta: 0.8671328671328671\n",
      "training: time: 2019-09-27T11:45:24.393635, step: 228, loss: 0.5959645509719849, acc: 0.8828125, recall: 0.927536231884058, precision: 0.8648648648648649, f_beta: 0.8951048951048951\n",
      "training: time: 2019-09-27T11:45:28.519554, step: 229, loss: 0.5119456052780151, acc: 0.890625, recall: 0.9322033898305084, precision: 0.8461538461538461, f_beta: 0.8870967741935484\n",
      "training: time: 2019-09-27T11:45:32.522285, step: 230, loss: 0.6010460257530212, acc: 0.875, recall: 0.8591549295774648, precision: 0.9104477611940298, f_beta: 0.8840579710144928\n",
      "training: time: 2019-09-27T11:45:36.365668, step: 231, loss: 0.7194795608520508, acc: 0.8203125, recall: 0.8382352941176471, precision: 0.8260869565217391, f_beta: 0.832116788321168\n",
      "training: time: 2019-09-27T11:45:40.257321, step: 232, loss: 0.4934315085411072, acc: 0.890625, recall: 0.864406779661017, precision: 0.8947368421052632, f_beta: 0.8793103448275862\n",
      "training: time: 2019-09-27T11:45:43.870147, step: 233, loss: 0.6154125928878784, acc: 0.84375, recall: 0.868421052631579, precision: 0.868421052631579, f_beta: 0.868421052631579\n",
      "training: time: 2019-09-27T11:45:47.616869, step: 234, loss: 0.5667282342910767, acc: 0.859375, recall: 0.8333333333333334, precision: 0.9090909090909091, f_beta: 0.8695652173913043\n",
      "training: time: 2019-09-27T11:45:51.561948, step: 235, loss: 0.6703716516494751, acc: 0.875, recall: 0.8852459016393442, precision: 0.8571428571428571, f_beta: 0.8709677419354839\n",
      "training: time: 2019-09-27T11:45:55.579612, step: 236, loss: 0.6215687394142151, acc: 0.84375, recall: 0.8461538461538461, precision: 0.7857142857142857, f_beta: 0.8148148148148148\n",
      "training: time: 2019-09-27T11:45:59.192302, step: 237, loss: 0.6413863301277161, acc: 0.890625, recall: 0.9032258064516129, precision: 0.875, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T11:46:02.974606, step: 238, loss: 0.7055104970932007, acc: 0.8515625, recall: 0.7692307692307693, precision: 0.9259259259259259, f_beta: 0.8403361344537816\n",
      "training: time: 2019-09-27T11:46:06.738375, step: 239, loss: 0.4640648365020752, acc: 0.8984375, recall: 0.8787878787878788, precision: 0.9206349206349206, f_beta: 0.8992248062015504\n",
      "training: time: 2019-09-27T11:46:10.842163, step: 240, loss: 0.6779836416244507, acc: 0.890625, recall: 0.9076923076923077, precision: 0.8805970149253731, f_beta: 0.8939393939393939\n",
      "training: time: 2019-09-27T11:46:14.406872, step: 241, loss: 0.5284599661827087, acc: 0.90625, recall: 0.9642857142857143, precision: 0.84375, f_beta: 0.8999999999999999\n",
      "training: time: 2019-09-27T11:46:17.825463, step: 242, loss: 0.7354564666748047, acc: 0.8359375, recall: 0.8461538461538461, precision: 0.8333333333333334, f_beta: 0.8396946564885497\n",
      "training: time: 2019-09-27T11:46:21.866517, step: 243, loss: 0.5974850654602051, acc: 0.8515625, recall: 0.875, precision: 0.8032786885245902, f_beta: 0.8376068376068376\n",
      "training: time: 2019-09-27T11:46:26.616380, step: 244, loss: 0.5988039374351501, acc: 0.8984375, recall: 0.8666666666666667, precision: 0.9122807017543859, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T11:46:30.824935, step: 245, loss: 0.6573379039764404, acc: 0.8671875, recall: 0.8507462686567164, precision: 0.890625, f_beta: 0.8702290076335878\n",
      "training: time: 2019-09-27T11:46:34.890721, step: 246, loss: 0.5856412649154663, acc: 0.8671875, recall: 0.8676470588235294, precision: 0.8805970149253731, f_beta: 0.874074074074074\n",
      "training: time: 2019-09-27T11:46:39.366295, step: 247, loss: 0.7899922132492065, acc: 0.8203125, recall: 0.8032786885245902, precision: 0.8166666666666667, f_beta: 0.8099173553719008\n",
      "training: time: 2019-09-27T11:46:43.405792, step: 248, loss: 0.6918193101882935, acc: 0.8515625, recall: 0.8253968253968254, precision: 0.8666666666666667, f_beta: 0.8455284552845528\n",
      "training: time: 2019-09-27T11:46:47.358731, step: 249, loss: 0.5685379505157471, acc: 0.8828125, recall: 0.803030303030303, precision: 0.9636363636363636, f_beta: 0.8760330578512396\n",
      "training: time: 2019-09-27T11:46:51.235797, step: 250, loss: 0.5690697431564331, acc: 0.8828125, recall: 0.8266666666666667, precision: 0.96875, f_beta: 0.8920863309352518\n",
      "training: time: 2019-09-27T11:46:55.479873, step: 251, loss: 0.5655385255813599, acc: 0.8984375, recall: 0.8805970149253731, precision: 0.921875, f_beta: 0.9007633587786259\n",
      "training: time: 2019-09-27T11:46:59.963405, step: 252, loss: 0.5346741676330566, acc: 0.8984375, recall: 0.9104477611940298, precision: 0.8970588235294118, f_beta: 0.9037037037037037\n",
      "training: time: 2019-09-27T11:47:04.186557, step: 253, loss: 0.6126759052276611, acc: 0.875, recall: 0.9310344827586207, precision: 0.8181818181818182, f_beta: 0.8709677419354839\n",
      "training: time: 2019-09-27T11:47:08.062486, step: 254, loss: 0.504169225692749, acc: 0.8984375, recall: 0.9722222222222222, precision: 0.8641975308641975, f_beta: 0.9150326797385621\n",
      "training: time: 2019-09-27T11:47:12.940102, step: 255, loss: 0.6477072238922119, acc: 0.8671875, recall: 0.8928571428571429, precision: 0.819672131147541, f_beta: 0.8547008547008548\n",
      "training: time: 2019-09-27T11:47:16.689707, step: 256, loss: 0.5869302749633789, acc: 0.890625, recall: 0.9444444444444444, precision: 0.8225806451612904, f_beta: 0.8793103448275862\n",
      "training: time: 2019-09-27T11:47:20.061879, step: 257, loss: 0.44583359360694885, acc: 0.890625, recall: 0.9344262295081968, precision: 0.8507462686567164, f_beta: 0.8906250000000001\n",
      "training: time: 2019-09-27T11:47:23.325040, step: 258, loss: 0.6731444001197815, acc: 0.8828125, recall: 0.8970588235294118, precision: 0.8840579710144928, f_beta: 0.8905109489051095\n",
      "training: time: 2019-09-27T11:47:26.679778, step: 259, loss: 0.5324467420578003, acc: 0.890625, recall: 0.9154929577464789, precision: 0.8904109589041096, f_beta: 0.9027777777777778\n",
      "training: time: 2019-09-27T11:47:30.121307, step: 260, loss: 0.6795046329498291, acc: 0.859375, recall: 0.8309859154929577, precision: 0.9076923076923077, f_beta: 0.8676470588235293\n",
      "training: time: 2019-09-27T11:47:33.650594, step: 261, loss: 0.559319019317627, acc: 0.875, recall: 0.8666666666666667, precision: 0.8666666666666667, f_beta: 0.8666666666666667\n",
      "training: time: 2019-09-27T11:47:37.309397, step: 262, loss: 0.5006332397460938, acc: 0.8984375, recall: 0.8656716417910447, precision: 0.9354838709677419, f_beta: 0.8992248062015503\n",
      "training: time: 2019-09-27T11:47:40.708886, step: 263, loss: 0.6119402647018433, acc: 0.84375, recall: 0.7580645161290323, precision: 0.9038461538461539, f_beta: 0.8245614035087719\n",
      "training: time: 2019-09-27T11:47:44.133460, step: 264, loss: 0.5855096578598022, acc: 0.8828125, recall: 0.8923076923076924, precision: 0.8787878787878788, f_beta: 0.8854961832061069\n",
      "training: time: 2019-09-27T11:47:47.680269, step: 265, loss: 0.5911316871643066, acc: 0.8515625, recall: 0.8928571428571429, precision: 0.7936507936507936, f_beta: 0.8403361344537815\n",
      "training: time: 2019-09-27T11:47:51.040376, step: 266, loss: 0.37266138195991516, acc: 0.921875, recall: 0.9107142857142857, precision: 0.9107142857142857, f_beta: 0.9107142857142857\n",
      "training: time: 2019-09-27T11:47:54.489175, step: 267, loss: 0.5029546022415161, acc: 0.9375, recall: 0.9838709677419355, precision: 0.8970588235294118, f_beta: 0.9384615384615386\n",
      "training: time: 2019-09-27T11:47:57.765201, step: 268, loss: 0.7142167091369629, acc: 0.8515625, recall: 0.847457627118644, precision: 0.8333333333333334, f_beta: 0.8403361344537815\n",
      "training: time: 2019-09-27T11:48:01.255346, step: 269, loss: 0.485106885433197, acc: 0.8984375, recall: 0.8852459016393442, precision: 0.9, f_beta: 0.8925619834710743\n",
      "training: time: 2019-09-27T11:48:04.790526, step: 270, loss: 0.6110967397689819, acc: 0.8515625, recall: 0.875, precision: 0.8032786885245902, f_beta: 0.8376068376068376\n",
      "training: time: 2019-09-27T11:48:08.092058, step: 271, loss: 0.6782588958740234, acc: 0.875, recall: 0.8524590163934426, precision: 0.8813559322033898, f_beta: 0.8666666666666666\n",
      "training: time: 2019-09-27T11:48:11.460779, step: 272, loss: 0.600135326385498, acc: 0.8984375, recall: 0.8615384615384616, precision: 0.9333333333333333, f_beta: 0.8960000000000001\n",
      "training: time: 2019-09-27T11:48:14.775306, step: 273, loss: 0.5702938437461853, acc: 0.890625, recall: 0.8382352941176471, precision: 0.95, f_beta: 0.890625\n",
      "training: time: 2019-09-27T11:48:18.256004, step: 274, loss: 0.4634932279586792, acc: 0.9140625, recall: 0.9305555555555556, precision: 0.9178082191780822, f_beta: 0.9241379310344828\n",
      "training: time: 2019-09-27T11:48:22.056414, step: 275, loss: 0.5940753221511841, acc: 0.875, recall: 0.9206349206349206, precision: 0.8405797101449275, f_beta: 0.8787878787878787\n",
      "training: time: 2019-09-27T11:48:25.453785, step: 276, loss: 0.6203086376190186, acc: 0.8359375, recall: 0.8064516129032258, precision: 0.847457627118644, f_beta: 0.8264462809917354\n",
      "training: time: 2019-09-27T11:48:28.907256, step: 277, loss: 0.387326180934906, acc: 0.9453125, recall: 0.9333333333333333, precision: 0.9491525423728814, f_beta: 0.9411764705882353\n",
      "training: time: 2019-09-27T11:48:32.435214, step: 278, loss: 0.6838650703430176, acc: 0.859375, recall: 0.8, precision: 0.9122807017543859, f_beta: 0.8524590163934427\n",
      "training: time: 2019-09-27T11:48:35.729133, step: 279, loss: 0.4210045635700226, acc: 0.9296875, recall: 0.9166666666666666, precision: 0.8979591836734694, f_beta: 0.9072164948453607\n",
      "training: time: 2019-09-27T11:48:39.036810, step: 280, loss: 0.5893889665603638, acc: 0.859375, recall: 0.875, precision: 0.8484848484848485, f_beta: 0.8615384615384615\n",
      "training: time: 2019-09-27T11:48:42.328311, step: 281, loss: 0.5225355625152588, acc: 0.8671875, recall: 0.8695652173913043, precision: 0.8823529411764706, f_beta: 0.8759124087591241\n",
      "training: time: 2019-09-27T11:48:46.071392, step: 282, loss: 0.5949762463569641, acc: 0.90625, recall: 0.9027777777777778, precision: 0.9285714285714286, f_beta: 0.9154929577464788\n",
      "training: time: 2019-09-27T11:48:49.435457, step: 283, loss: 0.402593731880188, acc: 0.9375, recall: 0.9444444444444444, precision: 0.9107142857142857, f_beta: 0.9272727272727271\n",
      "training: time: 2019-09-27T11:48:53.201776, step: 284, loss: 0.6552760601043701, acc: 0.8515625, recall: 0.8225806451612904, precision: 0.864406779661017, f_beta: 0.8429752066115702\n",
      "training: time: 2019-09-27T11:48:57.102292, step: 285, loss: 0.5816329717636108, acc: 0.8671875, recall: 0.8571428571428571, precision: 0.8709677419354839, f_beta: 0.864\n",
      "training: time: 2019-09-27T11:49:00.557401, step: 286, loss: 0.5227271914482117, acc: 0.8671875, recall: 0.9122807017543859, precision: 0.8125, f_beta: 0.859504132231405\n",
      "training: time: 2019-09-27T11:49:04.137105, step: 287, loss: 0.5548694133758545, acc: 0.890625, recall: 0.921875, precision: 0.8676470588235294, f_beta: 0.893939393939394\n",
      "training: time: 2019-09-27T11:49:07.791099, step: 288, loss: 0.6176035404205322, acc: 0.875, recall: 0.8615384615384616, precision: 0.8888888888888888, f_beta: 0.8750000000000001\n",
      "training: time: 2019-09-27T11:49:11.433554, step: 289, loss: 0.4697865843772888, acc: 0.8828125, recall: 0.847457627118644, precision: 0.8928571428571429, f_beta: 0.8695652173913044\n",
      "training: time: 2019-09-27T11:49:14.818104, step: 290, loss: 0.5332038402557373, acc: 0.8671875, recall: 0.8032786885245902, precision: 0.9074074074074074, f_beta: 0.8521739130434782\n",
      "training: time: 2019-09-27T11:49:18.231077, step: 291, loss: 0.4476155638694763, acc: 0.921875, recall: 0.9076923076923077, precision: 0.9365079365079365, f_beta: 0.9218749999999999\n",
      "training: time: 2019-09-27T11:49:21.867502, step: 292, loss: 0.4271928668022156, acc: 0.9296875, recall: 0.9682539682539683, precision: 0.8970588235294118, f_beta: 0.9312977099236641\n",
      "training: time: 2019-09-27T11:49:25.354292, step: 293, loss: 0.4594149589538574, acc: 0.9140625, recall: 0.873015873015873, precision: 0.9482758620689655, f_beta: 0.9090909090909091\n",
      "training: time: 2019-09-27T11:49:28.820345, step: 294, loss: 0.4414633512496948, acc: 0.9140625, recall: 0.8823529411764706, precision: 0.9523809523809523, f_beta: 0.916030534351145\n",
      "training: time: 2019-09-27T11:49:32.140192, step: 295, loss: 0.6293896436691284, acc: 0.890625, recall: 0.8939393939393939, precision: 0.8939393939393939, f_beta: 0.8939393939393939\n",
      "training: time: 2019-09-27T11:49:35.589107, step: 296, loss: 0.3952189087867737, acc: 0.921875, recall: 0.9310344827586207, precision: 0.9, f_beta: 0.9152542372881356\n",
      "training: time: 2019-09-27T11:49:38.947993, step: 297, loss: 0.5294325947761536, acc: 0.8984375, recall: 0.8769230769230769, precision: 0.9193548387096774, f_beta: 0.8976377952755904\n",
      "training: time: 2019-09-27T11:49:42.357147, step: 298, loss: 0.6167343854904175, acc: 0.8828125, recall: 0.9242424242424242, precision: 0.8591549295774648, f_beta: 0.8905109489051095\n",
      "training: time: 2019-09-27T11:49:45.891157, step: 299, loss: 0.512126088142395, acc: 0.8984375, recall: 0.9142857142857143, precision: 0.9014084507042254, f_beta: 0.9078014184397163\n",
      "training: time: 2019-09-27T11:49:49.242414, step: 300, loss: 0.3811917304992676, acc: 0.8984375, recall: 0.9264705882352942, precision: 0.8873239436619719, f_beta: 0.906474820143885\n",
      "begin to evaluation: \n",
      "\n",
      "evaluation: time: 2019-09-27T11:51:58.158522, step: 300, loss: 0.604674715262193, acc: 0.8754006410256411, recall: 0.8876146300242749, precision: 0.869602112689899, f_beta: 0.8777642110659104\n",
      "save model checkpoint to C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\adversarialLSTM\\model\\adversarialLSTM-300 \n",
      "\n",
      "training: time: 2019-09-27T11:52:03.527273, step: 301, loss: 0.6532410383224487, acc: 0.859375, recall: 0.8441558441558441, precision: 0.9154929577464789, f_beta: 0.8783783783783784\n",
      "training: time: 2019-09-27T11:52:07.132737, step: 302, loss: 0.5904381275177002, acc: 0.8671875, recall: 0.8970588235294118, precision: 0.8591549295774648, f_beta: 0.8776978417266187\n",
      "training: time: 2019-09-27T11:52:10.616518, step: 303, loss: 0.5415644645690918, acc: 0.875, recall: 0.8888888888888888, precision: 0.8275862068965517, f_beta: 0.8571428571428572\n",
      "training: time: 2019-09-27T11:52:13.930791, step: 304, loss: 0.6363202333450317, acc: 0.890625, recall: 0.9193548387096774, precision: 0.8636363636363636, f_beta: 0.890625\n",
      "training: time: 2019-09-27T11:52:17.334957, step: 305, loss: 0.660016655921936, acc: 0.859375, recall: 0.828125, precision: 0.8833333333333333, f_beta: 0.8548387096774193\n",
      "training: time: 2019-09-27T11:52:20.643029, step: 306, loss: 0.49871236085891724, acc: 0.90625, recall: 0.9375, precision: 0.8823529411764706, f_beta: 0.9090909090909091\n",
      "training: time: 2019-09-27T11:52:23.946582, step: 307, loss: 0.6062268614768982, acc: 0.875, recall: 0.8392857142857143, precision: 0.8703703703703703, f_beta: 0.8545454545454546\n",
      "training: time: 2019-09-27T11:52:27.241420, step: 308, loss: 0.6001906991004944, acc: 0.8828125, recall: 0.9245283018867925, precision: 0.8166666666666667, f_beta: 0.8672566371681416\n",
      "training: time: 2019-09-27T11:52:30.567185, step: 309, loss: 0.6919425129890442, acc: 0.8515625, recall: 0.8387096774193549, precision: 0.8524590163934426, f_beta: 0.8455284552845529\n",
      "training: time: 2019-09-27T11:52:33.888703, step: 310, loss: 0.6774741411209106, acc: 0.828125, recall: 0.7868852459016393, precision: 0.8421052631578947, f_beta: 0.8135593220338982\n",
      "training: time: 2019-09-27T11:52:37.275157, step: 311, loss: 0.679457426071167, acc: 0.859375, recall: 0.8064516129032258, precision: 0.8928571428571429, f_beta: 0.8474576271186439\n",
      "training: time: 2019-09-27T11:52:40.646138, step: 312, loss: 0.4700995087623596, acc: 0.8984375, recall: 0.8888888888888888, precision: 0.9032258064516129, f_beta: 0.8959999999999999\n",
      "begin to train model...\n",
      "the 2-th / 5 epoch\n",
      "training: time: 2019-09-27T11:52:44.319398, step: 313, loss: 0.6007170677185059, acc: 0.8828125, recall: 0.8305084745762712, precision: 0.9074074074074074, f_beta: 0.8672566371681415\n",
      "training: time: 2019-09-27T11:52:47.640324, step: 314, loss: 0.5036325454711914, acc: 0.9296875, recall: 0.9375, precision: 0.9230769230769231, f_beta: 0.9302325581395349\n",
      "training: time: 2019-09-27T11:52:51.043454, step: 315, loss: 0.5544830560684204, acc: 0.890625, recall: 0.9180327868852459, precision: 0.8615384615384616, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T11:52:54.358603, step: 316, loss: 0.5287966132164001, acc: 0.875, recall: 0.9, precision: 0.84375, f_beta: 0.870967741935484\n",
      "training: time: 2019-09-27T11:52:57.672681, step: 317, loss: 0.6012295484542847, acc: 0.859375, recall: 0.8852459016393442, precision: 0.8307692307692308, f_beta: 0.8571428571428572\n",
      "training: time: 2019-09-27T11:53:00.957352, step: 318, loss: 0.5315134525299072, acc: 0.921875, recall: 0.9253731343283582, precision: 0.9253731343283582, f_beta: 0.9253731343283582\n",
      "training: time: 2019-09-27T11:53:04.262675, step: 319, loss: 0.6814329624176025, acc: 0.8828125, recall: 0.859375, precision: 0.9016393442622951, f_beta: 0.88\n",
      "training: time: 2019-09-27T11:53:07.558118, step: 320, loss: 0.4691820740699768, acc: 0.9140625, recall: 0.9193548387096774, precision: 0.9047619047619048, f_beta: 0.912\n",
      "training: time: 2019-09-27T11:53:10.895849, step: 321, loss: 0.49262264370918274, acc: 0.8984375, recall: 0.8472222222222222, precision: 0.9682539682539683, f_beta: 0.9037037037037037\n",
      "training: time: 2019-09-27T11:53:14.189239, step: 322, loss: 0.5117387771606445, acc: 0.890625, recall: 0.868421052631579, precision: 0.9428571428571428, f_beta: 0.904109589041096\n",
      "training: time: 2019-09-27T11:53:17.503234, step: 323, loss: 0.580916166305542, acc: 0.875, recall: 0.8888888888888888, precision: 0.8615384615384616, f_beta: 0.8750000000000001\n",
      "training: time: 2019-09-27T11:53:20.826476, step: 324, loss: 0.6664177179336548, acc: 0.859375, recall: 0.8, precision: 0.8627450980392157, f_beta: 0.8301886792452831\n",
      "training: time: 2019-09-27T11:53:24.153634, step: 325, loss: 0.6285058259963989, acc: 0.8359375, recall: 0.8648648648648649, precision: 0.8533333333333334, f_beta: 0.8590604026845637\n",
      "training: time: 2019-09-27T11:53:27.417500, step: 326, loss: 0.5255947113037109, acc: 0.8828125, recall: 0.8983050847457628, precision: 0.8548387096774194, f_beta: 0.8760330578512397\n",
      "training: time: 2019-09-27T11:53:30.780376, step: 327, loss: 0.4343966245651245, acc: 0.921875, recall: 0.9428571428571428, precision: 0.9166666666666666, f_beta: 0.9295774647887323\n",
      "training: time: 2019-09-27T11:53:34.127122, step: 328, loss: 0.6790944933891296, acc: 0.859375, recall: 0.9148936170212766, precision: 0.7543859649122807, f_beta: 0.8269230769230769\n",
      "training: time: 2019-09-27T11:53:37.414139, step: 329, loss: 0.4985785484313965, acc: 0.890625, recall: 0.9818181818181818, precision: 0.8059701492537313, f_beta: 0.8852459016393442\n",
      "training: time: 2019-09-27T11:53:40.732233, step: 330, loss: 0.40692782402038574, acc: 0.921875, recall: 0.9344262295081968, precision: 0.9047619047619048, f_beta: 0.9193548387096775\n",
      "training: time: 2019-09-27T11:53:43.983008, step: 331, loss: 0.5813019275665283, acc: 0.859375, recall: 0.8923076923076924, precision: 0.8405797101449275, f_beta: 0.8656716417910447\n",
      "training: time: 2019-09-27T11:53:47.231044, step: 332, loss: 0.6250112056732178, acc: 0.8828125, recall: 0.8169014084507042, precision: 0.9666666666666667, f_beta: 0.8854961832061069\n",
      "training: time: 2019-09-27T11:53:50.496600, step: 333, loss: 0.6989725232124329, acc: 0.8203125, recall: 0.75, precision: 0.7959183673469388, f_beta: 0.7722772277227722\n",
      "training: time: 2019-09-27T11:53:53.801458, step: 334, loss: 0.4628363847732544, acc: 0.9140625, recall: 0.8813559322033898, precision: 0.9285714285714286, f_beta: 0.9043478260869564\n",
      "training: time: 2019-09-27T11:53:57.155201, step: 335, loss: 0.5882920026779175, acc: 0.8828125, recall: 0.8615384615384616, precision: 0.9032258064516129, f_beta: 0.8818897637795274\n",
      "training: time: 2019-09-27T11:54:00.511582, step: 336, loss: 0.5210479497909546, acc: 0.8671875, recall: 0.8448275862068966, precision: 0.8596491228070176, f_beta: 0.8521739130434783\n",
      "training: time: 2019-09-27T11:54:03.766609, step: 337, loss: 0.47281384468078613, acc: 0.8984375, recall: 0.8805970149253731, precision: 0.921875, f_beta: 0.9007633587786259\n",
      "training: time: 2019-09-27T11:54:07.117955, step: 338, loss: 0.3765318989753723, acc: 0.9375, recall: 0.9444444444444444, precision: 0.9444444444444444, f_beta: 0.9444444444444444\n",
      "training: time: 2019-09-27T11:54:10.584361, step: 339, loss: 0.49643611907958984, acc: 0.8828125, recall: 0.8769230769230769, precision: 0.890625, f_beta: 0.883720930232558\n",
      "training: time: 2019-09-27T11:54:14.016593, step: 340, loss: 0.5072858333587646, acc: 0.890625, recall: 0.8923076923076924, precision: 0.8923076923076924, f_beta: 0.8923076923076924\n",
      "training: time: 2019-09-27T11:54:17.323029, step: 341, loss: 0.3804944157600403, acc: 0.9296875, recall: 0.9298245614035088, precision: 0.9137931034482759, f_beta: 0.9217391304347825\n",
      "training: time: 2019-09-27T11:54:20.815296, step: 342, loss: 0.5262120962142944, acc: 0.90625, recall: 0.967741935483871, precision: 0.8571428571428571, f_beta: 0.909090909090909\n",
      "training: time: 2019-09-27T11:54:24.202107, step: 343, loss: 0.763136625289917, acc: 0.8515625, recall: 0.9166666666666666, precision: 0.7971014492753623, f_beta: 0.8527131782945736\n",
      "training: time: 2019-09-27T11:54:27.554898, step: 344, loss: 0.5195877552032471, acc: 0.8828125, recall: 0.9538461538461539, precision: 0.8378378378378378, f_beta: 0.8920863309352518\n",
      "training: time: 2019-09-27T11:54:30.830661, step: 345, loss: 0.5658213496208191, acc: 0.8984375, recall: 0.9375, precision: 0.8695652173913043, f_beta: 0.9022556390977444\n",
      "training: time: 2019-09-27T11:54:34.224558, step: 346, loss: 0.5871087312698364, acc: 0.890625, recall: 0.8709677419354839, precision: 0.9, f_beta: 0.8852459016393444\n",
      "training: time: 2019-09-27T11:54:37.575158, step: 347, loss: 0.5231215953826904, acc: 0.921875, recall: 0.9, precision: 0.9310344827586207, f_beta: 0.9152542372881356\n",
      "training: time: 2019-09-27T11:54:40.979310, step: 348, loss: 0.4984561800956726, acc: 0.890625, recall: 0.8448275862068966, precision: 0.9074074074074074, f_beta: 0.875\n",
      "training: time: 2019-09-27T11:54:44.381158, step: 349, loss: 0.5953625440597534, acc: 0.8984375, recall: 0.8529411764705882, precision: 0.9508196721311475, f_beta: 0.8992248062015503\n",
      "training: time: 2019-09-27T11:54:47.683910, step: 350, loss: 0.46232521533966064, acc: 0.8828125, recall: 0.84375, precision: 0.9152542372881356, f_beta: 0.8780487804878049\n",
      "training: time: 2019-09-27T11:54:51.040115, step: 351, loss: 0.5239666700363159, acc: 0.9140625, recall: 0.8676470588235294, precision: 0.9672131147540983, f_beta: 0.9147286821705426\n",
      "training: time: 2019-09-27T11:54:54.358878, step: 352, loss: 0.5063655376434326, acc: 0.875, recall: 0.8909090909090909, precision: 0.8305084745762712, f_beta: 0.8596491228070176\n",
      "training: time: 2019-09-27T11:54:57.689485, step: 353, loss: 0.6690477728843689, acc: 0.8671875, recall: 0.9272727272727272, precision: 0.796875, f_beta: 0.8571428571428571\n",
      "training: time: 2019-09-27T11:55:01.059730, step: 354, loss: 0.3984444737434387, acc: 0.921875, recall: 0.9545454545454546, precision: 0.9, f_beta: 0.9264705882352942\n",
      "training: time: 2019-09-27T11:55:04.357229, step: 355, loss: 0.6063436269760132, acc: 0.8828125, recall: 0.8833333333333333, precision: 0.8688524590163934, f_beta: 0.8760330578512396\n",
      "training: time: 2019-09-27T11:55:07.627158, step: 356, loss: 0.5238401889801025, acc: 0.8828125, recall: 0.8857142857142857, precision: 0.8985507246376812, f_beta: 0.8920863309352518\n",
      "training: time: 2019-09-27T11:55:11.173181, step: 357, loss: 0.47146084904670715, acc: 0.9140625, recall: 0.9117647058823529, precision: 0.9253731343283582, f_beta: 0.9185185185185185\n",
      "training: time: 2019-09-27T11:55:14.442733, step: 358, loss: 0.5471765995025635, acc: 0.8671875, recall: 0.8656716417910447, precision: 0.8787878787878788, f_beta: 0.8721804511278195\n",
      "training: time: 2019-09-27T11:55:17.803064, step: 359, loss: 0.48891544342041016, acc: 0.90625, recall: 0.8529411764705882, precision: 0.9666666666666667, f_beta: 0.90625\n",
      "training: time: 2019-09-27T11:55:21.118982, step: 360, loss: 0.451718807220459, acc: 0.8828125, recall: 0.9253731343283582, precision: 0.8611111111111112, f_beta: 0.8920863309352519\n",
      "training: time: 2019-09-27T11:55:24.360406, step: 361, loss: 0.44115006923675537, acc: 0.9140625, recall: 0.8688524590163934, precision: 0.9464285714285714, f_beta: 0.9059829059829059\n",
      "training: time: 2019-09-27T11:55:27.731549, step: 362, loss: 0.582234263420105, acc: 0.875, recall: 0.9104477611940298, precision: 0.8591549295774648, f_beta: 0.8840579710144928\n",
      "training: time: 2019-09-27T11:55:31.120271, step: 363, loss: 0.5903860330581665, acc: 0.8515625, recall: 0.8813559322033898, precision: 0.8125, f_beta: 0.8455284552845529\n",
      "training: time: 2019-09-27T11:55:34.850426, step: 364, loss: 0.6578028202056885, acc: 0.8828125, recall: 0.9152542372881356, precision: 0.84375, f_beta: 0.8780487804878049\n",
      "training: time: 2019-09-27T11:55:38.556098, step: 365, loss: 0.4645047187805176, acc: 0.9140625, recall: 0.896551724137931, precision: 0.9122807017543859, f_beta: 0.9043478260869565\n",
      "training: time: 2019-09-27T11:55:42.135996, step: 366, loss: 0.5794543027877808, acc: 0.875, recall: 0.8596491228070176, precision: 0.8596491228070176, f_beta: 0.8596491228070176\n",
      "training: time: 2019-09-27T11:55:45.518445, step: 367, loss: 0.44725745916366577, acc: 0.9296875, recall: 0.8983050847457628, precision: 0.9464285714285714, f_beta: 0.9217391304347826\n",
      "training: time: 2019-09-27T11:55:48.815221, step: 368, loss: 0.5950504541397095, acc: 0.875, recall: 0.8428571428571429, precision: 0.921875, f_beta: 0.880597014925373\n",
      "training: time: 2019-09-27T11:55:52.132686, step: 369, loss: 0.4345938563346863, acc: 0.9140625, recall: 0.8787878787878788, precision: 0.9508196721311475, f_beta: 0.9133858267716536\n",
      "training: time: 2019-09-27T11:55:55.448144, step: 370, loss: 0.38957715034484863, acc: 0.921875, recall: 0.9393939393939394, precision: 0.9117647058823529, f_beta: 0.9253731343283583\n",
      "training: time: 2019-09-27T11:55:58.688754, step: 371, loss: 0.42492103576660156, acc: 0.9140625, recall: 0.9076923076923077, precision: 0.921875, f_beta: 0.9147286821705427\n",
      "training: time: 2019-09-27T11:56:01.974811, step: 372, loss: 0.4633907079696655, acc: 0.8984375, recall: 0.890625, precision: 0.9047619047619048, f_beta: 0.8976377952755906\n",
      "training: time: 2019-09-27T11:56:05.312330, step: 373, loss: 0.4457961618900299, acc: 0.9140625, recall: 0.9014084507042254, precision: 0.9411764705882353, f_beta: 0.920863309352518\n",
      "training: time: 2019-09-27T11:56:08.591242, step: 374, loss: 0.3504736125469208, acc: 0.9296875, recall: 0.9473684210526315, precision: 0.9, f_beta: 0.9230769230769231\n",
      "training: time: 2019-09-27T11:56:11.888182, step: 375, loss: 0.5332307815551758, acc: 0.8828125, recall: 0.8840579710144928, precision: 0.8970588235294118, f_beta: 0.8905109489051095\n",
      "training: time: 2019-09-27T11:56:15.163731, step: 376, loss: 0.5022118091583252, acc: 0.90625, recall: 0.9253731343283582, precision: 0.8985507246376812, f_beta: 0.9117647058823529\n",
      "training: time: 2019-09-27T11:56:18.486378, step: 377, loss: 0.3381563127040863, acc: 0.9296875, recall: 0.953125, precision: 0.9104477611940298, f_beta: 0.931297709923664\n",
      "training: time: 2019-09-27T11:56:21.889486, step: 378, loss: 0.6981938481330872, acc: 0.875, recall: 0.9104477611940298, precision: 0.8591549295774648, f_beta: 0.8840579710144928\n",
      "training: time: 2019-09-27T11:56:25.218763, step: 379, loss: 0.7391117811203003, acc: 0.84375, recall: 0.8985507246376812, precision: 0.8266666666666667, f_beta: 0.8611111111111112\n",
      "training: time: 2019-09-27T11:56:28.577182, step: 380, loss: 0.45242857933044434, acc: 0.90625, recall: 0.9661016949152542, precision: 0.8507462686567164, f_beta: 0.9047619047619047\n",
      "training: time: 2019-09-27T11:56:31.860500, step: 381, loss: 0.5444679260253906, acc: 0.8828125, recall: 0.9180327868852459, precision: 0.8484848484848485, f_beta: 0.8818897637795275\n",
      "training: time: 2019-09-27T11:56:35.184772, step: 382, loss: 0.5821648836135864, acc: 0.8984375, recall: 0.9384615384615385, precision: 0.8714285714285714, f_beta: 0.9037037037037037\n",
      "training: time: 2019-09-27T11:56:38.621588, step: 383, loss: 0.5150641798973083, acc: 0.8984375, recall: 0.8888888888888888, precision: 0.927536231884058, f_beta: 0.9078014184397163\n",
      "training: time: 2019-09-27T11:56:41.929643, step: 384, loss: 0.4278797209262848, acc: 0.921875, recall: 0.9166666666666666, precision: 0.9166666666666666, f_beta: 0.9166666666666666\n",
      "training: time: 2019-09-27T11:56:45.277610, step: 385, loss: 0.5326645374298096, acc: 0.90625, recall: 0.875, precision: 0.9333333333333333, f_beta: 0.9032258064516129\n",
      "training: time: 2019-09-27T11:56:48.581326, step: 386, loss: 0.48146945238113403, acc: 0.8984375, recall: 0.873015873015873, precision: 0.9166666666666666, f_beta: 0.894308943089431\n",
      "training: time: 2019-09-27T11:56:51.824498, step: 387, loss: 0.4854620695114136, acc: 0.8828125, recall: 0.8095238095238095, precision: 0.9444444444444444, f_beta: 0.8717948717948718\n",
      "training: time: 2019-09-27T11:56:55.136661, step: 388, loss: 0.5378295183181763, acc: 0.875, recall: 0.8870967741935484, precision: 0.859375, f_beta: 0.8730158730158729\n",
      "training: time: 2019-09-27T11:56:58.395346, step: 389, loss: 0.46599993109703064, acc: 0.90625, recall: 0.9428571428571428, precision: 0.8918918918918919, f_beta: 0.9166666666666667\n",
      "training: time: 2019-09-27T11:57:01.689988, step: 390, loss: 0.5907649397850037, acc: 0.8828125, recall: 0.8970588235294118, precision: 0.8840579710144928, f_beta: 0.8905109489051095\n",
      "training: time: 2019-09-27T11:57:04.889975, step: 391, loss: 0.6994242668151855, acc: 0.859375, recall: 0.8695652173913043, precision: 0.8695652173913043, f_beta: 0.8695652173913043\n",
      "training: time: 2019-09-27T11:57:08.216134, step: 392, loss: 0.6565407514572144, acc: 0.8828125, recall: 0.9365079365079365, precision: 0.8428571428571429, f_beta: 0.887218045112782\n",
      "training: time: 2019-09-27T11:57:11.530966, step: 393, loss: 0.5695033669471741, acc: 0.859375, recall: 0.9253731343283582, precision: 0.8266666666666667, f_beta: 0.8732394366197183\n",
      "training: time: 2019-09-27T11:57:14.889472, step: 394, loss: 0.6140099763870239, acc: 0.875, recall: 0.9365079365079365, precision: 0.8309859154929577, f_beta: 0.8805970149253731\n",
      "training: time: 2019-09-27T11:57:18.176261, step: 395, loss: 0.5955365896224976, acc: 0.8671875, recall: 0.8679245283018868, precision: 0.8214285714285714, f_beta: 0.8440366972477065\n",
      "training: time: 2019-09-27T11:57:21.532028, step: 396, loss: 0.6340699791908264, acc: 0.8828125, recall: 0.8676470588235294, precision: 0.9076923076923077, f_beta: 0.887218045112782\n",
      "training: time: 2019-09-27T11:57:24.883767, step: 397, loss: 0.6470276117324829, acc: 0.859375, recall: 0.8253968253968254, precision: 0.8813559322033898, f_beta: 0.8524590163934426\n",
      "training: time: 2019-09-27T11:57:28.233836, step: 398, loss: 0.5077406167984009, acc: 0.890625, recall: 0.8214285714285714, precision: 0.92, f_beta: 0.8679245283018867\n",
      "training: time: 2019-09-27T11:57:31.555283, step: 399, loss: 0.6197354197502136, acc: 0.8203125, recall: 0.7313432835820896, precision: 0.9074074074074074, f_beta: 0.8099173553719008\n",
      "training: time: 2019-09-27T11:57:34.889908, step: 400, loss: 0.47768712043762207, acc: 0.875, recall: 0.8405797101449275, precision: 0.9206349206349206, f_beta: 0.8787878787878787\n",
      "begin to evaluation: \n",
      "\n",
      "evaluation: time: 2019-09-27T11:59:50.353886, step: 400, loss: 0.5843189641451224, acc: 0.8776041666666666, recall: 0.8540810391792204, precision: 0.8980422764735144, f_beta: 0.874892010555473\n",
      "save model checkpoint to C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\adversarialLSTM\\model\\adversarialLSTM-400 \n",
      "\n",
      "training: time: 2019-09-27T11:59:55.394330, step: 401, loss: 0.4916059672832489, acc: 0.890625, recall: 0.8524590163934426, precision: 0.9122807017543859, f_beta: 0.8813559322033898\n",
      "training: time: 2019-09-27T11:59:58.981626, step: 402, loss: 0.5050598382949829, acc: 0.8984375, recall: 0.8923076923076924, precision: 0.90625, f_beta: 0.8992248062015504\n",
      "training: time: 2019-09-27T12:00:02.654718, step: 403, loss: 0.567288875579834, acc: 0.8828125, recall: 0.90625, precision: 0.8656716417910447, f_beta: 0.8854961832061069\n",
      "training: time: 2019-09-27T12:00:06.231829, step: 404, loss: 0.5083188414573669, acc: 0.921875, recall: 0.9375, precision: 0.9090909090909091, f_beta: 0.923076923076923\n",
      "training: time: 2019-09-27T12:00:10.012499, step: 405, loss: 0.5084665417671204, acc: 0.90625, recall: 0.9508196721311475, precision: 0.8656716417910447, f_beta: 0.9062499999999999\n",
      "training: time: 2019-09-27T12:00:13.720919, step: 406, loss: 0.6874823570251465, acc: 0.84375, recall: 0.9454545454545454, precision: 0.7536231884057971, f_beta: 0.8387096774193549\n",
      "training: time: 2019-09-27T12:00:17.428734, step: 407, loss: 0.4305921792984009, acc: 0.921875, recall: 0.9552238805970149, precision: 0.9014084507042254, f_beta: 0.927536231884058\n",
      "training: time: 2019-09-27T12:00:20.987698, step: 408, loss: 0.6663703918457031, acc: 0.859375, recall: 0.9193548387096774, precision: 0.8142857142857143, f_beta: 0.8636363636363636\n",
      "training: time: 2019-09-27T12:00:24.664907, step: 409, loss: 0.5880352258682251, acc: 0.875, recall: 0.8970588235294118, precision: 0.8714285714285714, f_beta: 0.8840579710144928\n",
      "training: time: 2019-09-27T12:00:28.213160, step: 410, loss: 0.2809196710586548, acc: 0.96875, recall: 0.9836065573770492, precision: 0.9523809523809523, f_beta: 0.9677419354838709\n",
      "training: time: 2019-09-27T12:00:31.828305, step: 411, loss: 0.5109228491783142, acc: 0.9140625, recall: 0.8939393939393939, precision: 0.9365079365079365, f_beta: 0.9147286821705426\n",
      "training: time: 2019-09-27T12:00:35.386828, step: 412, loss: 0.5764673948287964, acc: 0.8671875, recall: 0.8507462686567164, precision: 0.890625, f_beta: 0.8702290076335878\n",
      "training: time: 2019-09-27T12:00:39.015127, step: 413, loss: 0.5277668237686157, acc: 0.8984375, recall: 0.8709677419354839, precision: 0.9152542372881356, f_beta: 0.8925619834710744\n",
      "training: time: 2019-09-27T12:00:42.668124, step: 414, loss: 0.5010017156600952, acc: 0.8828125, recall: 0.859375, precision: 0.9016393442622951, f_beta: 0.88\n",
      "training: time: 2019-09-27T12:00:46.433524, step: 415, loss: 0.5259052515029907, acc: 0.90625, recall: 0.8983050847457628, precision: 0.8983050847457628, f_beta: 0.8983050847457628\n",
      "training: time: 2019-09-27T12:00:50.061765, step: 416, loss: 0.4841194152832031, acc: 0.890625, recall: 0.8970588235294118, precision: 0.8970588235294118, f_beta: 0.8970588235294118\n",
      "training: time: 2019-09-27T12:00:53.694715, step: 417, loss: 0.44946402311325073, acc: 0.9296875, recall: 0.9538461538461539, precision: 0.9117647058823529, f_beta: 0.9323308270676691\n",
      "training: time: 2019-09-27T12:00:57.266094, step: 418, loss: 0.47840896248817444, acc: 0.921875, recall: 0.8787878787878788, precision: 0.9666666666666667, f_beta: 0.9206349206349207\n",
      "training: time: 2019-09-27T12:01:00.844370, step: 419, loss: 0.6746752858161926, acc: 0.8671875, recall: 0.8545454545454545, precision: 0.8392857142857143, f_beta: 0.8468468468468467\n",
      "training: time: 2019-09-27T12:01:04.473149, step: 420, loss: 0.6864101886749268, acc: 0.8359375, recall: 0.9344262295081968, precision: 0.7702702702702703, f_beta: 0.8444444444444444\n",
      "training: time: 2019-09-27T12:01:08.106509, step: 421, loss: 0.6429957151412964, acc: 0.875, recall: 0.8596491228070176, precision: 0.8596491228070176, f_beta: 0.8596491228070176\n",
      "training: time: 2019-09-27T12:01:11.688490, step: 422, loss: 0.7242734432220459, acc: 0.84375, recall: 0.835820895522388, precision: 0.8615384615384616, f_beta: 0.8484848484848485\n",
      "training: time: 2019-09-27T12:01:15.326350, step: 423, loss: 0.5122063159942627, acc: 0.890625, recall: 0.9344262295081968, precision: 0.8507462686567164, f_beta: 0.8906250000000001\n",
      "training: time: 2019-09-27T12:01:19.133867, step: 424, loss: 0.7025899887084961, acc: 0.859375, recall: 0.8484848484848485, precision: 0.875, f_beta: 0.8615384615384615\n",
      "training: time: 2019-09-27T12:01:22.675211, step: 425, loss: 0.47196924686431885, acc: 0.8984375, recall: 0.8970588235294118, precision: 0.9104477611940298, f_beta: 0.9037037037037037\n",
      "training: time: 2019-09-27T12:01:26.330745, step: 426, loss: 0.41800010204315186, acc: 0.8984375, recall: 0.8666666666666667, precision: 0.9122807017543859, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T12:01:29.949035, step: 427, loss: 0.5114604234695435, acc: 0.8828125, recall: 0.8813559322033898, precision: 0.8666666666666667, f_beta: 0.8739495798319329\n",
      "training: time: 2019-09-27T12:01:33.525684, step: 428, loss: 0.4398016333580017, acc: 0.890625, recall: 0.8955223880597015, precision: 0.8955223880597015, f_beta: 0.8955223880597015\n",
      "training: time: 2019-09-27T12:01:37.012587, step: 429, loss: 0.4822869896888733, acc: 0.90625, recall: 0.8793103448275862, precision: 0.9107142857142857, f_beta: 0.8947368421052632\n",
      "training: time: 2019-09-27T12:01:40.627113, step: 430, loss: 0.4526764750480652, acc: 0.9140625, recall: 0.9264705882352942, precision: 0.9130434782608695, f_beta: 0.9197080291970804\n",
      "training: time: 2019-09-27T12:01:44.361827, step: 431, loss: 0.4330693483352661, acc: 0.921875, recall: 0.9117647058823529, precision: 0.9393939393939394, f_beta: 0.9253731343283583\n",
      "training: time: 2019-09-27T12:01:48.086625, step: 432, loss: 0.4311808943748474, acc: 0.9140625, recall: 0.8571428571428571, precision: 0.9411764705882353, f_beta: 0.897196261682243\n",
      "training: time: 2019-09-27T12:01:51.680850, step: 433, loss: 0.5273976922035217, acc: 0.90625, recall: 0.90625, precision: 0.90625, f_beta: 0.90625\n",
      "training: time: 2019-09-27T12:01:55.401853, step: 434, loss: 0.4744519889354706, acc: 0.8984375, recall: 0.9726027397260274, precision: 0.8658536585365854, f_beta: 0.9161290322580644\n",
      "training: time: 2019-09-27T12:01:58.992182, step: 435, loss: 0.5423754453659058, acc: 0.90625, recall: 0.8873239436619719, precision: 0.9402985074626866, f_beta: 0.9130434782608696\n",
      "training: time: 2019-09-27T12:02:02.667261, step: 436, loss: 0.4989488124847412, acc: 0.8984375, recall: 0.9130434782608695, precision: 0.9, f_beta: 0.9064748201438848\n",
      "training: time: 2019-09-27T12:02:06.243655, step: 437, loss: 0.4256277084350586, acc: 0.90625, recall: 0.9523809523809523, precision: 0.8695652173913043, f_beta: 0.909090909090909\n",
      "training: time: 2019-09-27T12:02:09.917523, step: 438, loss: 0.4246200621128082, acc: 0.9140625, recall: 0.8833333333333333, precision: 0.9298245614035088, f_beta: 0.905982905982906\n",
      "training: time: 2019-09-27T12:02:13.480227, step: 439, loss: 0.7113723754882812, acc: 0.875, recall: 0.890625, precision: 0.8636363636363636, f_beta: 0.8769230769230768\n",
      "training: time: 2019-09-27T12:02:17.055140, step: 440, loss: 0.6254459619522095, acc: 0.890625, recall: 0.8709677419354839, precision: 0.9, f_beta: 0.8852459016393444\n",
      "training: time: 2019-09-27T12:02:20.593602, step: 441, loss: 0.42181846499443054, acc: 0.9140625, recall: 0.8970588235294118, precision: 0.9384615384615385, f_beta: 0.9172932330827067\n",
      "training: time: 2019-09-27T12:02:24.170192, step: 442, loss: 0.6116254329681396, acc: 0.8515625, recall: 0.8125, precision: 0.8813559322033898, f_beta: 0.8455284552845529\n",
      "training: time: 2019-09-27T12:02:27.644373, step: 443, loss: 0.7682209014892578, acc: 0.84375, recall: 0.8714285714285714, precision: 0.8472222222222222, f_beta: 0.8591549295774648\n",
      "training: time: 2019-09-27T12:02:31.345141, step: 444, loss: 0.42177116870880127, acc: 0.9375, recall: 0.9482758620689655, precision: 0.9166666666666666, f_beta: 0.9322033898305084\n",
      "training: time: 2019-09-27T12:02:34.839940, step: 445, loss: 0.33911246061325073, acc: 0.9375, recall: 0.9298245614035088, precision: 0.9298245614035088, f_beta: 0.9298245614035088\n",
      "training: time: 2019-09-27T12:02:38.393428, step: 446, loss: 0.36539438366889954, acc: 0.953125, recall: 0.9672131147540983, precision: 0.9365079365079365, f_beta: 0.9516129032258064\n",
      "training: time: 2019-09-27T12:02:41.976505, step: 447, loss: 0.3808494806289673, acc: 0.9140625, recall: 0.890625, precision: 0.9344262295081968, f_beta: 0.9120000000000001\n",
      "training: time: 2019-09-27T12:02:45.539287, step: 448, loss: 0.6546075344085693, acc: 0.875, recall: 0.8840579710144928, precision: 0.8840579710144928, f_beta: 0.8840579710144928\n",
      "training: time: 2019-09-27T12:02:49.055935, step: 449, loss: 0.5205076932907104, acc: 0.8984375, recall: 0.9142857142857143, precision: 0.9014084507042254, f_beta: 0.9078014184397163\n",
      "training: time: 2019-09-27T12:02:52.689997, step: 450, loss: 0.733364462852478, acc: 0.84375, recall: 0.9076923076923077, precision: 0.8082191780821918, f_beta: 0.8550724637681159\n",
      "training: time: 2019-09-27T12:02:56.303265, step: 451, loss: 0.5085513591766357, acc: 0.8828125, recall: 0.9166666666666666, precision: 0.8461538461538461, f_beta: 0.8799999999999999\n",
      "training: time: 2019-09-27T12:03:00.042494, step: 452, loss: 0.4385858476161957, acc: 0.90625, recall: 0.9636363636363636, precision: 0.8412698412698413, f_beta: 0.8983050847457628\n",
      "training: time: 2019-09-27T12:03:03.699722, step: 453, loss: 0.42561984062194824, acc: 0.90625, recall: 0.9523809523809523, precision: 0.8695652173913043, f_beta: 0.909090909090909\n",
      "training: time: 2019-09-27T12:03:07.330104, step: 454, loss: 0.5308723449707031, acc: 0.875, recall: 0.9420289855072463, precision: 0.8441558441558441, f_beta: 0.8904109589041096\n",
      "training: time: 2019-09-27T12:03:11.010759, step: 455, loss: 0.5348355770111084, acc: 0.8984375, recall: 0.8787878787878788, precision: 0.9206349206349206, f_beta: 0.8992248062015504\n",
      "training: time: 2019-09-27T12:03:14.604361, step: 456, loss: 0.4178012013435364, acc: 0.921875, recall: 0.9180327868852459, precision: 0.9180327868852459, f_beta: 0.9180327868852459\n",
      "training: time: 2019-09-27T12:03:18.204770, step: 457, loss: 0.6392658948898315, acc: 0.8046875, recall: 0.7258064516129032, precision: 0.8490566037735849, f_beta: 0.782608695652174\n",
      "training: time: 2019-09-27T12:03:21.902443, step: 458, loss: 0.3548579812049866, acc: 0.9375, recall: 0.9344262295081968, precision: 0.9344262295081968, f_beta: 0.9344262295081968\n",
      "training: time: 2019-09-27T12:03:25.482062, step: 459, loss: 0.6054425239562988, acc: 0.8984375, recall: 0.8571428571428571, precision: 0.9523809523809523, f_beta: 0.9022556390977443\n",
      "training: time: 2019-09-27T12:03:29.024669, step: 460, loss: 0.45516061782836914, acc: 0.9140625, recall: 0.9272727272727272, precision: 0.8793103448275862, f_beta: 0.902654867256637\n",
      "training: time: 2019-09-27T12:03:32.801105, step: 461, loss: 0.39334309101104736, acc: 0.9375, recall: 0.9411764705882353, precision: 0.9056603773584906, f_beta: 0.923076923076923\n",
      "training: time: 2019-09-27T12:03:36.388965, step: 462, loss: 0.4135229289531708, acc: 0.8984375, recall: 0.8947368421052632, precision: 0.9315068493150684, f_beta: 0.912751677852349\n",
      "training: time: 2019-09-27T12:03:40.090602, step: 463, loss: 0.481981098651886, acc: 0.8984375, recall: 0.9682539682539683, precision: 0.8472222222222222, f_beta: 0.9037037037037037\n",
      "training: time: 2019-09-27T12:03:43.645049, step: 464, loss: 0.5175669193267822, acc: 0.8828125, recall: 0.958904109589041, precision: 0.8536585365853658, f_beta: 0.9032258064516128\n",
      "training: time: 2019-09-27T12:03:47.349275, step: 465, loss: 0.6621960401535034, acc: 0.875, recall: 0.8714285714285714, precision: 0.8970588235294118, f_beta: 0.8840579710144928\n",
      "training: time: 2019-09-27T12:03:51.230259, step: 466, loss: 0.6894901990890503, acc: 0.8828125, recall: 0.8676470588235294, precision: 0.9076923076923077, f_beta: 0.887218045112782\n",
      "training: time: 2019-09-27T12:03:55.261386, step: 467, loss: 0.41872185468673706, acc: 0.9140625, recall: 0.9358974358974359, precision: 0.9240506329113924, f_beta: 0.9299363057324842\n",
      "training: time: 2019-09-27T12:03:59.082864, step: 468, loss: 0.49011868238449097, acc: 0.9140625, recall: 0.8904109589041096, precision: 0.9558823529411765, f_beta: 0.9219858156028369\n",
      "begin to train model...\n",
      "the 3-th / 5 epoch\n",
      "training: time: 2019-09-27T12:04:02.782197, step: 469, loss: 0.2831727862358093, acc: 0.9453125, recall: 0.953125, precision: 0.9384615384615385, f_beta: 0.9457364341085271\n",
      "training: time: 2019-09-27T12:04:06.412561, step: 470, loss: 0.5294522047042847, acc: 0.8984375, recall: 0.9090909090909091, precision: 0.8955223880597015, f_beta: 0.9022556390977443\n",
      "training: time: 2019-09-27T12:04:10.109806, step: 471, loss: 0.553676962852478, acc: 0.90625, recall: 0.9152542372881356, precision: 0.8852459016393442, f_beta: 0.9\n",
      "training: time: 2019-09-27T12:04:13.743674, step: 472, loss: 0.3330824077129364, acc: 0.9296875, recall: 0.9523809523809523, precision: 0.9090909090909091, f_beta: 0.9302325581395349\n",
      "training: time: 2019-09-27T12:04:17.359132, step: 473, loss: 0.4545476734638214, acc: 0.9296875, recall: 0.953125, precision: 0.9104477611940298, f_beta: 0.931297709923664\n",
      "training: time: 2019-09-27T12:04:21.024074, step: 474, loss: 0.48719438910484314, acc: 0.90625, recall: 0.8769230769230769, precision: 0.9344262295081968, f_beta: 0.9047619047619049\n",
      "training: time: 2019-09-27T12:04:24.711493, step: 475, loss: 0.3846116065979004, acc: 0.90625, recall: 0.8939393939393939, precision: 0.921875, f_beta: 0.9076923076923077\n",
      "training: time: 2019-09-27T12:04:28.332373, step: 476, loss: 0.4375631809234619, acc: 0.90625, recall: 0.9354838709677419, precision: 0.8787878787878788, f_beta: 0.90625\n",
      "training: time: 2019-09-27T12:04:32.079421, step: 477, loss: 0.6197652220726013, acc: 0.90625, recall: 0.9552238805970149, precision: 0.8767123287671232, f_beta: 0.9142857142857143\n",
      "training: time: 2019-09-27T12:04:35.735469, step: 478, loss: 0.464538037776947, acc: 0.9296875, recall: 0.9027777777777778, precision: 0.9701492537313433, f_beta: 0.9352517985611511\n",
      "training: time: 2019-09-27T12:04:39.355002, step: 479, loss: 0.42944085597991943, acc: 0.9140625, recall: 0.8873239436619719, precision: 0.9545454545454546, f_beta: 0.9197080291970803\n",
      "training: time: 2019-09-27T12:04:42.980642, step: 480, loss: 0.5208926200866699, acc: 0.8984375, recall: 0.9027777777777778, precision: 0.9154929577464789, f_beta: 0.9090909090909091\n",
      "training: time: 2019-09-27T12:04:46.576316, step: 481, loss: 0.5590266585350037, acc: 0.875, recall: 0.8870967741935484, precision: 0.859375, f_beta: 0.8730158730158729\n",
      "training: time: 2019-09-27T12:04:50.235426, step: 482, loss: 0.496504545211792, acc: 0.890625, recall: 0.8909090909090909, precision: 0.8596491228070176, f_beta: 0.875\n",
      "training: time: 2019-09-27T12:04:53.847718, step: 483, loss: 0.4399474859237671, acc: 0.9140625, recall: 0.9242424242424242, precision: 0.9104477611940298, f_beta: 0.9172932330827067\n",
      "training: time: 2019-09-27T12:04:57.404372, step: 484, loss: 0.4913039803504944, acc: 0.9140625, recall: 0.8983050847457628, precision: 0.9137931034482759, f_beta: 0.9059829059829059\n",
      "training: time: 2019-09-27T12:05:01.016902, step: 485, loss: 0.6417850255966187, acc: 0.84375, recall: 0.8947368421052632, precision: 0.7846153846153846, f_beta: 0.8360655737704918\n",
      "training: time: 2019-09-27T12:05:04.620181, step: 486, loss: 0.4558751583099365, acc: 0.9296875, recall: 0.9090909090909091, precision: 0.9259259259259259, f_beta: 0.9174311926605504\n",
      "training: time: 2019-09-27T12:05:08.240176, step: 487, loss: 0.35910168290138245, acc: 0.921875, recall: 0.8961038961038961, precision: 0.971830985915493, f_beta: 0.9324324324324325\n",
      "training: time: 2019-09-27T12:05:11.861486, step: 488, loss: 0.5714179277420044, acc: 0.890625, recall: 0.9210526315789473, precision: 0.8974358974358975, f_beta: 0.9090909090909091\n",
      "training: time: 2019-09-27T12:05:15.415343, step: 489, loss: 0.5026966333389282, acc: 0.890625, recall: 0.9166666666666666, precision: 0.859375, f_beta: 0.8870967741935484\n",
      "training: time: 2019-09-27T12:05:19.235632, step: 490, loss: 0.36864233016967773, acc: 0.9453125, recall: 0.96875, precision: 0.9253731343283582, f_beta: 0.9465648854961832\n",
      "training: time: 2019-09-27T12:05:22.768778, step: 491, loss: 0.32912808656692505, acc: 0.921875, recall: 0.9411764705882353, precision: 0.9142857142857143, f_beta: 0.9275362318840579\n",
      "training: time: 2019-09-27T12:05:26.461954, step: 492, loss: 0.4564456343650818, acc: 0.890625, recall: 0.8933333333333333, precision: 0.9178082191780822, f_beta: 0.9054054054054055\n",
      "training: time: 2019-09-27T12:05:30.107949, step: 493, loss: 0.3312763273715973, acc: 0.921875, recall: 0.9047619047619048, precision: 0.9344262295081968, f_beta: 0.9193548387096775\n",
      "training: time: 2019-09-27T12:05:33.734955, step: 494, loss: 0.5660239458084106, acc: 0.875, recall: 0.8676470588235294, precision: 0.8939393939393939, f_beta: 0.8805970149253731\n",
      "training: time: 2019-09-27T12:05:37.419685, step: 495, loss: 0.44990748167037964, acc: 0.90625, recall: 0.9104477611940298, precision: 0.9104477611940298, f_beta: 0.9104477611940298\n",
      "training: time: 2019-09-27T12:05:41.205897, step: 496, loss: 0.4110698401927948, acc: 0.921875, recall: 0.8870967741935484, precision: 0.9482758620689655, f_beta: 0.9166666666666667\n",
      "training: time: 2019-09-27T12:05:44.783701, step: 497, loss: 0.4983501732349396, acc: 0.890625, recall: 0.9565217391304348, precision: 0.8571428571428571, f_beta: 0.904109589041096\n",
      "training: time: 2019-09-27T12:05:48.460728, step: 498, loss: 0.40895524621009827, acc: 0.921875, recall: 0.9154929577464789, precision: 0.9420289855072463, f_beta: 0.9285714285714286\n",
      "training: time: 2019-09-27T12:05:52.044095, step: 499, loss: 0.5706907510757446, acc: 0.8984375, recall: 0.8857142857142857, precision: 0.9253731343283582, f_beta: 0.9051094890510949\n",
      "training: time: 2019-09-27T12:05:55.638164, step: 500, loss: 0.32705754041671753, acc: 0.9453125, recall: 0.9701492537313433, precision: 0.9285714285714286, f_beta: 0.948905109489051\n",
      "begin to evaluation: \n",
      "\n",
      "evaluation: time: 2019-09-27T12:08:17.061814, step: 500, loss: 0.6200131422434098, acc: 0.8707932692307693, recall: 0.9122417645091753, precision: 0.8452002627099495, f_beta: 0.8766050012714134\n",
      "save model checkpoint to C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\adversarialLSTM\\model\\adversarialLSTM-500 \n",
      "\n",
      "training: time: 2019-09-27T12:08:22.553516, step: 501, loss: 0.5968418121337891, acc: 0.859375, recall: 0.9491525423728814, precision: 0.7887323943661971, f_beta: 0.8615384615384616\n",
      "training: time: 2019-09-27T12:08:26.591039, step: 502, loss: 0.4280591607093811, acc: 0.890625, recall: 0.9310344827586207, precision: 0.84375, f_beta: 0.8852459016393444\n",
      "training: time: 2019-09-27T12:08:30.428277, step: 503, loss: 0.5258547067642212, acc: 0.90625, recall: 0.9298245614035088, precision: 0.8688524590163934, f_beta: 0.8983050847457625\n",
      "training: time: 2019-09-27T12:08:34.287147, step: 504, loss: 0.4685954451560974, acc: 0.9140625, recall: 0.9242424242424242, precision: 0.9104477611940298, f_beta: 0.9172932330827067\n",
      "training: time: 2019-09-27T12:08:38.638899, step: 505, loss: 0.770817756652832, acc: 0.8515625, recall: 0.8333333333333334, precision: 0.8181818181818182, f_beta: 0.8256880733944955\n",
      "training: time: 2019-09-27T12:08:42.402873, step: 506, loss: 0.4515964388847351, acc: 0.8984375, recall: 0.8387096774193549, precision: 0.9454545454545454, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T12:08:46.425411, step: 507, loss: 0.5469202995300293, acc: 0.8828125, recall: 0.8148148148148148, precision: 0.8979591836734694, f_beta: 0.8543689320388349\n",
      "training: time: 2019-09-27T12:08:50.554818, step: 508, loss: 0.3861343264579773, acc: 0.9140625, recall: 0.8615384615384616, precision: 0.9655172413793104, f_beta: 0.9105691056910569\n",
      "training: time: 2019-09-27T12:08:54.481078, step: 509, loss: 0.45978546142578125, acc: 0.8984375, recall: 0.8970588235294118, precision: 0.9104477611940298, f_beta: 0.9037037037037037\n",
      "training: time: 2019-09-27T12:08:58.390922, step: 510, loss: 0.43716686964035034, acc: 0.9296875, recall: 0.9054054054054054, precision: 0.9710144927536232, f_beta: 0.937062937062937\n",
      "training: time: 2019-09-27T12:09:02.366289, step: 511, loss: 0.42379868030548096, acc: 0.8984375, recall: 0.8909090909090909, precision: 0.875, f_beta: 0.8828828828828829\n",
      "training: time: 2019-09-27T12:09:06.219486, step: 512, loss: 0.3944527208805084, acc: 0.921875, recall: 0.9848484848484849, precision: 0.8783783783783784, f_beta: 0.9285714285714285\n",
      "training: time: 2019-09-27T12:09:09.956927, step: 513, loss: 0.5180317759513855, acc: 0.8828125, recall: 0.9423076923076923, precision: 0.8032786885245902, f_beta: 0.8672566371681416\n",
      "training: time: 2019-09-27T12:09:13.632098, step: 514, loss: 0.5285119414329529, acc: 0.8984375, recall: 0.9104477611940298, precision: 0.8970588235294118, f_beta: 0.9037037037037037\n",
      "training: time: 2019-09-27T12:09:17.428163, step: 515, loss: 0.3196012079715729, acc: 0.9296875, recall: 0.9552238805970149, precision: 0.9142857142857143, f_beta: 0.9343065693430657\n",
      "training: time: 2019-09-27T12:09:21.406315, step: 516, loss: 0.41069817543029785, acc: 0.921875, recall: 0.9242424242424242, precision: 0.9242424242424242, f_beta: 0.9242424242424242\n",
      "training: time: 2019-09-27T12:09:25.673401, step: 517, loss: 0.5246946811676025, acc: 0.8828125, recall: 0.9166666666666666, precision: 0.8461538461538461, f_beta: 0.8799999999999999\n",
      "training: time: 2019-09-27T12:09:29.754534, step: 518, loss: 0.4701269567012787, acc: 0.90625, recall: 0.9142857142857143, precision: 0.9142857142857143, f_beta: 0.9142857142857143\n",
      "training: time: 2019-09-27T12:09:33.524411, step: 519, loss: 0.366499125957489, acc: 0.9375, recall: 0.9285714285714286, precision: 0.9558823529411765, f_beta: 0.9420289855072465\n",
      "training: time: 2019-09-27T12:09:37.324906, step: 520, loss: 0.516480565071106, acc: 0.8984375, recall: 0.864406779661017, precision: 0.9107142857142857, f_beta: 0.8869565217391304\n",
      "training: time: 2019-09-27T12:09:41.103955, step: 521, loss: 0.46057969331741333, acc: 0.9296875, recall: 0.8870967741935484, precision: 0.9649122807017544, f_beta: 0.9243697478991597\n",
      "training: time: 2019-09-27T12:09:44.707333, step: 522, loss: 0.39031457901000977, acc: 0.890625, recall: 0.8611111111111112, precision: 0.9393939393939394, f_beta: 0.8985507246376813\n",
      "training: time: 2019-09-27T12:09:48.381320, step: 523, loss: 0.42458707094192505, acc: 0.90625, recall: 0.9206349206349206, precision: 0.8923076923076924, f_beta: 0.90625\n",
      "training: time: 2019-09-27T12:09:52.038022, step: 524, loss: 0.36111024022102356, acc: 0.9453125, recall: 0.9344262295081968, precision: 0.95, f_beta: 0.9421487603305784\n",
      "training: time: 2019-09-27T12:09:55.687124, step: 525, loss: 0.3967798352241516, acc: 0.9296875, recall: 0.9142857142857143, precision: 0.9552238805970149, f_beta: 0.9343065693430657\n",
      "training: time: 2019-09-27T12:09:59.496486, step: 526, loss: 0.41811275482177734, acc: 0.9296875, recall: 0.9, precision: 0.9473684210526315, f_beta: 0.9230769230769231\n",
      "training: time: 2019-09-27T12:10:03.054062, step: 527, loss: 0.5951752662658691, acc: 0.9140625, recall: 0.9393939393939394, precision: 0.8985507246376812, f_beta: 0.9185185185185185\n",
      "training: time: 2019-09-27T12:10:06.745836, step: 528, loss: 0.6069349646568298, acc: 0.8671875, recall: 0.9230769230769231, precision: 0.8333333333333334, f_beta: 0.8759124087591241\n",
      "training: time: 2019-09-27T12:10:10.375939, step: 529, loss: 0.35408109426498413, acc: 0.953125, recall: 0.967741935483871, precision: 0.9375, f_beta: 0.9523809523809523\n",
      "training: time: 2019-09-27T12:10:14.441569, step: 530, loss: 0.6147087812423706, acc: 0.8671875, recall: 0.9420289855072463, precision: 0.8333333333333334, f_beta: 0.8843537414965987\n",
      "training: time: 2019-09-27T12:10:18.639365, step: 531, loss: 0.40928441286087036, acc: 0.921875, recall: 0.9682539682539683, precision: 0.8840579710144928, f_beta: 0.9242424242424243\n",
      "training: time: 2019-09-27T12:10:22.734561, step: 532, loss: 0.4003710448741913, acc: 0.9296875, recall: 0.9705882352941176, precision: 0.9041095890410958, f_beta: 0.9361702127659575\n",
      "training: time: 2019-09-27T12:10:26.604077, step: 533, loss: 0.5967826843261719, acc: 0.890625, recall: 0.9090909090909091, precision: 0.8823529411764706, f_beta: 0.8955223880597014\n",
      "training: time: 2019-09-27T12:10:30.541538, step: 534, loss: 0.33470457792282104, acc: 0.9140625, recall: 0.9322033898305084, precision: 0.8870967741935484, f_beta: 0.9090909090909092\n",
      "training: time: 2019-09-27T12:10:34.183516, step: 535, loss: 0.5767905712127686, acc: 0.890625, recall: 0.8833333333333333, precision: 0.8833333333333333, f_beta: 0.8833333333333333\n",
      "training: time: 2019-09-27T12:10:38.028635, step: 536, loss: 0.45439988374710083, acc: 0.890625, recall: 0.8135593220338984, precision: 0.9411764705882353, f_beta: 0.8727272727272728\n",
      "training: time: 2019-09-27T12:10:41.988585, step: 537, loss: 0.4338168799877167, acc: 0.90625, recall: 0.90625, precision: 0.90625, f_beta: 0.90625\n",
      "training: time: 2019-09-27T12:10:45.747638, step: 538, loss: 0.5333090424537659, acc: 0.890625, recall: 0.8615384615384616, precision: 0.9180327868852459, f_beta: 0.8888888888888888\n",
      "training: time: 2019-09-27T12:10:49.614667, step: 539, loss: 0.41454750299453735, acc: 0.9453125, recall: 0.9705882352941176, precision: 0.9295774647887324, f_beta: 0.9496402877697842\n",
      "training: time: 2019-09-27T12:10:53.390810, step: 540, loss: 0.36995935440063477, acc: 0.9140625, recall: 0.9508196721311475, precision: 0.8787878787878788, f_beta: 0.9133858267716536\n",
      "training: time: 2019-09-27T12:10:56.971276, step: 541, loss: 0.3824373483657837, acc: 0.9375, recall: 0.9636363636363636, precision: 0.8983050847457628, f_beta: 0.9298245614035089\n",
      "training: time: 2019-09-27T12:11:00.638656, step: 542, loss: 0.4589773416519165, acc: 0.90625, recall: 0.9130434782608695, precision: 0.9130434782608695, f_beta: 0.9130434782608695\n",
      "training: time: 2019-09-27T12:11:04.247746, step: 543, loss: 0.3579881191253662, acc: 0.9453125, recall: 0.9242424242424242, precision: 0.9682539682539683, f_beta: 0.9457364341085271\n",
      "training: time: 2019-09-27T12:11:07.980863, step: 544, loss: 0.36728858947753906, acc: 0.9140625, recall: 0.8947368421052632, precision: 0.9107142857142857, f_beta: 0.9026548672566371\n",
      "training: time: 2019-09-27T12:11:11.603926, step: 545, loss: 0.5409353971481323, acc: 0.8828125, recall: 0.8181818181818182, precision: 0.9473684210526315, f_beta: 0.8780487804878049\n",
      "training: time: 2019-09-27T12:11:15.283375, step: 546, loss: 0.5412843227386475, acc: 0.8984375, recall: 0.9090909090909091, precision: 0.8620689655172413, f_beta: 0.8849557522123893\n",
      "training: time: 2019-09-27T12:11:18.978557, step: 547, loss: 0.4030316174030304, acc: 0.9375, recall: 0.9264705882352942, precision: 0.9545454545454546, f_beta: 0.9402985074626866\n",
      "training: time: 2019-09-27T12:11:23.261337, step: 548, loss: 0.32628899812698364, acc: 0.9296875, recall: 0.9452054794520548, precision: 0.9324324324324325, f_beta: 0.9387755102040816\n",
      "training: time: 2019-09-27T12:11:27.108963, step: 549, loss: 0.37871599197387695, acc: 0.9140625, recall: 0.921875, precision: 0.9076923076923077, f_beta: 0.9147286821705427\n",
      "training: time: 2019-09-27T12:11:30.751320, step: 550, loss: 0.686241626739502, acc: 0.875, recall: 0.8870967741935484, precision: 0.859375, f_beta: 0.8730158730158729\n",
      "training: time: 2019-09-27T12:11:34.594248, step: 551, loss: 0.5960313081741333, acc: 0.859375, recall: 0.8918918918918919, precision: 0.868421052631579, f_beta: 0.88\n",
      "training: time: 2019-09-27T12:11:38.512406, step: 552, loss: 0.3789083957672119, acc: 0.9296875, recall: 0.9322033898305084, precision: 0.9166666666666666, f_beta: 0.9243697478991596\n",
      "training: time: 2019-09-27T12:11:42.146920, step: 553, loss: 0.34575700759887695, acc: 0.921875, recall: 0.9508196721311475, precision: 0.8923076923076924, f_beta: 0.9206349206349206\n",
      "training: time: 2019-09-27T12:11:45.768941, step: 554, loss: 0.4045509099960327, acc: 0.921875, recall: 0.9508196721311475, precision: 0.8923076923076924, f_beta: 0.9206349206349206\n",
      "training: time: 2019-09-27T12:11:49.451649, step: 555, loss: 0.4969114065170288, acc: 0.875, recall: 0.9041095890410958, precision: 0.88, f_beta: 0.8918918918918919\n",
      "training: time: 2019-09-27T12:11:53.135444, step: 556, loss: 0.41307002305984497, acc: 0.921875, recall: 0.9090909090909091, precision: 0.9375, f_beta: 0.923076923076923\n",
      "training: time: 2019-09-27T12:11:57.317900, step: 557, loss: 0.3619413375854492, acc: 0.9296875, recall: 0.9193548387096774, precision: 0.9344262295081968, f_beta: 0.9268292682926829\n",
      "training: time: 2019-09-27T12:12:01.350622, step: 558, loss: 0.5126667022705078, acc: 0.8984375, recall: 0.90625, precision: 0.8923076923076924, f_beta: 0.8992248062015504\n",
      "training: time: 2019-09-27T12:12:04.988377, step: 559, loss: 0.2844690680503845, acc: 0.953125, recall: 0.9552238805970149, precision: 0.9552238805970149, f_beta: 0.9552238805970149\n",
      "training: time: 2019-09-27T12:12:08.535235, step: 560, loss: 0.3006801903247833, acc: 0.921875, recall: 0.9074074074074074, precision: 0.9074074074074074, f_beta: 0.9074074074074074\n",
      "training: time: 2019-09-27T12:12:12.177699, step: 561, loss: 0.572715699672699, acc: 0.875, recall: 0.9242424242424242, precision: 0.8472222222222222, f_beta: 0.8840579710144927\n",
      "training: time: 2019-09-27T12:12:15.770846, step: 562, loss: 0.3816068172454834, acc: 0.9375, recall: 0.9076923076923077, precision: 0.9672131147540983, f_beta: 0.9365079365079365\n",
      "training: time: 2019-09-27T12:12:19.533502, step: 563, loss: 0.5849599838256836, acc: 0.9140625, recall: 0.953125, precision: 0.8840579710144928, f_beta: 0.9172932330827068\n",
      "training: time: 2019-09-27T12:12:23.234331, step: 564, loss: 0.3343377113342285, acc: 0.9140625, recall: 0.9230769230769231, precision: 0.9090909090909091, f_beta: 0.9160305343511451\n",
      "training: time: 2019-09-27T12:12:27.017372, step: 565, loss: 0.3550988733768463, acc: 0.921875, recall: 0.9076923076923077, precision: 0.9365079365079365, f_beta: 0.9218749999999999\n",
      "training: time: 2019-09-27T12:12:30.791041, step: 566, loss: 0.3943534791469574, acc: 0.9375, recall: 0.9803921568627451, precision: 0.8771929824561403, f_beta: 0.9259259259259259\n",
      "training: time: 2019-09-27T12:12:34.494348, step: 567, loss: 0.24467769265174866, acc: 0.9453125, recall: 0.9838709677419355, precision: 0.9104477611940298, f_beta: 0.9457364341085271\n",
      "training: time: 2019-09-27T12:12:38.258963, step: 568, loss: 0.3965509235858917, acc: 0.9296875, recall: 0.9, precision: 0.9692307692307692, f_beta: 0.9333333333333333\n",
      "training: time: 2019-09-27T12:12:42.027143, step: 569, loss: 0.5803320407867432, acc: 0.8828125, recall: 0.8653846153846154, precision: 0.8490566037735849, f_beta: 0.8571428571428571\n",
      "training: time: 2019-09-27T12:12:45.872250, step: 570, loss: 0.4634365439414978, acc: 0.8984375, recall: 0.8840579710144928, precision: 0.9242424242424242, f_beta: 0.9037037037037037\n",
      "training: time: 2019-09-27T12:12:49.430335, step: 571, loss: 0.5116593837738037, acc: 0.890625, recall: 0.828125, precision: 0.9464285714285714, f_beta: 0.8833333333333333\n",
      "training: time: 2019-09-27T12:12:53.010699, step: 572, loss: 0.5443956851959229, acc: 0.9140625, recall: 0.9285714285714286, precision: 0.8813559322033898, f_beta: 0.9043478260869564\n",
      "training: time: 2019-09-27T12:12:56.650133, step: 573, loss: 0.3892804980278015, acc: 0.9453125, recall: 0.9538461538461539, precision: 0.9393939393939394, f_beta: 0.9465648854961831\n",
      "training: time: 2019-09-27T12:13:00.294363, step: 574, loss: 0.46114224195480347, acc: 0.9140625, recall: 0.9384615384615385, precision: 0.8970588235294118, f_beta: 0.9172932330827067\n",
      "training: time: 2019-09-27T12:13:04.046755, step: 575, loss: 0.537665843963623, acc: 0.875, recall: 0.9047619047619048, precision: 0.8507462686567164, f_beta: 0.8769230769230769\n",
      "training: time: 2019-09-27T12:13:07.661444, step: 576, loss: 0.4324745237827301, acc: 0.9140625, recall: 0.9090909090909091, precision: 0.8928571428571429, f_beta: 0.9009009009009009\n",
      "training: time: 2019-09-27T12:13:11.370983, step: 577, loss: 0.42145833373069763, acc: 0.875, recall: 0.9166666666666666, precision: 0.8333333333333334, f_beta: 0.8730158730158729\n",
      "training: time: 2019-09-27T12:13:15.015536, step: 578, loss: 0.34063300490379333, acc: 0.953125, recall: 0.9705882352941176, precision: 0.9428571428571428, f_beta: 0.9565217391304348\n",
      "training: time: 2019-09-27T12:13:18.592683, step: 579, loss: 0.6110435724258423, acc: 0.8515625, recall: 0.828125, precision: 0.8688524590163934, f_beta: 0.8480000000000001\n",
      "training: time: 2019-09-27T12:13:22.248458, step: 580, loss: 0.3006954789161682, acc: 0.9375, recall: 0.9393939393939394, precision: 0.9393939393939394, f_beta: 0.9393939393939394\n",
      "training: time: 2019-09-27T12:13:25.871162, step: 581, loss: 0.6490347385406494, acc: 0.890625, recall: 0.8823529411764706, precision: 0.9090909090909091, f_beta: 0.8955223880597014\n",
      "training: time: 2019-09-27T12:13:29.691551, step: 582, loss: 0.5110012888908386, acc: 0.90625, recall: 0.8484848484848485, precision: 0.9655172413793104, f_beta: 0.9032258064516129\n",
      "training: time: 2019-09-27T12:13:33.481758, step: 583, loss: 0.3526710867881775, acc: 0.9375, recall: 0.9056603773584906, precision: 0.9411764705882353, f_beta: 0.923076923076923\n",
      "training: time: 2019-09-27T12:13:37.070607, step: 584, loss: 0.49603867530822754, acc: 0.90625, recall: 0.9423076923076923, precision: 0.8448275862068966, f_beta: 0.890909090909091\n",
      "training: time: 2019-09-27T12:13:40.723636, step: 585, loss: 0.4299595355987549, acc: 0.9296875, recall: 0.9848484848484849, precision: 0.8904109589041096, f_beta: 0.935251798561151\n",
      "training: time: 2019-09-27T12:13:44.383634, step: 586, loss: 0.40360260009765625, acc: 0.9140625, recall: 0.9444444444444444, precision: 0.864406779661017, f_beta: 0.9026548672566371\n",
      "training: time: 2019-09-27T12:13:48.023467, step: 587, loss: 0.5372766256332397, acc: 0.8828125, recall: 0.859375, precision: 0.9016393442622951, f_beta: 0.88\n",
      "training: time: 2019-09-27T12:13:51.712283, step: 588, loss: 0.4606356620788574, acc: 0.921875, recall: 0.9523809523809523, precision: 0.8955223880597015, f_beta: 0.923076923076923\n",
      "training: time: 2019-09-27T12:13:55.357527, step: 589, loss: 0.3644742965698242, acc: 0.9453125, recall: 0.927536231884058, precision: 0.9696969696969697, f_beta: 0.9481481481481481\n",
      "training: time: 2019-09-27T12:13:59.071521, step: 590, loss: 0.5552859306335449, acc: 0.890625, recall: 0.8620689655172413, precision: 0.8928571428571429, f_beta: 0.8771929824561403\n",
      "training: time: 2019-09-27T12:14:02.610977, step: 591, loss: 0.33974090218544006, acc: 0.9375, recall: 0.9333333333333333, precision: 0.9333333333333333, f_beta: 0.9333333333333333\n",
      "training: time: 2019-09-27T12:14:06.409060, step: 592, loss: 0.47505128383636475, acc: 0.921875, recall: 0.9206349206349206, precision: 0.9206349206349206, f_beta: 0.9206349206349206\n",
      "training: time: 2019-09-27T12:14:10.040454, step: 593, loss: 0.5117603540420532, acc: 0.875, recall: 0.9, precision: 0.84375, f_beta: 0.870967741935484\n",
      "training: time: 2019-09-27T12:14:13.720638, step: 594, loss: 0.6026638150215149, acc: 0.8671875, recall: 0.8846153846153846, precision: 0.8961038961038961, f_beta: 0.8903225806451613\n",
      "training: time: 2019-09-27T12:14:17.264834, step: 595, loss: 0.6659526228904724, acc: 0.859375, recall: 0.859375, precision: 0.859375, f_beta: 0.859375\n",
      "training: time: 2019-09-27T12:14:20.822471, step: 596, loss: 0.40000540018081665, acc: 0.9296875, recall: 0.9242424242424242, precision: 0.9384615384615385, f_beta: 0.9312977099236641\n",
      "training: time: 2019-09-27T12:14:24.476592, step: 597, loss: 0.516281008720398, acc: 0.890625, recall: 0.881578947368421, precision: 0.9305555555555556, f_beta: 0.9054054054054054\n",
      "training: time: 2019-09-27T12:14:27.989439, step: 598, loss: 0.5875216722488403, acc: 0.8984375, recall: 0.9206349206349206, precision: 0.8787878787878788, f_beta: 0.8992248062015504\n",
      "training: time: 2019-09-27T12:14:31.635030, step: 599, loss: 0.45602983236312866, acc: 0.8984375, recall: 0.8970588235294118, precision: 0.9104477611940298, f_beta: 0.9037037037037037\n",
      "training: time: 2019-09-27T12:14:35.373059, step: 600, loss: 0.3760058581829071, acc: 0.9375, recall: 0.9508196721311475, precision: 0.9206349206349206, f_beta: 0.9354838709677418\n",
      "begin to evaluation: \n",
      "\n",
      "evaluation: time: 2019-09-27T12:16:56.164160, step: 600, loss: 0.6066494935598129, acc: 0.8719951923076923, recall: 0.9089257327469641, precision: 0.8489111249826481, f_beta: 0.8769107200153733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0927 12:16:56.310768 17616 deprecation.py:323] From d:\\Users\\123\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model checkpoint to C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\adversarialLSTM\\model\\adversarialLSTM-600 \n",
      "\n",
      "training: time: 2019-09-27T12:17:02.303243, step: 601, loss: 0.6239484548568726, acc: 0.8359375, recall: 0.9117647058823529, precision: 0.8051948051948052, f_beta: 0.8551724137931035\n",
      "training: time: 2019-09-27T12:17:06.231912, step: 602, loss: 0.5308969020843506, acc: 0.8515625, recall: 0.8840579710144928, precision: 0.8472222222222222, f_beta: 0.8652482269503546\n",
      "training: time: 2019-09-27T12:17:10.273605, step: 603, loss: 0.48731115460395813, acc: 0.9140625, recall: 0.9452054794520548, precision: 0.9078947368421053, f_beta: 0.9261744966442953\n",
      "training: time: 2019-09-27T12:17:14.038483, step: 604, loss: 0.5149098634719849, acc: 0.890625, recall: 0.8833333333333333, precision: 0.8833333333333333, f_beta: 0.8833333333333333\n",
      "training: time: 2019-09-27T12:17:17.704832, step: 605, loss: 0.4294637441635132, acc: 0.9140625, recall: 0.9245283018867925, precision: 0.875, f_beta: 0.8990825688073395\n",
      "training: time: 2019-09-27T12:17:21.332770, step: 606, loss: 0.3601657748222351, acc: 0.953125, recall: 0.9491525423728814, precision: 0.9491525423728814, f_beta: 0.9491525423728814\n",
      "training: time: 2019-09-27T12:17:24.944812, step: 607, loss: 0.5129003524780273, acc: 0.875, recall: 0.7884615384615384, precision: 0.8913043478260869, f_beta: 0.836734693877551\n",
      "training: time: 2019-09-27T12:17:28.963982, step: 608, loss: 0.5171395540237427, acc: 0.875, recall: 0.7846153846153846, precision: 0.9622641509433962, f_beta: 0.864406779661017\n",
      "training: time: 2019-09-27T12:17:32.865283, step: 609, loss: 0.4498773217201233, acc: 0.875, recall: 0.8307692307692308, precision: 0.9152542372881356, f_beta: 0.870967741935484\n",
      "training: time: 2019-09-27T12:17:36.225297, step: 610, loss: 0.5630383491516113, acc: 0.90625, recall: 0.890625, precision: 0.9193548387096774, f_beta: 0.9047619047619047\n",
      "training: time: 2019-09-27T12:17:39.880372, step: 611, loss: 0.5663430690765381, acc: 0.890625, recall: 0.9122807017543859, precision: 0.8524590163934426, f_beta: 0.8813559322033898\n",
      "training: time: 2019-09-27T12:17:43.267440, step: 612, loss: 0.4292188286781311, acc: 0.890625, recall: 0.9, precision: 0.8709677419354839, f_beta: 0.8852459016393444\n",
      "training: time: 2019-09-27T12:17:46.651033, step: 613, loss: 0.5005530118942261, acc: 0.8984375, recall: 0.9076923076923077, precision: 0.8939393939393939, f_beta: 0.900763358778626\n",
      "training: time: 2019-09-27T12:17:50.221349, step: 614, loss: 0.40311187505722046, acc: 0.8984375, recall: 0.9014084507042254, precision: 0.9142857142857143, f_beta: 0.9078014184397163\n",
      "training: time: 2019-09-27T12:17:53.877860, step: 615, loss: 0.34631961584091187, acc: 0.9296875, recall: 0.8955223880597015, precision: 0.967741935483871, f_beta: 0.930232558139535\n",
      "training: time: 2019-09-27T12:17:57.377406, step: 616, loss: 0.39148086309432983, acc: 0.9296875, recall: 0.9661016949152542, precision: 0.890625, f_beta: 0.9268292682926829\n",
      "training: time: 2019-09-27T12:18:00.768752, step: 617, loss: 0.6200586557388306, acc: 0.8359375, recall: 0.8939393939393939, precision: 0.8082191780821918, f_beta: 0.8489208633093526\n",
      "training: time: 2019-09-27T12:18:04.346839, step: 618, loss: 0.45402103662490845, acc: 0.8828125, recall: 0.9047619047619048, precision: 0.8636363636363636, f_beta: 0.8837209302325582\n",
      "training: time: 2019-09-27T12:18:08.208606, step: 619, loss: 0.4131902754306793, acc: 0.9296875, recall: 0.9516129032258065, precision: 0.9076923076923077, f_beta: 0.9291338582677167\n",
      "training: time: 2019-09-27T12:18:11.774477, step: 620, loss: 0.654839038848877, acc: 0.8515625, recall: 0.8166666666666667, precision: 0.8596491228070176, f_beta: 0.8376068376068376\n",
      "training: time: 2019-09-27T12:18:15.279543, step: 621, loss: 0.36870771646499634, acc: 0.9296875, recall: 0.9354838709677419, precision: 0.9206349206349206, f_beta: 0.9279999999999999\n",
      "training: time: 2019-09-27T12:18:19.015934, step: 622, loss: 0.4701710641384125, acc: 0.921875, recall: 0.9, precision: 0.9545454545454546, f_beta: 0.9264705882352942\n",
      "training: time: 2019-09-27T12:18:23.048995, step: 623, loss: 0.45305371284484863, acc: 0.9140625, recall: 0.9178082191780822, precision: 0.9305555555555556, f_beta: 0.9241379310344828\n",
      "training: time: 2019-09-27T12:18:26.905793, step: 624, loss: 0.5203232765197754, acc: 0.890625, recall: 0.8983050847457628, precision: 0.8688524590163934, f_beta: 0.8833333333333333\n",
      "begin to train model...\n",
      "the 4-th / 5 epoch\n",
      "training: time: 2019-09-27T12:18:31.059015, step: 625, loss: 0.34791257977485657, acc: 0.9140625, recall: 0.8955223880597015, precision: 0.9375, f_beta: 0.9160305343511451\n",
      "training: time: 2019-09-27T12:18:34.812876, step: 626, loss: 0.49067866802215576, acc: 0.90625, recall: 0.8923076923076924, precision: 0.9206349206349206, f_beta: 0.90625\n",
      "training: time: 2019-09-27T12:18:38.417391, step: 627, loss: 0.32757675647735596, acc: 0.9375, recall: 0.9259259259259259, precision: 0.9259259259259259, f_beta: 0.9259259259259259\n",
      "training: time: 2019-09-27T12:18:42.021190, step: 628, loss: 0.4414040744304657, acc: 0.921875, recall: 0.9545454545454546, precision: 0.9, f_beta: 0.9264705882352942\n",
      "training: time: 2019-09-27T12:18:46.062707, step: 629, loss: 0.4530910849571228, acc: 0.875, recall: 0.8636363636363636, precision: 0.890625, f_beta: 0.8769230769230768\n",
      "training: time: 2019-09-27T12:18:49.874472, step: 630, loss: 0.5021020770072937, acc: 0.8828125, recall: 0.8833333333333333, precision: 0.8688524590163934, f_beta: 0.8760330578512396\n",
      "training: time: 2019-09-27T12:18:53.385086, step: 631, loss: 0.4966888427734375, acc: 0.90625, recall: 0.9, precision: 0.9, f_beta: 0.9\n",
      "training: time: 2019-09-27T12:18:56.811145, step: 632, loss: 0.3565927743911743, acc: 0.9296875, recall: 0.948051948051948, precision: 0.9358974358974359, f_beta: 0.9419354838709677\n",
      "training: time: 2019-09-27T12:19:00.659823, step: 633, loss: 0.354547381401062, acc: 0.9453125, recall: 0.9661016949152542, precision: 0.9193548387096774, f_beta: 0.9421487603305785\n",
      "training: time: 2019-09-27T12:19:04.754120, step: 634, loss: 0.44350144267082214, acc: 0.890625, recall: 0.890625, precision: 0.890625, f_beta: 0.890625\n",
      "training: time: 2019-09-27T12:19:08.360127, step: 635, loss: 0.29412513971328735, acc: 0.953125, recall: 0.9333333333333333, precision: 0.9655172413793104, f_beta: 0.9491525423728815\n",
      "training: time: 2019-09-27T12:19:13.665392, step: 636, loss: 0.37148749828338623, acc: 0.9296875, recall: 0.8970588235294118, precision: 0.9682539682539683, f_beta: 0.9312977099236641\n",
      "training: time: 2019-09-27T12:19:18.245590, step: 637, loss: 0.42957615852355957, acc: 0.921875, recall: 0.8857142857142857, precision: 0.96875, f_beta: 0.9253731343283582\n",
      "training: time: 2019-09-27T12:19:22.383871, step: 638, loss: 0.5344211459159851, acc: 0.8984375, recall: 0.8620689655172413, precision: 0.9090909090909091, f_beta: 0.8849557522123893\n",
      "training: time: 2019-09-27T12:19:26.325990, step: 639, loss: 0.36336639523506165, acc: 0.9296875, recall: 0.9444444444444444, precision: 0.9315068493150684, f_beta: 0.9379310344827586\n",
      "training: time: 2019-09-27T12:19:30.476104, step: 640, loss: 0.5074069499969482, acc: 0.9140625, recall: 0.9516129032258065, precision: 0.8805970149253731, f_beta: 0.9147286821705426\n",
      "training: time: 2019-09-27T12:19:34.343219, step: 641, loss: 0.4281236529350281, acc: 0.9296875, recall: 0.9833333333333333, precision: 0.8805970149253731, f_beta: 0.9291338582677166\n",
      "training: time: 2019-09-27T12:19:37.991915, step: 642, loss: 0.331143856048584, acc: 0.921875, recall: 0.9452054794520548, precision: 0.92, f_beta: 0.9324324324324323\n",
      "training: time: 2019-09-27T12:19:41.847923, step: 643, loss: 0.3229411244392395, acc: 0.9453125, recall: 1.0, precision: 0.9078947368421053, f_beta: 0.9517241379310345\n",
      "training: time: 2019-09-27T12:19:45.525569, step: 644, loss: 0.37790200114250183, acc: 0.90625, recall: 0.9516129032258065, precision: 0.8676470588235294, f_beta: 0.9076923076923077\n",
      "training: time: 2019-09-27T12:19:49.879399, step: 645, loss: 0.43556830286979675, acc: 0.9296875, recall: 0.9264705882352942, precision: 0.9402985074626866, f_beta: 0.9333333333333335\n",
      "training: time: 2019-09-27T12:19:53.630021, step: 646, loss: 0.3414955735206604, acc: 0.9296875, recall: 0.8771929824561403, precision: 0.9615384615384616, f_beta: 0.9174311926605504\n",
      "training: time: 2019-09-27T12:19:57.455553, step: 647, loss: 0.38380762934684753, acc: 0.9140625, recall: 0.9, precision: 0.9402985074626866, f_beta: 0.9197080291970803\n",
      "training: time: 2019-09-27T12:20:01.655951, step: 648, loss: 0.3269496262073517, acc: 0.9296875, recall: 0.8888888888888888, precision: 0.9655172413793104, f_beta: 0.9256198347107438\n",
      "training: time: 2019-09-27T12:20:05.824910, step: 649, loss: 0.42688751220703125, acc: 0.921875, recall: 0.9322033898305084, precision: 0.9016393442622951, f_beta: 0.9166666666666666\n",
      "training: time: 2019-09-27T12:20:10.042656, step: 650, loss: 0.3554672598838806, acc: 0.9296875, recall: 0.9393939393939394, precision: 0.9253731343283582, f_beta: 0.9323308270676692\n",
      "training: time: 2019-09-27T12:20:13.683633, step: 651, loss: 0.28016477823257446, acc: 0.9296875, recall: 0.9821428571428571, precision: 0.873015873015873, f_beta: 0.9243697478991596\n",
      "training: time: 2019-09-27T12:20:17.117159, step: 652, loss: 0.2144816368818283, acc: 0.9765625, recall: 0.9841269841269841, precision: 0.96875, f_beta: 0.9763779527559054\n",
      "training: time: 2019-09-27T12:20:20.589056, step: 653, loss: 0.39177441596984863, acc: 0.9140625, recall: 0.953125, precision: 0.8840579710144928, f_beta: 0.9172932330827068\n",
      "training: time: 2019-09-27T12:20:24.021478, step: 654, loss: 0.3763261139392853, acc: 0.9453125, recall: 0.9583333333333334, precision: 0.9452054794520548, f_beta: 0.9517241379310345\n",
      "training: time: 2019-09-27T12:20:27.560187, step: 655, loss: 0.3362279236316681, acc: 0.9375, recall: 0.9152542372881356, precision: 0.9473684210526315, f_beta: 0.9310344827586206\n",
      "training: time: 2019-09-27T12:20:30.950232, step: 656, loss: 0.30317068099975586, acc: 0.9296875, recall: 0.9375, precision: 0.9230769230769231, f_beta: 0.9302325581395349\n",
      "training: time: 2019-09-27T12:20:34.344261, step: 657, loss: 0.29402869939804077, acc: 0.9453125, recall: 0.9482758620689655, precision: 0.9322033898305084, f_beta: 0.94017094017094\n",
      "training: time: 2019-09-27T12:20:37.777105, step: 658, loss: 0.3450108468532562, acc: 0.9375, recall: 0.9090909090909091, precision: 0.967741935483871, f_beta: 0.9374999999999999\n",
      "training: time: 2019-09-27T12:20:41.207694, step: 659, loss: 0.3859819769859314, acc: 0.9140625, recall: 0.8873239436619719, precision: 0.9545454545454546, f_beta: 0.9197080291970803\n",
      "training: time: 2019-09-27T12:20:44.614614, step: 660, loss: 0.4636591672897339, acc: 0.921875, recall: 0.9206349206349206, precision: 0.9206349206349206, f_beta: 0.9206349206349206\n",
      "training: time: 2019-09-27T12:20:48.109466, step: 661, loss: 0.37130922079086304, acc: 0.9453125, recall: 0.9692307692307692, precision: 0.9264705882352942, f_beta: 0.9473684210526316\n",
      "training: time: 2019-09-27T12:20:52.588060, step: 662, loss: 0.28701797127723694, acc: 0.9453125, recall: 1.0, precision: 0.9054054054054054, f_beta: 0.950354609929078\n",
      "training: time: 2019-09-27T12:20:57.009538, step: 663, loss: 0.3501286506652832, acc: 0.921875, recall: 0.9482758620689655, precision: 0.8870967741935484, f_beta: 0.9166666666666667\n",
      "training: time: 2019-09-27T12:21:00.901147, step: 664, loss: 0.3635736107826233, acc: 0.921875, recall: 0.8955223880597015, precision: 0.9523809523809523, f_beta: 0.923076923076923\n",
      "training: time: 2019-09-27T12:21:04.505351, step: 665, loss: 0.3828628361225128, acc: 0.8984375, recall: 0.8947368421052632, precision: 0.8793103448275862, f_beta: 0.8869565217391304\n",
      "training: time: 2019-09-27T12:21:08.070117, step: 666, loss: 0.3400862514972687, acc: 0.9296875, recall: 0.8870967741935484, precision: 0.9649122807017544, f_beta: 0.9243697478991597\n",
      "training: time: 2019-09-27T12:21:11.665204, step: 667, loss: 0.3881206214427948, acc: 0.8984375, recall: 0.8947368421052632, precision: 0.8793103448275862, f_beta: 0.8869565217391304\n",
      "training: time: 2019-09-27T12:21:15.175133, step: 668, loss: 0.36816155910491943, acc: 0.9140625, recall: 0.8870967741935484, precision: 0.9322033898305084, f_beta: 0.9090909090909092\n",
      "training: time: 2019-09-27T12:21:18.638986, step: 669, loss: 0.3269879221916199, acc: 0.9375, recall: 0.927536231884058, precision: 0.9552238805970149, f_beta: 0.9411764705882353\n",
      "training: time: 2019-09-27T12:21:22.181035, step: 670, loss: 0.4067807197570801, acc: 0.9375, recall: 0.9655172413793104, precision: 0.9032258064516129, f_beta: 0.9333333333333333\n",
      "training: time: 2019-09-27T12:21:25.627723, step: 671, loss: 0.3768270015716553, acc: 0.921875, recall: 0.9305555555555556, precision: 0.9305555555555556, f_beta: 0.9305555555555556\n",
      "training: time: 2019-09-27T12:21:29.184349, step: 672, loss: 0.4966033399105072, acc: 0.9296875, recall: 0.9090909090909091, precision: 0.9523809523809523, f_beta: 0.9302325581395349\n",
      "training: time: 2019-09-27T12:21:32.765522, step: 673, loss: 0.29143309593200684, acc: 0.9453125, recall: 0.9666666666666667, precision: 0.9206349206349206, f_beta: 0.943089430894309\n",
      "training: time: 2019-09-27T12:21:36.218765, step: 674, loss: 0.29371464252471924, acc: 0.9375, recall: 0.9444444444444444, precision: 0.9107142857142857, f_beta: 0.9272727272727271\n",
      "training: time: 2019-09-27T12:21:39.720840, step: 675, loss: 0.4668370485305786, acc: 0.8828125, recall: 0.8596491228070176, precision: 0.875, f_beta: 0.8672566371681416\n",
      "training: time: 2019-09-27T12:21:43.233794, step: 676, loss: 0.40020328760147095, acc: 0.921875, recall: 0.9384615384615385, precision: 0.9104477611940298, f_beta: 0.9242424242424243\n",
      "training: time: 2019-09-27T12:21:46.677196, step: 677, loss: 0.3917493224143982, acc: 0.90625, recall: 0.9032258064516129, precision: 0.9032258064516129, f_beta: 0.9032258064516129\n",
      "training: time: 2019-09-27T12:21:50.182433, step: 678, loss: 0.38184329867362976, acc: 0.9375, recall: 0.9166666666666666, precision: 0.9482758620689655, f_beta: 0.9322033898305084\n",
      "training: time: 2019-09-27T12:21:53.617430, step: 679, loss: 0.48152947425842285, acc: 0.8828125, recall: 0.8873239436619719, precision: 0.9, f_beta: 0.8936170212765958\n",
      "training: time: 2019-09-27T12:21:57.195029, step: 680, loss: 0.30387482047080994, acc: 0.921875, recall: 0.9264705882352942, precision: 0.9264705882352942, f_beta: 0.9264705882352942\n",
      "training: time: 2019-09-27T12:22:00.663706, step: 681, loss: 0.27424710988998413, acc: 0.953125, recall: 0.9841269841269841, precision: 0.9253731343283582, f_beta: 0.9538461538461538\n",
      "training: time: 2019-09-27T12:22:04.162822, step: 682, loss: 0.30035150051116943, acc: 0.9453125, recall: 0.9322033898305084, precision: 0.9482758620689655, f_beta: 0.94017094017094\n",
      "training: time: 2019-09-27T12:22:07.544393, step: 683, loss: 0.4146929979324341, acc: 0.90625, recall: 0.9285714285714286, precision: 0.9027777777777778, f_beta: 0.9154929577464788\n",
      "training: time: 2019-09-27T12:22:11.088533, step: 684, loss: 0.2640700042247772, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9696969696969697, f_beta: 0.955223880597015\n",
      "training: time: 2019-09-27T12:22:14.552255, step: 685, loss: 0.3073131740093231, acc: 0.9296875, recall: 0.9423076923076923, precision: 0.8909090909090909, f_beta: 0.9158878504672897\n",
      "training: time: 2019-09-27T12:22:18.063384, step: 686, loss: 0.5181363821029663, acc: 0.90625, recall: 0.96875, precision: 0.8611111111111112, f_beta: 0.911764705882353\n",
      "training: time: 2019-09-27T12:22:21.603824, step: 687, loss: 0.39977705478668213, acc: 0.9609375, recall: 0.9393939393939394, precision: 0.9841269841269841, f_beta: 0.9612403100775193\n",
      "training: time: 2019-09-27T12:22:25.159043, step: 688, loss: 0.4504077434539795, acc: 0.921875, recall: 0.8904109589041096, precision: 0.9701492537313433, f_beta: 0.9285714285714287\n",
      "training: time: 2019-09-27T12:22:28.653674, step: 689, loss: 0.3981991410255432, acc: 0.90625, recall: 0.9508196721311475, precision: 0.8656716417910447, f_beta: 0.9062499999999999\n",
      "training: time: 2019-09-27T12:22:32.130962, step: 690, loss: 0.3019687533378601, acc: 0.9140625, recall: 0.935064935064935, precision: 0.9230769230769231, f_beta: 0.9290322580645162\n",
      "training: time: 2019-09-27T12:22:35.684893, step: 691, loss: 0.3706539273262024, acc: 0.921875, recall: 0.9047619047619048, precision: 0.9344262295081968, f_beta: 0.9193548387096775\n",
      "training: time: 2019-09-27T12:22:39.263369, step: 692, loss: 0.30965662002563477, acc: 0.9375, recall: 0.967741935483871, precision: 0.9090909090909091, f_beta: 0.9374999999999999\n",
      "training: time: 2019-09-27T12:22:42.748001, step: 693, loss: 0.48183542490005493, acc: 0.8828125, recall: 0.8787878787878788, precision: 0.8923076923076924, f_beta: 0.8854961832061069\n",
      "training: time: 2019-09-27T12:22:46.267583, step: 694, loss: 0.29944908618927, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9696969696969697, f_beta: 0.955223880597015\n",
      "training: time: 2019-09-27T12:22:49.816170, step: 695, loss: 0.3428146541118622, acc: 0.953125, recall: 0.9692307692307692, precision: 0.9402985074626866, f_beta: 0.9545454545454547\n",
      "training: time: 2019-09-27T12:22:53.326217, step: 696, loss: 0.356577068567276, acc: 0.9296875, recall: 0.9154929577464789, precision: 0.9558823529411765, f_beta: 0.9352517985611511\n",
      "training: time: 2019-09-27T12:22:56.742225, step: 697, loss: 0.3396625518798828, acc: 0.953125, recall: 1.0, precision: 0.9047619047619048, f_beta: 0.9500000000000001\n",
      "training: time: 2019-09-27T12:23:00.266767, step: 698, loss: 0.3598519563674927, acc: 0.9296875, recall: 0.927536231884058, precision: 0.9411764705882353, f_beta: 0.9343065693430658\n",
      "training: time: 2019-09-27T12:23:03.729108, step: 699, loss: 0.3256987929344177, acc: 0.921875, recall: 0.875, precision: 0.984375, f_beta: 0.9264705882352942\n",
      "training: time: 2019-09-27T12:23:07.217757, step: 700, loss: 0.21615879237651825, acc: 0.9765625, recall: 0.9642857142857143, precision: 0.9818181818181818, f_beta: 0.972972972972973\n",
      "begin to evaluation: \n",
      "\n",
      "evaluation: time: 2019-09-27T12:25:22.224742, step: 700, loss: 0.6598041944014721, acc: 0.8743990384615384, recall: 0.8809882720310387, precision: 0.8718102662848406, f_beta: 0.8753911701118472\n",
      "save model checkpoint to C:\\Users\\123\\Documents\\python_experence\\nlp_model\\model\\adversarialLSTM\\model\\adversarialLSTM-700 \n",
      "\n",
      "training: time: 2019-09-27T12:25:26.806765, step: 701, loss: 0.3183819651603699, acc: 0.9375, recall: 0.927536231884058, precision: 0.9552238805970149, f_beta: 0.9411764705882353\n",
      "training: time: 2019-09-27T12:25:30.290487, step: 702, loss: 0.4660399258136749, acc: 0.8828125, recall: 0.9444444444444444, precision: 0.8095238095238095, f_beta: 0.8717948717948718\n",
      "training: time: 2019-09-27T12:25:33.875074, step: 703, loss: 0.5567200779914856, acc: 0.890625, recall: 0.803921568627451, precision: 0.9111111111111111, f_beta: 0.8541666666666666\n",
      "training: time: 2019-09-27T12:25:37.369574, step: 704, loss: 0.3035237789154053, acc: 0.953125, recall: 0.9454545454545454, precision: 0.9454545454545454, f_beta: 0.9454545454545454\n",
      "training: time: 2019-09-27T12:25:40.835180, step: 705, loss: 0.3679124414920807, acc: 0.9375, recall: 0.9230769230769231, precision: 0.9523809523809523, f_beta: 0.9375\n",
      "training: time: 2019-09-27T12:25:44.347714, step: 706, loss: 0.33562159538269043, acc: 0.9296875, recall: 0.9264705882352942, precision: 0.9402985074626866, f_beta: 0.9333333333333335\n",
      "training: time: 2019-09-27T12:25:47.830096, step: 707, loss: 0.3176906108856201, acc: 0.9453125, recall: 0.921875, precision: 0.9672131147540983, f_beta: 0.944\n",
      "training: time: 2019-09-27T12:25:51.345306, step: 708, loss: 0.2829344868659973, acc: 0.9453125, recall: 0.9516129032258065, precision: 0.9365079365079365, f_beta: 0.944\n",
      "training: time: 2019-09-27T12:25:54.934768, step: 709, loss: 0.442746102809906, acc: 0.890625, recall: 0.9047619047619048, precision: 0.8769230769230769, f_beta: 0.890625\n",
      "training: time: 2019-09-27T12:25:58.529231, step: 710, loss: 0.3645332455635071, acc: 0.9296875, recall: 0.9342105263157895, precision: 0.9466666666666667, f_beta: 0.9403973509933775\n",
      "training: time: 2019-09-27T12:26:02.069016, step: 711, loss: 0.4314959645271301, acc: 0.9296875, recall: 0.9841269841269841, precision: 0.8857142857142857, f_beta: 0.9323308270676691\n",
      "training: time: 2019-09-27T12:26:05.482967, step: 712, loss: 0.6512024402618408, acc: 0.890625, recall: 0.9104477611940298, precision: 0.8840579710144928, f_beta: 0.8970588235294118\n",
      "training: time: 2019-09-27T12:26:08.959946, step: 713, loss: 0.3633958101272583, acc: 0.9375, recall: 0.9714285714285714, precision: 0.918918918918919, f_beta: 0.9444444444444445\n",
      "training: time: 2019-09-27T12:26:12.490004, step: 714, loss: 0.3266991972923279, acc: 0.953125, recall: 0.9607843137254902, precision: 0.9245283018867925, f_beta: 0.9423076923076923\n",
      "training: time: 2019-09-27T12:26:16.008285, step: 715, loss: 0.36014825105667114, acc: 0.921875, recall: 0.8955223880597015, precision: 0.9523809523809523, f_beta: 0.923076923076923\n",
      "training: time: 2019-09-27T12:26:19.536337, step: 716, loss: 0.3681187927722931, acc: 0.9375, recall: 0.9166666666666666, precision: 0.9482758620689655, f_beta: 0.9322033898305084\n",
      "training: time: 2019-09-27T12:26:23.063176, step: 717, loss: 0.731552243232727, acc: 0.859375, recall: 0.8285714285714286, precision: 0.90625, f_beta: 0.8656716417910447\n",
      "training: time: 2019-09-27T12:26:26.535612, step: 718, loss: 0.5856145620346069, acc: 0.875, recall: 0.8095238095238095, precision: 0.9272727272727272, f_beta: 0.864406779661017\n",
      "training: time: 2019-09-27T12:26:29.990791, step: 719, loss: 0.2990313172340393, acc: 0.9609375, recall: 0.9666666666666667, precision: 0.9508196721311475, f_beta: 0.9586776859504132\n",
      "training: time: 2019-09-27T12:26:33.504667, step: 720, loss: 0.44943463802337646, acc: 0.9453125, recall: 0.9558823529411765, precision: 0.9420289855072463, f_beta: 0.9489051094890512\n",
      "training: time: 2019-09-27T12:26:36.960095, step: 721, loss: 0.4620843231678009, acc: 0.90625, recall: 0.9436619718309859, precision: 0.8933333333333333, f_beta: 0.9178082191780823\n",
      "training: time: 2019-09-27T12:26:40.425796, step: 722, loss: 0.4177234470844269, acc: 0.9296875, recall: 0.95, precision: 0.9047619047619048, f_beta: 0.9268292682926829\n",
      "training: time: 2019-09-27T12:26:43.927971, step: 723, loss: 0.45327723026275635, acc: 0.8984375, recall: 0.9344262295081968, precision: 0.8636363636363636, f_beta: 0.8976377952755905\n",
      "training: time: 2019-09-27T12:26:47.379720, step: 724, loss: 0.28752589225769043, acc: 0.921875, recall: 0.9393939393939394, precision: 0.9117647058823529, f_beta: 0.9253731343283583\n",
      "training: time: 2019-09-27T12:26:50.826025, step: 725, loss: 0.41936495900154114, acc: 0.90625, recall: 0.8970588235294118, precision: 0.9242424242424242, f_beta: 0.9104477611940298\n",
      "training: time: 2019-09-27T12:26:54.388162, step: 726, loss: 0.37788599729537964, acc: 0.9375, recall: 0.9692307692307692, precision: 0.9130434782608695, f_beta: 0.9402985074626865\n",
      "training: time: 2019-09-27T12:26:57.859810, step: 727, loss: 0.3591509759426117, acc: 0.9375, recall: 0.9577464788732394, precision: 0.9315068493150684, f_beta: 0.9444444444444444\n",
      "training: time: 2019-09-27T12:27:02.028158, step: 728, loss: 0.3689291477203369, acc: 0.9140625, recall: 0.8688524590163934, precision: 0.9464285714285714, f_beta: 0.9059829059829059\n",
      "training: time: 2019-09-27T12:27:06.094058, step: 729, loss: 0.4478578269481659, acc: 0.90625, recall: 0.8933333333333333, precision: 0.9436619718309859, f_beta: 0.9178082191780823\n",
      "training: time: 2019-09-27T12:27:09.576239, step: 730, loss: 0.4674444794654846, acc: 0.90625, recall: 0.9242424242424242, precision: 0.8970588235294118, f_beta: 0.9104477611940298\n",
      "training: time: 2019-09-27T12:27:13.042837, step: 731, loss: 0.4761195182800293, acc: 0.9140625, recall: 0.9152542372881356, precision: 0.9, f_beta: 0.9075630252100839\n",
      "training: time: 2019-09-27T12:27:16.537252, step: 732, loss: 0.32241421937942505, acc: 0.921875, recall: 0.9402985074626866, precision: 0.9130434782608695, f_beta: 0.9264705882352942\n",
      "training: time: 2019-09-27T12:27:20.038199, step: 733, loss: 0.31656450033187866, acc: 0.9453125, recall: 0.9649122807017544, precision: 0.9166666666666666, f_beta: 0.9401709401709402\n",
      "training: time: 2019-09-27T12:27:23.562996, step: 734, loss: 0.3443262577056885, acc: 0.9375, recall: 0.9655172413793104, precision: 0.9032258064516129, f_beta: 0.9333333333333333\n",
      "training: time: 2019-09-27T12:27:27.012984, step: 735, loss: 0.41314369440078735, acc: 0.90625, recall: 0.9215686274509803, precision: 0.8545454545454545, f_beta: 0.8867924528301887\n",
      "training: time: 2019-09-27T12:27:30.566301, step: 736, loss: 0.31440597772598267, acc: 0.9375, recall: 0.927536231884058, precision: 0.9552238805970149, f_beta: 0.9411764705882353\n",
      "training: time: 2019-09-27T12:27:34.026717, step: 737, loss: 0.4830339550971985, acc: 0.921875, recall: 0.8727272727272727, precision: 0.9411764705882353, f_beta: 0.9056603773584905\n",
      "training: time: 2019-09-27T12:27:37.499131, step: 738, loss: 0.32982784509658813, acc: 0.9375, recall: 0.9107142857142857, precision: 0.9444444444444444, f_beta: 0.9272727272727271\n",
      "training: time: 2019-09-27T12:27:40.996270, step: 739, loss: 0.5290868878364563, acc: 0.890625, recall: 0.8387096774193549, precision: 0.9285714285714286, f_beta: 0.8813559322033899\n",
      "training: time: 2019-09-27T12:27:44.512462, step: 740, loss: 0.42957401275634766, acc: 0.921875, recall: 0.9180327868852459, precision: 0.9180327868852459, f_beta: 0.9180327868852459\n",
      "training: time: 2019-09-27T12:27:47.929004, step: 741, loss: 0.3550945520401001, acc: 0.9140625, recall: 0.9264705882352942, precision: 0.9130434782608695, f_beta: 0.9197080291970804\n",
      "training: time: 2019-09-27T12:27:51.441223, step: 742, loss: 0.3493545949459076, acc: 0.90625, recall: 0.9054054054054054, precision: 0.9305555555555556, f_beta: 0.9178082191780821\n",
      "training: time: 2019-09-27T12:27:54.899710, step: 743, loss: 0.3862212598323822, acc: 0.9375, recall: 0.9852941176470589, precision: 0.9054054054054054, f_beta: 0.943661971830986\n",
      "training: time: 2019-09-27T12:27:58.320471, step: 744, loss: 0.30016857385635376, acc: 0.921875, recall: 0.9333333333333333, precision: 0.9333333333333333, f_beta: 0.9333333333333333\n",
      "training: time: 2019-09-27T12:28:01.787854, step: 745, loss: 0.3162883222103119, acc: 0.9453125, recall: 0.9583333333333334, precision: 0.9452054794520548, f_beta: 0.9517241379310345\n",
      "training: time: 2019-09-27T12:28:05.201434, step: 746, loss: 0.5353165864944458, acc: 0.8984375, recall: 0.921875, precision: 0.8805970149253731, f_beta: 0.9007633587786259\n",
      "training: time: 2019-09-27T12:28:08.703313, step: 747, loss: 0.3649418354034424, acc: 0.9296875, recall: 0.9375, precision: 0.9230769230769231, f_beta: 0.9302325581395349\n",
      "training: time: 2019-09-27T12:28:12.186657, step: 748, loss: 0.3040231466293335, acc: 0.9609375, recall: 0.9605263157894737, precision: 0.9733333333333334, f_beta: 0.9668874172185431\n",
      "training: time: 2019-09-27T12:28:15.716760, step: 749, loss: 0.47343313694000244, acc: 0.9296875, recall: 0.9333333333333333, precision: 0.9180327868852459, f_beta: 0.9256198347107439\n",
      "training: time: 2019-09-27T12:28:19.355131, step: 750, loss: 0.41118156909942627, acc: 0.90625, recall: 0.8955223880597015, precision: 0.9230769230769231, f_beta: 0.9090909090909091\n",
      "training: time: 2019-09-27T12:28:22.791118, step: 751, loss: 0.3713519871234894, acc: 0.9375, recall: 0.9253731343283582, precision: 0.9538461538461539, f_beta: 0.9393939393939394\n",
      "training: time: 2019-09-27T12:28:26.200217, step: 752, loss: 0.4300037622451782, acc: 0.8984375, recall: 0.8923076923076924, precision: 0.90625, f_beta: 0.8992248062015504\n",
      "training: time: 2019-09-27T12:28:29.827501, step: 753, loss: 0.43229883909225464, acc: 0.90625, recall: 0.9230769230769231, precision: 0.8955223880597015, f_beta: 0.9090909090909091\n",
      "training: time: 2019-09-27T12:28:33.317206, step: 754, loss: 0.39460301399230957, acc: 0.9375, recall: 0.9538461538461539, precision: 0.9253731343283582, f_beta: 0.9393939393939394\n",
      "training: time: 2019-09-27T12:28:36.873386, step: 755, loss: 0.38815030455589294, acc: 0.9296875, recall: 0.9375, precision: 0.9230769230769231, f_beta: 0.9302325581395349\n",
      "training: time: 2019-09-27T12:28:40.359504, step: 756, loss: 0.4550360143184662, acc: 0.9140625, recall: 0.9295774647887324, precision: 0.9166666666666666, f_beta: 0.9230769230769231\n",
      "training: time: 2019-09-27T12:28:44.001162, step: 757, loss: 0.5394680500030518, acc: 0.890625, recall: 0.8857142857142857, precision: 0.9117647058823529, f_beta: 0.8985507246376812\n",
      "training: time: 2019-09-27T12:28:47.454065, step: 758, loss: 0.3852422833442688, acc: 0.921875, recall: 0.9558823529411765, precision: 0.9027777777777778, f_beta: 0.9285714285714286\n",
      "training: time: 2019-09-27T12:28:50.980910, step: 759, loss: 0.4273703098297119, acc: 0.9375, recall: 0.9558823529411765, precision: 0.9285714285714286, f_beta: 0.9420289855072465\n",
      "training: time: 2019-09-27T12:28:54.381177, step: 760, loss: 0.34932634234428406, acc: 0.90625, recall: 0.9516129032258065, precision: 0.8676470588235294, f_beta: 0.9076923076923077\n",
      "training: time: 2019-09-27T12:28:57.787037, step: 761, loss: 0.30441856384277344, acc: 0.9296875, recall: 0.9538461538461539, precision: 0.9117647058823529, f_beta: 0.9323308270676691\n",
      "training: time: 2019-09-27T12:29:01.254482, step: 762, loss: 0.3975732922554016, acc: 0.9375, recall: 0.90625, precision: 0.9666666666666667, f_beta: 0.9354838709677419\n",
      "training: time: 2019-09-27T12:29:04.761527, step: 763, loss: 0.3794972002506256, acc: 0.9375, recall: 0.9193548387096774, precision: 0.95, f_beta: 0.9344262295081968\n",
      "training: time: 2019-09-27T12:29:08.290953, step: 764, loss: 0.464705228805542, acc: 0.921875, recall: 0.873015873015873, precision: 0.9649122807017544, f_beta: 0.9166666666666667\n",
      "training: time: 2019-09-27T12:29:11.724370, step: 765, loss: 0.24245652556419373, acc: 0.9453125, recall: 0.9322033898305084, precision: 0.9482758620689655, f_beta: 0.94017094017094\n",
      "training: time: 2019-09-27T12:29:15.193008, step: 766, loss: 0.54437255859375, acc: 0.890625, recall: 0.8636363636363636, precision: 0.9193548387096774, f_beta: 0.890625\n",
      "training: time: 2019-09-27T12:29:18.741017, step: 767, loss: 0.4180452823638916, acc: 0.9296875, recall: 0.9285714285714286, precision: 0.9122807017543859, f_beta: 0.9203539823008849\n",
      "training: time: 2019-09-27T12:29:22.282965, step: 768, loss: 0.4539404511451721, acc: 0.9140625, recall: 0.9833333333333333, precision: 0.855072463768116, f_beta: 0.9147286821705427\n",
      "training: time: 2019-09-27T12:29:25.739446, step: 769, loss: 0.4945529103279114, acc: 0.921875, recall: 0.8888888888888888, precision: 0.9230769230769231, f_beta: 0.9056603773584906\n",
      "training: time: 2019-09-27T12:29:29.148276, step: 770, loss: 0.36081498861312866, acc: 0.9375, recall: 0.9444444444444444, precision: 0.9107142857142857, f_beta: 0.9272727272727271\n",
      "training: time: 2019-09-27T12:29:32.615431, step: 771, loss: 0.2889711856842041, acc: 0.9453125, recall: 0.953125, precision: 0.9384615384615385, f_beta: 0.9457364341085271\n",
      "training: time: 2019-09-27T12:29:36.066835, step: 772, loss: 0.4590151607990265, acc: 0.9140625, recall: 0.9402985074626866, precision: 0.9, f_beta: 0.9197080291970803\n",
      "training: time: 2019-09-27T12:29:39.552198, step: 773, loss: 0.4409538805484772, acc: 0.9140625, recall: 0.8253968253968254, precision: 1.0, f_beta: 0.9043478260869565\n",
      "training: time: 2019-09-27T12:29:43.095828, step: 774, loss: 0.48637181520462036, acc: 0.9296875, recall: 0.9, precision: 0.9473684210526315, f_beta: 0.9230769230769231\n",
      "training: time: 2019-09-27T12:29:46.508633, step: 775, loss: 0.6271129250526428, acc: 0.875, recall: 0.847457627118644, precision: 0.8771929824561403, f_beta: 0.8620689655172413\n",
      "training: time: 2019-09-27T12:29:49.985411, step: 776, loss: 0.4805896282196045, acc: 0.8984375, recall: 0.85, precision: 0.9272727272727272, f_beta: 0.8869565217391303\n",
      "training: time: 2019-09-27T12:29:53.460538, step: 777, loss: 0.4130789041519165, acc: 0.921875, recall: 0.9245283018867925, precision: 0.8909090909090909, f_beta: 0.9074074074074073\n",
      "training: time: 2019-09-27T12:29:57.009137, step: 778, loss: 0.5646183490753174, acc: 0.9140625, recall: 0.8888888888888888, precision: 0.9333333333333333, f_beta: 0.9105691056910569\n",
      "training: time: 2019-09-27T12:30:00.417782, step: 779, loss: 0.5441755056381226, acc: 0.90625, recall: 0.9672131147540983, precision: 0.855072463768116, f_beta: 0.9076923076923077\n",
      "training: time: 2019-09-27T12:30:03.920390, step: 780, loss: 0.4538849890232086, acc: 0.890625, recall: 0.9074074074074074, precision: 0.8448275862068966, f_beta: 0.875\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=sess_config)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        lstm = AdversarialLSTM(config, wordEmbedding, indexFreqs)\n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summary\"))\n",
    "        print(\"write to {} \\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter =tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        savedModelPath = \"\\\\\".join(directory_path.split(\"\\\\\")[:-1]) + \"\\\\model\\\\adversarialLSTM\\\\savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "#             os.rmdir(savedModelPath)\n",
    "            import shutil\n",
    "            shutil.rmtree(savedModelPath)\n",
    "        \n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        def trainStep(batchX, batchY):\n",
    "            feed_dict = {\n",
    "                lstm.inputX: batchX, \n",
    "                lstm.inputY: batchY, \n",
    "                lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run([trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions], \n",
    "                                                          feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, prec, recall, f_beta\n",
    "        \n",
    "        \n",
    "        def devStep(batchX, batchY):\n",
    "            feed_dict = {\n",
    "                lstm.inputX: batchX, \n",
    "                lstm.inputY: batchY, \n",
    "                lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run([summaryOp, globalStep, lstm.loss, lstm.predictions], \n",
    "                                                       feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, prec, recall, f_beta\n",
    "        \n",
    "        \n",
    "        for index in range(config.training.epochs):\n",
    "            print(\"begin to train model...\")\n",
    "            print(\"the {}-th / {} epoch\".format(str(index), str(config.training.epochs)))\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "                currentStep = tf.train.global_step(sess, globalStep)\n",
    "                timeStr = datetime.datetime.now().isoformat()\n",
    "                \n",
    "                print(\"training: time: {}, step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                    timeStr, currentStep, loss, acc, recall, prec, f_beta))\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"begin to evaluation: \\n\")\n",
    "                    losses, accs, recalls, precisions, f_betas = [], [], [], [], []\n",
    "                    for evalBatch in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, precision, recall, f_beta = devStep(evalBatch[0], evalBatch[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        recalls.append(recall)\n",
    "                        precisions.append(precision)\n",
    "                        f_betas.append(f_beta)\n",
    "                    \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"evaluation: time: {}, step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                        time_str, currentStep, mean(losses), mean(accs), mean(recalls), mean(precisions), mean(f_betas)))\n",
    "                \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    path = saver.save(sess, \"\\\\\".join(savedModelPath.split(\"\\\\\")[:-1]) + \"\\\\model\\\\adversarialLSTM\", \n",
    "                                     global_step=currentStep)\n",
    "                    print(\"save model checkpoint to {} \\n\".format(path))\n",
    "                \n",
    "#                 inputs = {\n",
    "#                     \"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX), \n",
    "#                     \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)\n",
    "#                 }\n",
    "#                 outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(lstm.predictions)}\n",
    "                \n",
    "#                 prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, \n",
    "#                                                                                              outputs=outputs, \n",
    "#                                                                                              method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "                \n",
    "#                 legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#                 builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING], \n",
    "#                                                     signature_def_map={\"predict\": prediction_signature, }, \n",
    "#                                                     legacy_init_op=legacy_init_op)\n",
    "\n",
    "# #                 builder.add_meta_graph(sess, [tf.saved_model.tag_constants.SERVING], \n",
    "# #                                                     signature_def_map={\"predict\": prediction_signature, }, \n",
    "# #                                                     legacy_init_op=legacy_init_op)\n",
    "                \n",
    "#                 builder.save()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = \"this movie is full of references like mad max ii the wild one and many others the ladybug´s face it´s a clear reference or tribute to peter lorre this movie is a masterpiece we´ll talk much more about in the future\"\n",
    "\n",
    "with open(directory_path + \"\\\\wordJson\\\\word2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx = json.load(f)\n",
    "\n",
    "with open(directory_path + \"\\\\wordJson\\\\label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label2idx = json.load(f)\n",
    "\n",
    "idx2label = {value: key for key, value in label2idx.items()}\n",
    "xIds = [word2idx.get(item, word2idx[\"UNK\"]) for item in x.split(\" \")]\n",
    "if len(xIds) >= config.sequenceLength:\n",
    "    xIds = xIds[:config.sequenceLength]\n",
    "else:\n",
    "    xIds = xIds + [word2idx[\"PAD\"]] * (config.sequenceLength - len(xIds))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=sess_config)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        local_path = \"\\\\\".join(savedModelPath.split(\"\\\\\")[:-1]) + \"\\\\model\\\\adversarialLSTM\\\\\"\n",
    "        checkpoint_file = tf.train.latest_checkpoint(local_path)\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "        \n",
    "        # 获取model的输入参数、输出的结果所依赖的输入值\n",
    "        inputX = graph.get_operation_by_name(\"inputX\").outputs[0]\n",
    "        dropoutKeepProb = graph.get_operation_by_nmae(\"dropoutKeepProb\").outputs[0]\n",
    "        predictions = graph.get_tensor_by_name(\"output/predictions:0\")\n",
    "        pred = sess.run(predictions, feed_dict={\n",
    "                inputX: [xIds], \n",
    "                dropoutKeepProb: 1.0\n",
    "            })[0]\n",
    "\n",
    "pred = [idx2label[item] for item in pred]\n",
    "print(\"the prediction result is: \", pred)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
